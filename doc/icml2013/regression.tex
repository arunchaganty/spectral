\subsection{Low-rank Regression}
\label{sec:regression}

The goal of this section is to bound the error of
the compound parameter estimates,
specifically $\|\Delta_2\|_F^2$ and $\|\Delta_3\|_F^2$,
where $\Delta_2 \eqdef \hat M_2 - M_2$
and $\Delta_3 \eqdef \hat M_3 - M_3$.

Our analysis is based on the low-rank regression framework of
\citet{Tomioka2011} for tensors, which builds off of
\citet{NegahbanWainwright2009} for matrices.
Define the observation operator $\opX(M_p) : \Re^{d\tp{p}} \to \Re^{n}$
mapping compound parameters $M_p$:
\begin{align}
  \opX(M_p; \sD)_i &\eqdef \frac12 \sum_{(x,y) \in \sD} \innerp{M_p}{x\tp{p}}_i, \quad i  \in [n].
\end{align}

Let $\kappa(\opX)$ be the restricted strong constant,
and let $\opX^*(\eta_p; \sD) = \sum_{(x,y) \in \sD} \eta_p(x) x\tp{p}$
be the adjoint.

\paragraph{Restricted strong convexity}

In this section, we compute the restricted strong convexity constant
$\kappa(\opX)$ for our problem.

%In the previous section, we described an algorithm for the mixture of
%linear regressions using regression to recover $M_2$ and $M_3$,
%described by \equationref{eqn:y2} and \equationref{eq:y3}, as
%a subroutine. In this section, we will characterize the rate of
%convergence of regression.

%Analysis for regression in the fixed and random design settings have
%been studied before \citep{HsuKakadeZhang}, however our setup differs
%substantially from the noise models assumed in the literature. In our
%scenario, the variance in the estimation comes not only from the
%Gaussian observation noise (which has been studied before), but also
%from the variance in the latent variable $h$.

%Let us now formally define the class of regression problems we wish to
%analyze, i.e. regression on the set $(x\tp{p}, y^p)$,
%\begin{align*}
%  y^p &= \innerp{x\tp{p}}{M_p} + (\innerp{x\tp{p}}{M_p - \beta_h\tp{p}} + \varepsilon).
%\end{align*}

%\todo{Describe/define the convex tensor stuff.}
%We would like to exploit
%the property that $M_p$ is low rank (as typically $K \ll D$). It has
%been shown that a convex relaxation for this problem regression with
%trace norm regularization, which can be solved using a proximal
%subgradient descent algorithm\citationneeded.
%The analogue of trace norm
%regularization for higher order tensors corresponds to the sum of the
%trace norms of the mode-k unfolding of the tensor \cite{Tomioka2011},
%$X_{(k)}$, is a $d_k \times (\prod_{k' \neq k} d_{k'})$ matrix obtained
%by concatenating the entries for all dimensions other than $k$. For
%example, the 1-mode unfolding of a 3rd order tensor has entries,
%$X_{(1)}^{i_1, (i_2, i_3)} = X_{i_1, i_2, i_3}$.

%In general, the optimization problem we'd like to solve is,
%\begin{align}
%  \hat M_p &= 
%  \arg\min_{M_p}& \frac{1}{2N} \| \vec y - \opX(M_p) \|_2^2 + \frac{\lambda_n}{K} \sum_{h=1}^K \| (M_p)_{(h)} \|_* \label{eq:regression}.
%\end{align}

%We begin our analysis by stating a generic theorem about convex
%optimization problem in \equationref{eq:regression}, 
\begin{lemma}[convergence of low-rank regression]
If restricted strong convexity holds
$$\frac{1}{2n} \| \opX( \Delta )\|_2^2 \ge \kappa(\opX) \|\Delta\|^2_F \quad \text{and} \quad
\lambda_n \ge \frac{|\opX^*(\eta_p)|}{n}.$$
Then the error of $\hat M_p$ is bounded as follows:
$\| \hat M_p - M_p^* \|_F \le \frac{\lambda_n \sqrt{k}}{\kappa(\opX)}$.
\end{lemma}
\begin{proof}
  \citet{Tomioka2011}, Theorem 1.

  This proof follows from convex properties \todo{elaborate}.
  \citet{NegahbanWainwright2009} prove this theorem for matrices, i.e.
  when $p = 2$. \citet{Tomioka2011} generalized it for arbitrary $p$.
\end{proof}

Going forward, we need to lower bound the restricted strong convexity
parameter $\kappa(\opX)$ and upper bound the adjoint operator
$\|\opX^*(\epsilon)\|_{2}^2$, which corresponds to a gradient update. We
will appeal to the random design framework that models the input $x$ as
random and show bounds that hold with high probability.

\begin{lemma}[restricted strong convexity]
Let $\opX$ be the linear operator previously defined. Then,
$$\kappa(\opX) \ge Y$$.
\end{lemma}
\begin{proof}
  \todo{Do I need to transform rewrite $M_p$ in terms of orthogonal components?}

  \begin{align*}
    \frac{\|\opX(M_p)\|_1}{N} &= \frac{1}{N}\sum_{i=1}^{N} |\innerp{x_i\tp{p}}{M_p}| \\
    &\ge \frac{1}{N}\sum_{i=1}^{N} \sigma_{min}( M_p ) \|x_i\|^p \\
    &\ge \frac{\kappa(M_p)}{\sqrt{D}} \frac{\sum_{i=1}^{N} \|x_i\|^p}{N} \| M_p \|_F.
  \end{align*}

  While $\kappa(M_p)$ depends on $M_p$ we can lower bound it by
  restricting the convex space to $\{ M_p | \kappa(M_p) \ge \kappa \}$.
  Further, using concentration bounds, we can lower bound $\|x_i\|^p$ as
  well. \todo{What is $\E[\|x_i\|^3]$? It is some quantity greater than
  zero}.
  
  Thus, 
  \begin{align*}
  \frac{ \|\opX(M_p)\|_2 }{2 N} 
  &\ge
  \frac{ \|\opX(M_p)\|_1 }{2 \sqrt{D}N} 
  &\ge \frac{\kappa}{2 D} (\E[\|x_i\|^p] - \frac{t}{N}) \| M_p \|_F.
  \end{align*}
  with high probability $1 - \delta$. \todo{The bound needs to be shown
  for $\| \opX(M_p) \|_2^2$.}
  
  In other words, $\kappa(\opX)
  = \frac{\kappa}{2 D} (\E[\|x_i\|^p] - \frac{t}{N})$ is a valid
  lower bound.
\end{proof}

By Lemma 19 of \cite{hsu13spherical},
we have that $\frac{1}{n} \sum_{i=1}^n \epsilon_i^3 = O(\sqrt{\frac{\log^3(1/\delta)}{n}})$.
with probability at least $1-\delta$.

\begin{lemma}[bounded error]
  Let $\opX$ be the linear operator previously defined. Then,
  $$\opX^*(\epsilon) \le Y$$.
\end{lemma}
\begin{proof}
  \begin{align*}
    \frac{\opX^*(\epsilon)}{N} 
    &= \frac{1}{N}\sum_{i=1}^{N} \epsilon_i \innerp{x_i\tp{p}}{M_p} \\
    &= \frac{1}{N}\sum_{i=1}^{N} (\innerp{x_i\tp{p}}{M_p - \beta_i\tp{p}} + \varepsilon)  \innerp{x_i\tp{p}}{M_p}.
  \end{align*}
  Let $M_p - \beta_i\tp{p} = \tilde M_i$ and consider it to be a random
  variable.
  \begin{align*}
    \frac{\opX^*(\epsilon)}{N} 
    &= \frac{1}{N}\sum_{i=1}^{N} \innerp{\tilde M_i}{x_i\tp{2p}} + \varepsilon \innerp{M_p}{x_i\tp{p}} \\
    &\le \frac{1}{N}\sum_{i=1}^{N} \|\tilde M_i\|_{op} \|x_i\|^{2p} + \varepsilon \|M_p\|_{op}\|x_i\|^{p} \\
    &\le \|M_p\|_{op} ( \frac{1}{N}\sum_{i=1}^{N} \|x_i\|^{2p} + \varepsilon \|x_i\|^{p} ).
  \end{align*}

  We can probabilistically bound the above by considering the union
  bound of $\varepsilon$ and $\|x_i\|^p$.


  Finally,
  \begin{align*}
    \frac{\opX^*(\epsilon)}{N} 
    &\le \|M_p\|_{op} (\E[\|x_i\|^{2p}] + t/N + (t/N) (\E[ \|x_i\|^{p} ] + t/N) ),
  \end{align*}
  with high probability. Note that this does not converge to $0$ as $N
  \to \infty$ because of the variance of the selection probability.
\end{proof}
