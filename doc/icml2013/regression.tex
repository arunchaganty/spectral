\subsection{Low-rank Regression}
\label{sec:regression}

The goal of this section is to bound the error of
the compound parameter estimates,
specifically $\|\Delta_2\|_F^2$ and $\|\Delta_3\|_F^2$,
where $\Delta_2 \eqdef \hat M_2 - M_2$
and $\Delta_3 \eqdef \hat M_3 - M_3$.

Our analysis is based on the low-rank regression framework of
\citet{Tomioka2011} for tensors, which builds off of
\citet{NegahbanWainwright2009} for matrices.
The main challenge is to control the noise $\eta_p(x)$,
which in addition to containing the usual Gaussian part, is also dependent on
$x$, involves the mixing noise and various cross terms.
We first set up some notation to all three regressions (\refeqn{estimateM1}, \refeqn{estimateM2}, and \refeqn{estimateM3}).
Define the observation operator $\opX(M_p) : \Re^{d\tp{p}} \to \Re^{n}$
mapping compound parameters $M_p$:
\begin{align}
  \opX(M_p; \sD)_i &\eqdef \frac12 \sum_{(x,y) \in \sD} \innerp{M_p}{x\tp{p}}_i, \quad i  \in [n].
\end{align}

Let $\kappa(\opX)$ be the restricted strong constant,
and let $\opX^*(\eta_p; \sD) = \sum_{(x,y) \in \sD} \eta_p(x) x\tp{p}$
be the adjoint.

\paragraph{Restricted strong convexity}

In this section, we lower bound the restricted strong convexity constant
$\kappa(\opX)$.

\paragraph{Adjoint operator}

In this section, we upper bound the operator norm of the adjoint
$\|\opX(\eta_p)\|_\text{op}$.



%In the previous section, we described an algorithm for the mixture of
%linear regressions using regression to recover $M_2$ and $M_3$,
%described by \equationref{eqn:y2} and \equationref{eq:y3}, as
%a subroutine. In this section, we will characterize the rate of
%convergence of regression.

%Analysis for regression in the fixed and random design settings have
%been studied before \citep{HsuKakadeZhang}, however our setup differs
%substantially from the noise models assumed in the literature. In our
%scenario, the variance in the estimation comes not only from the
%Gaussian observation noise (which has been studied before), but also
%from the variance in the latent variable $h$.

%Let us now formally define the class of regression problems we wish to
%analyze, i.e. regression on the set $(x\tp{p}, y^p)$,
%\begin{align*}
%  y^p &= \innerp{x\tp{p}}{M_p} + (\innerp{x\tp{p}}{M_p - \beta_h\tp{p}} + \varepsilon).
%\end{align*}

%\todo{Describe/define the convex tensor stuff.}
%We would like to exploit
%the property that $M_p$ is low rank (as typically $K \ll D$). It has
%been shown that a convex relaxation for this problem regression with
%trace norm regularization, which can be solved using a proximal
%subgradient descent algorithm\citationneeded.
%The analogue of trace norm
%regularization for higher order tensors corresponds to the sum of the
%trace norms of the mode-k unfolding of the tensor \cite{Tomioka2011},
%$X_{(k)}$, is a $d_k \times (\prod_{k' \neq k} d_{k'})$ matrix obtained
%by concatenating the entries for all dimensions other than $k$. For
%example, the 1-mode unfolding of a 3rd order tensor has entries,
%$X_{(1)}^{i_1, (i_2, i_3)} = X_{i_1, i_2, i_3}$.

%In general, the optimization problem we'd like to solve is,
%\begin{align}
%  \hat M_p &= 
%  \arg\min_{M_p}& \frac{1}{2N} \| \vec y - \opX(M_p) \|_2^2 + \frac{\lambda_n}{K} \sum_{h=1}^K \| (M_p)_{(h)} \|_* \label{eq:regression}.
%\end{align}

\begin{lemma}[\citet{Tomioka2011}, Theorem 1]
Suppose there exists a restricted strong convexity constant $\kappa(\opX)$ such that
$$\frac{1}{2n} \| \opX( \Delta )\|_2^2 \ge \kappa(\opX) \|\Delta\|^2_F \quad \text{and} \quad
\lambda_n \ge \frac{\|\opX^*(\eta_p)\|_2}{n}.$$
Then the error of $\hat M_p$ is bounded as follows:
$\| \hat M_p - M_p^* \|_F \le \frac{\lambda_n \sqrt{k}}{\kappa(\opX)}$.
\end{lemma}

Going forward, we need to lower bound the restricted strong convexity
parameter $\kappa(\opX)$ and upper bound the adjoint operator
$\|\opX^*(\epsilon)\|_{2}^2$.
%We
%will appeal to the random design framework that models the input $x$ as
%random and show bounds that hold with high probability.

\begin{lemma}[lower bound on restricted strong convexity]
Let $\Sigma_p \eqdef \E[\cvec(x\tp{p})\tp{2}]$.
With high probability at least $1-\delta$,
for some constant $c$,
$$\kappa(\opX) \ge c \sigma_\text{min}^p(\Sigma_p)$$,
where
%$\sigma_\text{min}^p(\Sigma_p) = \min_{M_p \in \sM_p : \|M_p\|_F \le 1} \cvec(M_p)^\top \Sigma_p \cvec(M_p)$.
$\sigma_\text{min}^p(\Sigma_p) = \min_{M_p \in \sM_p : \|M_p\|_F \le 1} \trace(\cvec(M_p)^{\otimes 2} \Sigma_p)$.
\end{lemma}
Note that $\sigma_\text{min}$ 
\begin{proof}
Expanding the definition of the observation operator:
$\frac{1}{2n} \|\opX(\Delta)\|_2^2
= \frac{1}{2n} \sum_{(x,y) \in \sD} \innerp{\Delta}{x\tp{p}}^2$.
Unfolding the tensor and letting $\hat\Sigma_p \eqdef \frac{1}{n} \sum_{(x,y) \in \sD} \cvec(x\tp{p})\tp{2}$,
we have 
$\frac{1}{2n} \|\opX(\Delta)\|_2^2
\ge \frac{1}{2} \trace(\cvec(\Delta)\tp{2} \hat\Sigma_p)
\ge \frac{1}{2} \|\Delta\|_F^2 \sigma_\text{min}^p(\hat\Sigma_p)$.

%  \begin{align*}
%    \frac{\|\opX(M_p)\|_1}{N} &= \frac{1}{N}\sum_{i=1}^{N} |\innerp{x_i\tp{p}}{M_p}| \\
%    &\ge \frac{1}{N}\sum_{i=1}^{N} \sigma_{min}( M_p ) \|x_i\|^p \\
%    &\ge \frac{\kappa(M_p)}{\sqrt{D}} \frac{\sum_{i=1}^{N} \|x_i\|^p}{N} \| M_p \|_F.
%  \end{align*}
%
%  While $\kappa(M_p)$ depends on $M_p$ we can lower bound it by
%  restricting the convex space to $\{ M_p | \kappa(M_p) \ge \kappa \}$.
%  Further, using concentration bounds, we can lower bound $\|x_i\|^p$ as
%  well. \todo{What is $\E[\|x_i\|^3]$? It is some quantity greater than
%  zero}.
%  
%  Thus, 
%  \begin{align*}
%  \frac{ \|\opX(M_p)\|_2 }{2 N} 
%  &\ge
%  \frac{ \|\opX(M_p)\|_1 }{2 \sqrt{D}N} 
%  &\ge \frac{\kappa}{2 D} (\E[\|x_i\|^p] - \frac{t}{N}) \| M_p \|_F.
%  \end{align*}
%  with high probability $1 - \delta$. \todo{The bound needs to be shown
%  for $\| \opX(M_p) \|_2^2$.}
%  
%  In other words, $\kappa(\opX)
%  = \frac{\kappa}{2 D} (\E[\|x_i\|^p] - \frac{t}{N})$ is a valid
%  lower bound.
\end{proof}

\begin{lemma}[bounded error]
  Let $\opX$ be the linear operator previously defined. Then,
  $$\|\opX^*(\epsilon)\|_\text{op} \le Y$$.
\end{lemma}
\begin{proof}
%  \begin{align*}
%    \frac{\opX^*(\epsilon)}{N} 
%    &= \frac{1}{N}\sum_{i=1}^{N} \epsilon_i \innerp{x_i\tp{p}}{M_p} \\
%    &= \frac{1}{N}\sum_{i=1}^{N} (\innerp{x_i\tp{p}}{M_p - \beta_i\tp{p}} + \varepsilon)  \innerp{x_i\tp{p}}{M_p}.
%  \end{align*}
%  Let $M_p - \beta_i\tp{p} = \tilde M_i$ and consider it to be a random
%  variable.
%  \begin{align*}
%    \frac{\opX^*(\epsilon)}{N} 
%    &= \frac{1}{N}\sum_{i=1}^{N} \innerp{\tilde M_i}{x_i\tp{2p}} + \varepsilon \innerp{M_p}{x_i\tp{p}} \\
%    &\le \frac{1}{N}\sum_{i=1}^{N} \|\tilde M_i\|_{op} \|x_i\|^{2p} + \varepsilon \|M_p\|_{op}\|x_i\|^{p} \\
%    &\le \|M_p\|_{op} ( \frac{1}{N}\sum_{i=1}^{N} \|x_i\|^{2p} + \varepsilon \|x_i\|^{p} ).
%  \end{align*}
%
%  We can probabilistically bound the above by considering the union
%  bound of $\varepsilon$ and $\|x_i\|^p$.
%
%
%  Finally,
%  \begin{align*}
%    \frac{\opX^*(\epsilon)}{N} 
%    &\le \|M_p\|_{op} (\E[\|x_i\|^{2p}] + t/N + (t/N) (\E[ \|x_i\|^{p} ] + t/N) ),
%  \end{align*}
%  with high probability. Note that this does not converge to $0$ as $N
%  \to \infty$ because of the variance of the selection probability.

By Lemma 19 of \cite{hsu13spherical},
we have that $\frac{1}{n} \sum_{i=1}^n \epsilon_i^3 = O(\sqrt{\frac{\log^3(1/\delta)}{n}})$.
with probability at least $1-\delta$.

\end{proof}
