\paragraph{Low-rank regression}
%\label{sec:regression}
\vspace{-0.5em}
We now bound the error of
the compound parameter estimates,
$\|\Delta_2\|_F^2$ and $\|\Delta_3\|_F^2$,
where $\Delta_2 \eqdef \hat M_2 - M_2$
and $\Delta_3 \eqdef \hat M_3 - M_3$.

Our analysis is based on the low-rank regression framework of
\citet{Tomioka2011} for tensors, which builds off of
\citet{NegahbanWainwright2009} for matrices.
The main challenge is to control the noise $\eta_p(x)$,
which in addition to containing the usual Gaussian part, is also dependent on
$x$, involves the mixing noise and various cross terms.
We first set up some notation that unifies all three regressions (\refeqn{estimateM1}, \refeqn{estimateM2}, and \refeqn{estimateM3}).
Define the observation operator $\opX_p(M_p) : \Re^{d\tp{p}} \to \Re^{n}$
mapping compound parameters $M_p$:
\begin{align}
\opX_p(M_p; \sD)_i &\eqdef \frac12 \sum_{(x,y) \in \sD} \innerp{M_p}{x\tp{p}}_i, \quad i  \in [n].
\end{align}

Let $\kappa(\opX_p)$ be the restricted strong convexity constant,
and let $\opX^*_p(\eta_p; \sD) = \sum_{(x,y) \in \sD} \eta_p(x) x\tp{p}$
be the adjoint.

%\paragraph{Restricted strong convexity}

%Let us first lower bound the restricted strong convexity constant
%$\kappa(\opX_p)$:

%In the previous section, we described an algorithm for the mixture of
%linear regressions using regression to recover $M_2$ and $M_3$,
%described by \equationref{eqn:y2} and \equationref{eq:y3}, as
%a subroutine. In this section, we will characterize the rate of
%convergence of regression.

%Analysis for regression in the fixed and random design settings have
%been studied before \citep{HsuKakadeZhang}, however our setup differs
%substantially from the noise models assumed in the literature. In our
%scenario, the variance in the estimation comes not only from the
%Gaussian observation noise (which has been studied before), but also
%from the variance in the latent variable $h$.

%Let us now formally define the class of regression problems we wish to
%analyze, i.e. regression on the set $(x\tp{p}, y^p)$,
%\begin{align*}
%  y^p &= \innerp{x\tp{p}}{M_p} + (\innerp{x\tp{p}}{M_p - \beta_h\tp{p}} + \varepsilon).
%\end{align*}

%\todo{Describe/define the convex tensor stuff.}
%We would like to exploit
%the property that $M_p$ is low rank (as typically $K \ll D$). It has
%been shown that a convex relaxation for this problem regression with
%trace norm regularization, which can be solved using a proximal
%subgradient descent algorithm\citationneeded.
%The analogue of trace norm
%regularization for higher order tensors corresponds to the sum of the
%trace norms of the mode-k unfolding of the tensor \cite{Tomioka2011},
%$X_{(k)}$, is a $d_k \times (\prod_{k' \neq k} d_{k'})$ matrix obtained
%by concatenating the entries for all dimensions other than $k$. For
%example, the 1-mode unfolding of a 3rd order tensor has entries,
%$X_{(1)}^{i_1, (i_2, i_3)} = X_{i_1, i_2, i_3}$.

%In general, the optimization problem we'd like to solve is,
%\begin{align}
%  \hat M_p &= 
%  \arg\min_{M_p}& \frac{1}{2N} \| \vec y - \opX_p(M_p) \|_2^2 + \frac{\lambda_n}{K} \sum_{h=1}^K \| (M_p)_{(h)} \|_* \label{eq:regression}.
%\end{align}

\begin{lemma}[\citet{Tomioka2011}, Theorem 1]
\label{lem:lowRank}
Suppose there exists a restricted strong convexity constant $\kappa(\opX)$ such that
$$\frac{1}{2n} \| \opX_p( \Delta )\|_2^2 \ge \kappa(\opX) \|\Delta\|^2_F \quad \text{and} \quad
\lambda_n \ge \frac{\|\opX^*(\eta_p)\|_2}{n}.$$
Then the error of $\hat M_p$ is bounded as follows:
$\| \hat M_p - M_p^* \|_F \le \frac{\lambda_n \sqrt{k}}{\kappa(\opX)}$.
\end{lemma}

Going forward, we need to lower bound the restricted strong convexity
parameter $\kappa(\opX)$ and upper bound the adjoint operator
$\|\opX^*(\eta_p)\|_{2}^2$.
%We
%will appeal to the random design framework that models the input $x$ as
%random and show bounds that hold with high probability.

%\paragraph{Adjoint operator}

%In this section, we upper bound the operator norm of the adjoint
%$\|\opX_p(\eta_p)\|_\text{op}$.

First, let us lower bound the restricted strong convexity parameter $\kappa(\opX_p)$:

\begin{lemma}[lower bound on restricted strong convexity]
\label{lem:lowRankLower}
Let $\Sigma_p \eqdef \E[\cvec(x\tp{p})\tp{2}]$.
With probability at least $1-\delta$,
$$\kappa(\opX) = \Omega\left(\sigmamin(\Sigma_p) - d^p R^{p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}}\right).$$
\end{lemma}
\begin{proof}
Expanding the definition of the observation operator:
$\frac{1}{2n} \|\opX_p(\Delta)\|_2^2
= \frac{1}{2n} \sum_{(x,y) \in \sD} \innerp{\Delta}{x\tp{p}}^2$.
Unfolding the tensor and letting $\hat\Sigma_p \eqdef \frac{1}{n} \sum_{(x,y) \in \sD} \cvec(x\tp{p})\tp{2}$,
we have 
$\frac{1}{2n} \|\opX_p(\Delta)\|_2^2
\ge \frac{1}{2} \trace(\cvec(\Delta)\tp{2} \hat\Sigma_p)
\ge \frac{1}{2} \|\Delta\|_F^2 \sigmamin(\hat\Sigma_p)$.
It remains to show that $\sigmamin(\hat\Sigma_p)$ and $\sigmamin(\Sigma_p)$ are close.
To do this, we first apply Hoeffding's inequality elementwise.
Since $\|x\| \le R$, we have that for each element $(i,j)$,
$|(\hat\Sigma_p)_{ij} - (\Sigma_p)_{ij}| = O(R^{p}\sqrt{\frac{\log (1/\delta)}{n}})$.
Applying the union bound over the $d^{2p}$ elements of $\hat\Sigma_p - \Sigma_p$,
we have that the max norm is bounded:
$\|\hat\Sigma_p - \Sigma_p\|_\text{\rm max} = O(R^{p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}})$.
The max norm times $d^p$ upper bounds the Frobenius norm, which upper bounds the operator norm, so we have that
$\|\hat\Sigma_p - \Sigma_p\|_\text{\rm op} = O(d^p R^{p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}})$.
Using the fact that $\sigmamin(\hat\Sigma_p) = \sigmamin(\Sigma_p) + \|\hat\Sigma_p - \Sigma_p\|_\text{\rm op}$
yields the result.
\end{proof}

Next, let us upper bound the operator norm of the observation adjoint:

\begin{lemma}[upper bound on adjoint noise]
\label{lem:lowRankUpper}
Let $\opX_p$ be the linear operator previously defined. Then with probability at least $1-\delta$,
$$\frac1{n} \|\opX_p^*(\eta_p)\|_\op \le O\left(\sigma^3 L^3 R^6 \sqrt{\frac{\log^3(1/\delta)}{n}}\right).$$
for each $p \in \{1,2,3\}$.
\end{lemma}
\begin{proof}
Let $\hat\E_p[f(x,\epsilon,h)]$ denote the empirical expectation over the examples in dataset $\sD_p$
(recall the $\sD_p$'s are independent to simplify the analysis).
By definition,
$\frac1n \|\opX_p^*(\eta_p)\|_\op = \hat\E_p[\eta_p(x) x\tp{p}]$
for $p \in \{1,2,3\}$ defined in \refeqn{y1}, \refeqn{y2}, and \refeqn{y3}.
Each $\eta_p(x)$ is composed of several zero-mean random variables,
and since $\|A + B\|_\op \le \|A\|_\op + \|B\|_\op$,
it suffices to consider each term in turn.
For each term, we will condition on $x$, use a standard concentration inequality to
bound the noise.

First, consider $\eta_1(x)$, which contains two terms.
The first term is $\innerp{M_1 - \beta_h}{x}$.
We have that $\|M_1 - \beta_h\|_2 \le L$,
so by Hoeffding's inequality, and using the fact that $\|x\|_2 \le R$,
with probability $1-\delta_1$,
$\|\hat\E_1[\innerp{M_1 - \beta_h}{x} x]\|_\op = O(R^2 L \sqrt{\frac{\log(1/\delta_1)}{n}})$.
The second term is $\epsilon \sim \normal{0}{\sigma^2}$,
with probability $1-\delta_1$,
$\|\hat\E_1[\epsilon x]\|_\op = O(R \sigma \sqrt{\frac{\log(1/\delta_1)}{n}})$.

Now, consider $\eta_2(x)$, which contains three terms.
Using similar techniques, we have that
$\|\hat\E_2[\eta_2(x) x\tp{2}]\|_\op = O(R^2 (L^2 R^2 + \sigma L R + \sigma^2) \sqrt{\frac{\log(1/\delta_1)}{n}})$.

Finally, $\eta_3(x)$ contains five terms plus an additional bias since the estimator uses $\hat M_1$ rather than $M_1$.
There are two additional considerations.
First, to bound $\hat\E_3[\epsilon^3]$, we use Lemma 19 of \cite{hsu13spherical}.
Second, the additional bias is small by the previous bound (and it is constructed on $\sD_1$, which is independent of $\sD_3$
used to construct $\hat M_3$.
Therefore, we incur another $O(\sigma^2 \cdot R \sigma \sqrt{\frac{\log(1/\delta_1)}{n}})$.
We can handle the other terms using conventional means.
Combining everything, we obtain
$\|\hat\E_3[\eta_3(x) x\tp{3}]\|_\op = O(R^3 (L^3 R^3 + \sigma L^2 R^2 + \sigma^2 L R + \sigma^3 + \sigma^3 R) \sqrt{\frac{\log^3(1/\delta_1)}{n}})$.

Finally, taking $\delta_1 = \delta/3$, and taking the union bound over the bounds for $p \in \{1,2,3\}$,
we get our result.
Note that we are bounding quantities fairly crudely for the sake of simplicity.
\end{proof}

These lemmas allow us to control the compound parameter error.
We now apply them in the proof of \refthm{convergence}:
\begin{proof}
By \reflem{lowRank} along with \reflem{lowRankLower} and \reflem{lowRankUpper},
we can control the Frobenius norm of the parameter estimates:
If $n \ge n_1$, then
\begin{align}
  \|\hat M_p - M_p\|_F = O\left( \sigma^3 L^3 R^6 k^{\frac12} \sigmamin(\Sigma_p)^{-1} \sqrt{\frac{\log^3 (1/\delta)}{n}} \right).
  %\|\hat M_p - M_p\|_F = O\left( \frac{\sigma^3 L^3 R^6 \sqrt{k} \log^3 (1/\delta)}{\sqrt{n} (\sigmamin(\Sigma_p) - d^p R^p \sqrt{\frac{p \log(d) \log(1/\delta)}{n}}} \right).
\end{align}
The Frobenius norm upper bounds the operator norm,
so we can directly apply Theorem 5.1 of \citet{AnandkumarGeHsu2012}.
%Let $T^* = M_3^*(W^*, W^*, W^*)$.
%Need to convert error from $\epsilon$
%hit by $(W^\top)^\dagger$
\end{proof}

