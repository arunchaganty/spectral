\section{Regression}

Let us review the regression problem set up in \cite[Section
3]{ChagantyLiang2013}. We assume we are given data $(x_i,y_i) \in \sD_p$
generated by the following process,
\begin{align}
  y_i &= \innerp{M_p}{x_i\tp{p}} + \eta_p(x_i),
\end{align}
where $M_p = \sum_{h=1}^k \pi_h \beta_h\tp{p}$, the p-th order moments
of $\beta_h$ and $\eta_p(x)$ is zero mean noise. In particular, for $p \in \{1,2,3\}$,
we showed that $\eta_p(x)$ were defined to be,
\begin{align}
  \eta_1(x) &= \innerp{\beta_h - M_1}{x} + \epsilon \label{eqn:eta1} \\
  \eta_2(x) &= \innerp{\beta_h\tp{2} - M_2}{x\tp{2}} + 2 \epsilon \innerp{\beta_h}{x} + (\epsilon^2 - \E[\epsilon^2]) \label{eqn:eta2}\\
  \eta_3(x) &= \innerp{\beta_h\tp{3} - M_3}{x\tp{3}}
        + 3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}} 
        + 3(\epsilon^2 \innerp{\beta_h}{x} - \E[\epsilon^2] \innerp{M_1}{x})
        + (\epsilon^3 - \E[\epsilon^3]). \label{eqn:eta3}
\end{align}
We assume that $\|x_i\| \le R$, $\| \beta_h \| \le L$ and $|\epsilon| \le S$.

We then defined the observation operator $\opX_p(M_p) : \Re^{d\tp{p}} \to \Re^{n}$,
\begin{align}
\opX_p(M_p; \sD_p)_i &\eqdef \innerp{M_p}{x\tp{p}_i}, & (x_i, y_i) \in \sD_p,
\end{align}
which let us succinctly represent the low-rank regression problem as follows,
\begin{align}
  \min_{M_p \in \Re^{d\tp{p}}} \frac{1}{2n} \| y - \opX_p(M_p; \sD_p) \|^2_2 + \lambda_p \|M_p\|_*.
\end{align}

Let us also recall the adjoint of the observation operator, $\opX_p^* : \Re^{n} \to \Re^{d^p}$,
\begin{align}
  \opX_p^*(\eta_p; \sD_p) &= \sum_{x \in \sD_p} \eta_p(x) x\tp{p},
\end{align}
where we have used $\eta_p$ to represent the vector $\left[\eta_p(x)\right]_{x \in \sD_p}$. 

\citet{Tomioka2011} showed that error in the estimated $\hat M_p$ can be
bounded as follows;

\begin{lemma}[\citet{Tomioka2011}, Theorem 1]
\label{lem:lowRank}
Suppose there exists a restricted strong convexity constant $\kappa(\opX_p)$ such that
$$\frac{1}{2n} \| \opX_p( \Delta )\|_2^2 \ge \kappa(\opX_p) \|\Delta\|^2_F \quad \text{and} \quad
\lambda_n \ge \frac{\|\opX_p^*(\eta_p)\|_\op}{n}.$$
Then the error of $\hat M_p$ is bounded as follows:
$\| \hat M_p - M_p^* \|_F \le \frac{\lambda_n \sqrt{k}}{\kappa(\opX_p)}$.
\end{lemma}

In this section, we will derive an upper bound on $\kappa(\opX_p)$ and
a lower bound on $\frac{1}{n} \| \opX_p^*(\eta_p) \|_\op$.

\begin{lemma}[Lower bound on restricted strong convexity]
\label{lem:lowRankLower}
Let $\Sigma_p \eqdef \E[\cvec(x\tp{p})\tp{2}]$.
If $$n \ge \frac{16 (p!)^2 R^{4p}}{\sigmamin(\Sigma_p)^2} \left(1 + \sqrt{\frac{\log(1/\delta)}{2}}\right)^2,$$
then, with probability at least $1-\delta$,
$$\kappa(\opX_p) \ge \frac{\sigmamin(\Sigma_p)}{2}.$$
\end{lemma}

\begin{proof}
  Recall the definition of $\kappa(\opX_p)$, 
  $$\frac{1}{n} \|\opX_p(\Delta)\|_2^2 \ge \kappa(\opX_p) \|\Delta\|^2_F.$$
Expanding the definition of the observation operator:
$$\frac{1}{n} \|\opX_p(\Delta)\|_2^2 = \frac{1}{n} \sum_{(x,y) \in \sD_p} \innerp{\Delta}{x\tp{p}}^2.$$
Unfolding the tensors, letting $\hat\Sigma_p \eqdef \frac{1}{n}
\sum_{(x,y) \in \sD_p} \cvec(x\tp{p})\tp{2}$, $\frac{1}{n}
\|\opX_p(\Delta)\|_2^2 = \trace(\cvec(\Delta)\tp{2} \hat\Sigma_p)$. 
We recall that each element of $\cvec(\Delta)$ aggregates elements with
permuted indices, so $\|\vvec(\Delta)\|_2 \le \|\cvec(\Delta)\|_2 \le p!
\|\vvec(\Delta)\|_2$. Then, we have 
\begin{align}
\frac{1}{n} \|\opX_p(\Delta)\|_2^2 
  &= \trace(\cvec(\Delta)\tp{2} \hat\Sigma_p) \\
  &\ge \sigmamin(\hat\Sigma_p) \|\Delta\|_F^2 .
\end{align}
By Weyl's theorem, $$\sigmamin(\hat\Sigma_p) \ge
\sigmamin(\Sigma_p) - \|\hat\Sigma_p - \Sigma_p\|_\Lop.$$
Since $\|\hat\Sigma_p - \Sigma_p\|_\Lop \le \|\hat\Sigma_p - \Sigma_p\|_{F}$,
it suffices to show that the empirical covariance concentrates in Frobenius norm.
Applying \reflem{conc-norms}, with
probability at least $1 - \delta$, $$\| \hat\Sigma_p - \Sigma_p \|_F
\le \frac{2 \|\Sigma_p\|_F}{\sqrt n} \left( 1 + \sqrt{\frac{\log(1/\delta)}{2}} \right).$$
Now we seek to control $\|\Sigma_p\|_F$.
Since $\|x\|_2 \le R$, we can use the
bound $$\| \Sigma_p \|_F \le p! \| \vvec(x\tp{p})\tp{2} \|_F \le p! R^{2p}.$$

Finally, $\|\hat\Sigma_p - \Sigma_p\|_\op \le \sigmamin(\Sigma_p)/2$ with probability at least $1 - \delta$ if,
$$n \ge \frac{16 (p!)^2 R^{4p}}{\sigmamin(\Sigma_p)^2} \left(1 + \sqrt{\frac{\log(1/\delta)}{2}}\right)^2.$$

%To do this, we first apply Hoeffding's inequality elementwise.
%Since $\|x\|_2 \le R$, we have that for each element $(i,j)$,
%$|(\hat\Sigma_p)_{ij} - (\Sigma_p)_{ij}| = O(R^{2p}\sqrt{\frac{\log (1/\delta)}{n}})$.
%Applying the union bound over the $d^{2p}$ elements of $\hat\Sigma_p - \Sigma_p$,
%we have that the max norm is bounded:
%$\|\hat\Sigma_p - \Sigma_p\|_\text{\rm max} = O(R^{2p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}})$.
%The max norm times $d^p$ upper bounds the Frobenius norm, which upper bounds the operator norm, so we have that
%$\|\hat\Sigma_p - \Sigma_p\|_\text{\rm op} = O(d^p R^{2p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}})$.
%Using the fact that $\sigmamin(\hat\Sigma_p) \ge \sigmamin(\Sigma_p) - \|\hat\Sigma_p - \Sigma_p\|_\text{\rm op}$
%yields the result.
\end{proof}

\begin{lemma}[Upper bound on adjoint operator]
\label{lem:lowRankUpper}
With probability at least $1-\delta$, the following holds,
\begin{align*}
  \frac{1}{n} \|\opX_1^*(\eta_1)\|_\op
      &\le \frac{2R (2LR + S)}{\sqrt{n}} \left( 1 + \sqrt{\frac{\log(3/\delta)}{2}} \right) \\
  \frac{1}{n}  \|\opX_2^*(\eta_2)\|_\op 
      &\le \frac{(4L^2 R^2 + 2 S L R + 4S^2)R^2}{\sqrt{n}} \left( 1 + \sqrt{\frac{\log(3/\delta)}{2}} \right) \\
  \frac{1}{n} \|\opX_3^*(\eta_3)\|_\op 
      &\le \frac{(8L^3 R^3 + 3 L^2 R^2 S + 6 L R S^2 + 2S^3) R^3}{\sqrt{n}} \left( 1 + \sqrt{\frac{\log(6/\delta)}{2}} \right) \\
  &\quad + 3 R^4 S^2 \left(\frac{4R(2LR+S)}{\sigmamin(\Sigma_1) \sqrt{n}} \left(1 + \sqrt{\frac{\log(6/\delta)}{2}} \right) \right).
\end{align*}
\end{lemma}

It follows that, with probability at least $1-\delta$,
\begin{align*}
  \frac{1}{n} \|\opX_p^*(\eta_p)\|_\op
  &= O\left( L^p S^p R^{2p} \sigmamin(\Sigma_1)^{-1} \sqrt{\frac{\log(1/\delta)}{n}} \right),
\end{align*}
for each $p \in \{1,2,3\}$.

\begin{proof}
Let $\hat\E_p[f(x,\epsilon,h)]$ denote the empirical expectation over
the examples in dataset $\sD_p$ (recall the $\sD_p$'s are independent to
simplify the analysis).  By definition,
$$\frac1n \|\opX_p^*(\eta_p)\|_\op = \left\| \hat\E_p[\eta_p(x) x\tp{p}] \right\|_\op $$
for $p \in \{1,2,3\}$. To proceed, we will bound each $\eta_p(x)$, defined in \refeqn{eta1},
\refeqn{eta2} and \refeqn{eta3} and use \reflem{conc-norms} to bound $\|
\hat\E_p[\eta_p(x) x\tp{p}] \|_F$. The Frobenius norm to bounds the
operator norm, completing the proof.

%composed of several zero-mean random variables, and
%since $\|A + B\|_\op \le \|A\|_\op + \|B\|_\op$, it suffices to consider
%each term in turn. 

\paragraph{Bounding $\eta_p(x)$.}
Using the assumptions that $\|\beta_h\|_2 \le L$, $\|x\|_2 \le R$ and
$|\epsilon| \le S$, it is easy to bound each $\eta_p(x)$,
\begin{align*}
  \eta_1(x) &= \innerp{\beta_h - M_1}{x} + \epsilon \\
            &\le \|\beta_h - M_1\|_2 \|x\|_2 + |\epsilon| \\
            &\le 2LR + S \\
  \eta_2(x) 
    &= \innerp{\beta_h\tp{2} - M_2}{x\tp{2}} + 2 \epsilon \innerp{\beta_h}{x} + (\epsilon^2 - \E[\epsilon^2]) \\
    &\le \|\beta_h\tp{2} - M_2\|_F \|x\tp{2}\|_F + 2 |\epsilon| \|\beta_h\|_2\|x\|_2 + |\epsilon^2 - \E[\epsilon^2]| \\
    &\le (2L)^2 R^2 + 2 S L R + (2S)^2 \\
  \eta_3(x) &= \innerp{\beta_h\tp{3} - M_3}{x\tp{3}}
        + 3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}} \\
        &\quad + 3\left(\epsilon^2 \innerp{\beta_h}{x} - \E[\epsilon^2] \innerp{\hat M_1}{x}\right)
        + (\epsilon^3 - \E[\epsilon^3]) \\
  &\le \|\beta_h\tp{3} - M_3\|_F\|x\tp{3}\|_F
        + 3 |\epsilon| \|\beta_h\tp{2}\|_F \|x\tp{2}\|_F  \\
        &\quad + 3 \left( |\epsilon^2|~\|\beta_h\|_F\|x\|_F + \left|\E[\epsilon^2]\right|~\|\hat M_1\|_2\|x\|_2 \right)
        + |\epsilon^3| + \left| \E[\epsilon^3] \right| \\
  &\le (2L)^3 R^3 + 3 S L^2 R^2 + 3 ( S^2 L R + S^2 L R ) + 2S^3.
\end{align*}
We have used inequality $\|M_1 - \beta_h\|_2 \le 2L$ above. 

\paragraph{Bounding $\left\| \hat\E[\eta_p(x)x\tp{p}] \right\|_F$.}
We may now apply the above bounds on $\eta_p(x)$ to bound $\|\eta_p(x) x\tp{p}\|_F$, using the fact that $\|c X\|_F \le c\|X\|_F$.
By \reflem{conc-norms}, each of the following holds with probability at least $1-\delta_1$,
\begin{align*}
    \left\|\hat\E_1[\eta_1(x) x] \right\|_2
    &\le \frac{2R (2LR + S)}{\sqrt{n}} \left( 1 + \sqrt{\frac{\log(1/\delta_1)}{2}} \right) \\
  \left\|\hat\E_2[\eta_2(x) x\tp{2}] \right\|_F
      &\le \frac{(4L^2 R^2 + 2 S L R + 4S^2)R^2}{\sqrt{n}} \left( 1 + \sqrt{\frac{\log(1/\delta_2)}{2}} \right) \\
  \left\|\hat\E_3[\eta_3(x) x\tp{3}] - \E[\eta_3(x) x\tp{3} \mid x] \right\|_F
      &\le \frac{(8L^3 R^3 + 3 L^2 R^2 S + 6 L R S^2 + 2S^3) R^3}{\sqrt{n}} \left( 1 + \sqrt{\frac{\log(1/\delta_3)}{2}} \right).
\end{align*}

Recall that $\eta_3(x)$ does not have zero mean, so we must bound the bias:
\begin{align*}
  \| \E[\eta_3(x) x\tp{3} \mid x] \|_F &= \|3 \E[\epsilon^2] \innerp{M_1 - \hat M_1}{x} x\tp{3} \|_F \\
    &\le 3 \E[\epsilon^2] \|M_1 - \hat M_1\|_2 \|x\|_2 \|x\tp{3}\|_F.
\end{align*}
Note that in all of this, both $\hat M_1$ and $M_1$ are treated as
constants. Further, by applying \reflem{lowRank}, we have a bound on $\|M_1 - \hat
M_1\|_2$. So, with probability at least $1 - \delta_3$,
\begin{align*}
  \| \E[\eta_3(x) x\tp{3} \mid x] \|_F
  &\le 3 R^4 S^2 \left(\frac{4R(2LR+S)}{\sigmamin(\Sigma_1) \sqrt{n}} \left(1 + \sqrt{\frac{\log(1/\delta_3)}{2}} \right) \right).
\end{align*}

% {%
% \begin{align*}
% %   \| \hat\E_1[ \eta_1(x) x ] \|_\Lop &\le 
% %             \underbrace{ \|\hat\E_1[\innerp{M_1 - \beta_h}{x} x]\|_\op  }_{ O( L R^2     \sqrt{\frac{\log(1/\delta_1)}{n}} ) } +
% %             \underbrace{ \|\hat\E_1[\epsilon x]\|_\op                   }_{ O( R S  \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
%   %  &= O( S L R^2 \sqrt{\frac{\log(1/\delta_1)}{n}} ) \\
%     \| \hat\E_2[ \eta_2(x) x\tp{2} ] \|_\Lop &\le 
%             \underbrace{ \|\hat\E_2[\innerp{\beta_h\tp{2} - M_2}{x\tp{2}} x\tp{2}]\|_\op }_{ O( L^2 R^4      \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
%    &\quad + \underbrace{ \|\hat\E_2[2 \epsilon \innerp{\beta_h}{x} x\tp{2}]\|_\op        }_{ O( S L R^3 \sqrt{\frac{\log(1/\delta_1)}{n}} ) } 
%           + \underbrace{ \|\hat\E_2[(\epsilon^2 - S^2) x\tp{2}]\|_\op               }_{ O( S^2 R^2 \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
%   %&= O( S^2 L^2 R^4 \sqrt{\frac{\log^2(1/\delta_1)}{n}} ) \\
%   \| \hat\E_3[ \eta_3(x) x\tp{3} ] \|_\Lop &\le 
%             \underbrace{ \|\hat\E_3[\innerp{\beta_h\tp{3} - M_3}{x\tp{3}}                      x\tp{3}]\|_\op }_{O( L^3 R^6        \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
%    &\quad + \underbrace{ \|\hat\E_3[3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}}                 x\tp{3}]\|_\op }_{O( S L^2 R^5 \sqrt{\frac{\log(1/\delta_1)}{n}} ) } 
%           + \underbrace{ \|\hat\E_3[\epsilon^3                                                 x\tp{3}]\|_\op }_{O( S^3 R^3   \sqrt{\frac{\log (1/\delta_1)}{n}} ) } \\
%    &\quad + \underbrace{ \|\hat\E_3[3(\epsilon^2 \innerp{\beta_h}{x} -S^2 \innerp{\hat M_1}{x}) x\tp{3}]\|_\op }_{O( (S^2 L R^4 + S^2 R^4) \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
%   %&= O( S^3 L^3 R^6 \sqrt{\frac{\log^3(1/\delta_1)}{n}} ).
% \end{align*}
% }
% There are two additional considerations when bounding the terms in $\eta_3$.
% First, to bound $\hat\E_3[\epsilon^3]$, we use Lemma 19 of \cite{hsu13spherical}.
% Second, there is an additional bias since the estimator uses $\hat M_1$ instead of $M_1$; this bias is small by the previous bound (and is constructed on an independent dataset, $\sD_1$).
% Therefore, we incur another $O(S^2 \cdot R^4 \sqrt{\frac{\log(1/\delta_1)}{n}})$.
%We can handle the other terms using conventional means, the details of which can be found in \appendixref{sec:proofs}.
%Note that we are bounding quantities fairly crudely for the sake of simplicity.

Finally, taking $\delta_1 = \delta/3, \delta_2 = \delta/3, \delta_3
= \delta/6$, and taking the union bound over the bounds for $p \in
\{1,2,3\}$, we get our result.
\end{proof}

%%%%%%%%%%%%

\section{Tensor Decomposition}

Once we have estimated the moments from the data through regression, we apply the robust tensor eigen-decomposition algorithm to recover the parameters, $\beta_h$ and $\pi$. However, the algorithm is guaranteed to work only for symmetric matrices with (nearly) orthogonal eigenvectors, so as a first step, we will need to whiten the third-order moment tensor using the second moments. Once we get the eigenvalues and eigenvectors from this orthogonal tensor, we have to undo the transformation by applying an un-whitening step. In this section, we present error bounds for each step, and combine them to prove the following lemma,
\begin{lemma}[Tensor Decomposition with Whitening]
  \label{lem:tensorPower} 
  Let $M_3 = \sum_{h=1}^{k} \pi_h \beta_h\tp{3}$.
  Let $\|\hat M_2 - M_2\|_\op$ and
  $\|\hat M_3 - M_3\|_\op$ both be less than
  \begin{align*}
    \frac{3\sigma_k(M_2)^{3/2}}{10 k \pi_{\max}^{5/2}
    \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)}~ \epsilon,
  \end{align*}
  and,
\begin{align*}
  \frac{\sigma_k(M_2)}
    {\|M_2\|_\op^{1/2} \left(
    4\sqrt{3/2} + 8 k \pi_{\max} 
    \sigma_k(M_2)^{-1/2}
    \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right) \right) }~ \epsilon,
\end{align*}
for some $\epsilon$ such that 
\begin{align}
  \epsilon &\le 
    \min\Bigg\{
      \Bigg( 4\sqrt{3/2} \|M_2\|_\op^{1/2} \sigma_k(M_2)^{-1} \aerr{M_2} \\
     &\quad + 8 \|M_2\|_\op^{1/2} k \pi_{\max} \sigma_k(M_2)^{-3/2}
        \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right) \Bigg) \frac{\sigma_k(M_2)}{2}, \\
  &\quad \left( \frac{2 \pi_{\max}^{3/2}}{3} 5 k \pi_{\max} 
    \sigma_k(M_2)^{-3/2}
    \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)  \right) \frac{\sigma_k(M_2)}{2}, \\
      &\quad \frac{1}{2\sqrt{\pi_{\max}}} \\
   &\quad \Bigg\}.
\end{align}

  Then, there exists a permutation of indices such that  the parameter
  estimates found in step 2 of \citet[Algorithm 1]{ChagantyLiang2013}
  satisfy the following with probability at least $1 - \delta$,
  \begin{align*}
  \|\hat \pi - \pi \|_{\infty} &\le \epsilon \\
  \|\hat \beta_h - \beta_h\|_2 &\le \epsilon.
  \end{align*}
  for all $h \in [k]$.
\end{lemma}

\begin{proof}
We will use the general notation, $\aerr{X} \eqdef \|\hat X - X\|_\Lop$
to represent the error of the estimate, $\hat X$, of $X$ in the operator
norm. 
\paragraph{Step 1: Whitening}
Let $W$ and $\hat W$ be the whitening matrices for $M_2$ and $\hat M_2$
respectively. Also define $\Winv$ and $\Whinv$ to be their
pseudo-inverses.

We will first show that the whitened tensors $T = M_3(W,W,W)$ and $\hat
T = \hat M_3(\hat W, \hat W, \hat W)$ are symmetric with orthogonal
eigenvectors. Recall that $M_2 = \sum_h \pi_h \beta_h\tp{2}$, and thus
$W \beta_h = \frac{v_h}{\sqrt{\pi_h}}$, where $v_h$ form an orthonormal
basis. Applying the whitening transform to $M_3$, we get, 
\begin{align}
  M_3 &= \sum_h \pi_h \beta_h\tp{3} \\
  M_3(W,W,W) &= \sum_h \pi_h (W\beta_h)\tp{3} \\
  &= \sum_h \frac{1}{\sqrt{\pi_h}} v_h\tp{3}.
\end{align}
Consequently, $T$ has orthogonal eigenvectors, with eigenvalues $1/\sqrt{\pi_h}$.

Let us now study how far $\hat T$ differs from $T$, in terms of the
errors of $M_2$ and $M_3$. To do so, we use the triangle inequality to
break the difference into a number of simple terms,
\begin{align*}
  \aerr{T} &= \|M_3(W,W,W) - \hat M_3(\hat W,\hat W, \hat W)\|_\op \\
           &\le 
           \| {M_3}(W,W,W) - {M_3}(W,W,\hat W) \|_\op
           + \| {M_3}(W,W,\hat W) - {M_3}(W, \hat W, \hat W)\|_\op \\
           &\quad 
           + \|{M_3}(W,\hat W,\hat W) - {M_3}(\hat W, \hat W, \hat W)\|_\op 
           + \|{M_3}(\hat W,\hat W,\hat W) - - \hat {M_3}(\hat W,\hat W, \hat W)\|_\op \\
           &\le
           \| {M_3} \|_\op \|W\|^2_\op \aerr{W} +
            \| {M_3} \|_\op \|\hat W\|_\op \|W\|_\op \aerr{W} +
            \| {M_3} \|_\op \|\hat W\|^2_\op \aerr{W} +
            \aerr{M_3} \|\hat W\|^3_\op  \\
           &\le
           \| {M_3} \|_\op (\|W\|^2_\op + \|\hat W\|_\op \|W\|_\op + \|\hat W\|^2_\op) \aerr{W} +
            \aerr{M_3} \|\hat W\|^3_\op 
\end{align*}
We can relate $\|\hat W\|$ and $\aerr{W}$ to $\aerr{M_2}$ using using
\refprop{white}. The conditions on $\aerr{M_2}$ imply that $\aerr{M_2}
< \sigma_k(M_2)/2$, giving us,
\begin{align*}
  \|\hat W\|_\op &\le \sqrt{2} \sigma_k(M_2)^{-1/2} \\
  \aerr{W} &\le 4 \sigma_k(M_2)^{-3/2} \aerr{M_2}.
\end{align*}
Thus,
\begin{align*}
  \aerr{T} &\le 
  6 \| {M_3} \|_\op \|W\|^2_\op (4 \sigma_k(M_2)^{-3/2}) \aerr{M_2} +
  \aerr{M_3} 2\sqrt{2} \|W\|^3_\op \\
  &\le 
  24 \| {M_3} \|_\op \sigma_k(M_2)^{-5/2} \aerr{M_2} +
  2\sqrt{2} \sigma_k(M_2)^{-3/2} \aerr{M_3} \\
  &\le 
    \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)
      \max\{ \aerr{M_2}, \aerr{M_3} \}.
\end{align*}

\paragraph{Step 2: Decomposition}

We have constructed $T$ to be a symmetric tensor with orthogonal
eigenvectors. We can now apply the results of \citet[Theorem
5.1]{AnandkumarGeHsu2012} to bound the error in the eigenvalues,
$\lambda_W$, and eigenvectors, $\omega$, returned by the robust tensor
power method;
\newcommand{\lW}{\lambda_W}
\newcommand{\lhW}{{\hat\lambda}_W}
\newcommand{\mW}{\omega}
\newcommand{\mhW}{{\hat\omega}}
\begin{align}
  \|\lW - \lhW \|_{\infty} 
  &\le \frac{5 k \aerr{T}}{(\lW)_{\min}} \\
\|\mW_h -\mhW_h \|_2 
&\le \frac{8 k \aerr{T}}{(\lW)_{\min}^2},
\end{align}
for all $h \in [k]$, where $(\lW)_{min}$ is the smallest
eigenvalue of $T$. 

\paragraph{Step 3: Unwhitening}

Finally, we need to invert the whitening transformation to recover $\pi$ and
$\beta_h$ from $\lW$ and $\mW_h$. Let us complete the proof by
studying how this inversion relates the error in $\pi$ and $\beta$ to
the error in $\lW$ and $\mW$.

First, we will bound the error in the $\beta$s,
\begin{align*}
  \|\hat \beta_h - \beta_h\|_2
  &= \| \Whinv \mhW - \Winv \mW \|_2 \\
  &\le \aerr{\Winv} \|\mhW_h\|_2 + \|\Winv\|_2 \|\mhW_h - \mW_h \|_2. \comment{Triangle inequality}
\end{align*}

Once more, we can apply the results of \refprop{white}, with the assumptions on $\aerr{M_2}$, to get, 
\begin{align*}
  \|\Whinv\|_\op &\le \sqrt{3/2} \|M_2\|_\op^{1/2} \\
  \aerr{\Winv} &\le 4\sqrt{3/2} \|M_2\|_\op^{1/2} \sigma_k(M_2)^{-1} \aerr{M_2}.
\end{align*}

Thus,
\begin{align*}
  \|\hat \beta_h - \beta_h\|_2
  &\le 4\sqrt{3/2} \|M_2\|_\op^{1/2} \sigma_k(M_2)^{-1} \aerr{M_2} 
    + 8 \|M_2\|_\op^{1/2} \frac{k \aerr{T}}{(\lW)_{min}^2} \\
    &\le 4\sqrt{3/2} \|M_2\|_\op^{1/2} \sigma_k(M_2)^{-1} \aerr{M_2} \\
  &\quad + 8 \|M_2\|_\op^{1/2} k \pi_{\max} 
    \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)
      \max\{\aerr{M_2}, \aerr{M_3}\}.
\end{align*}

Next, let us bound the error in $\pi$,
\begin{align*}
  |\hat \pi_h - \pi_h |
  &= \left| \frac{1}{(\lW)_h^2} - \frac{1}{(\lhW)_h^2} \right| \\
  &= \left| \frac{\left( (\lW)_h + (\lhW)_h \right) \left( (\lW)_h - (\lhW)_h \right)}
  {(\lW)_h^2(\lhW)_h^2} \right| \\
  &\le \frac{( 2(\lW)_h - \|\lW - \lhW\|_{\infty} )}{(\lW)_h^2 \left( (\lW)_h + \|\lW - \lhW\|_{\infty} \right)^2} \|\lW - \lhW\|_{\infty}.
\end{align*}
Recall that $(\lW)_h = \pi_h^{-1/2}$, so the
assumptions that $\epsilon$ imply that $\|\lW - \lhW \|_{\infty} \le
(\lW)_{\min}/2$. This allows us to simplify the above expression as
follows,
\begin{align*}
  |\hat \pi_h - \pi_h |
  &\le \frac{(3/2)(\lW)_h}{(3/2)^2 (\lW)_h^4}
  \|\lW - \lhW\|_{\infty} \\
  &\le \frac{2}{3(\lW)_h^3} \frac{5 k \aerr{T}}{(\lW)_{\min}^2} \\
  &\le \frac{2 \pi_{\max}^{3/2}}{3} 5 k \pi_{\max} 
    \sigma_k(M_2)^{-3/2}
    \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right) \max\{\aerr{M_2}, \aerr{M_3}\}.
\end{align*}

We complete the proof by requiring that the bounds $\aerr{M_2}$ and
$\aerr{M_3}$ imply that $\|\hat \pi - \pi \|_{\infty} \le \epsilon$ and
$\|\hat \beta_h - \beta_h\|_2 \le \epsilon$, i.e.
\begin{align*}
  \max\{\aerr{M_2}, \aerr{M_3}\} &\le 
  \frac{3\sigma_k(M_2)^{3/2}}{10 k \pi_{\max}^{5/2}
  \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)}~ \epsilon,
\end{align*}
as well as,
\begin{align*}
  \max\{\aerr{M_2}, \aerr{M_3}\} 
  &\le 
  \frac{\sigma_k(M_2)}
    {\|M_2\|_\op^{1/2} \left(
    4\sqrt{3/2} + 8 k \pi_{\max} 
    \sigma_k(M_2)^{-1/2}
    \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right) \right) }~ \epsilon.
\end{align*}


\end{proof}

\section{Basic Lemmas}

\begin{lemma}[Concentration of vector norms]
  \label{lem:conc-norms}
  Let $X, X_1, \cdots, X_n \in \Re^d$ be i.i.d.\ samples
  from some distribution with bounded support
  ($\|X\|_2 \le M$ with probability 1).
  Then with probability at least $1 - \delta$,
  \begin{align}
    \left\| \frac{1}{n} \sum_{i=1}^{n} X_i - \E[X] \right\|_2 &
    \le \frac{2M}{\sqrt{n}} \left(1 + \sqrt{\frac{\log(1/\delta)}{2}}\right).
  \end{align}
\end{lemma}
\begin{proof}
  Define $Z_i = X_i - \E[X]$.

%  First, $\|Z_i\|_2 \le \|X_i\|_2 + \|\E[X_i]\|_2 \le 2M$,
%  where the first inequality is due to the triangle inequality,
%  and the second follows by Jensen's inequality on $\|\cdot\|_2$
%  and the boundedness assumption on $X_i$.

The quantity we want to bound can be expressed as follows:
  \begin{align}
  f(Z_1, Z_2, \cdots, Z_n) = \left\| \frac1n \sum_{i=1}^n Z_i \right\|_2.
  \end{align}

Let us check that $f$ satisfies the bounded differences inequality:
  \begin{align}
|f(Z_1, \cdots, Z_i, \cdots, Z_n) - f(Z_1, \cdots, Z_i', \cdots, Z_n)|
& \le \frac1n \|Z_i - Z_i'\|_2 \\
& = \frac1n \|X_i - X_i'\|_2 \\
&\le \frac{2M}{n},
  \end{align}
  by the bounded assumption of $X_i$ and the triangle inequality.

By McDiarmid's inequality,
with probability at least $1 - \delta$,
we have:
\begin{align}
\Pr[f - \E[f] \ge \epsilon] \le
\exp\left(\frac{-2 \epsilon^2}{\sum_{i=1}^n (2M/n)^2}\right).
\end{align}
Re-arranging:
\begin{align}
  \left\|\frac{1}{n}\sum_{i=1}^n Z_i\right\|_2
  &\le \E\left[ \left\| \frac{1}{n} \sum_{i=1}^n Z_i \right\|_2 \right]
  + M\sqrt{\frac{2\log(1/\delta)}{n}}.
\end{align}

Now it remains to bound $\E[f]$.
By Jensen's inequality, $\E[f] \le \sqrt{\E[f^2]}$,
so it suffices to bound $\E[f^2]$:
\begin{align}
  \E\left[ \frac1{n^2} \left\| \sum_{i=1}^n Z_i \right\|^2 \right]
  &= \E\left[ \frac1{n^2} \sum_{i=1}^n \|Z_i\|_2^2 \right] +
\E\left[ \frac1{n^2} \sum_{i\neq j} \innerp{Z_i}{Z_j} \right] \\ 
& \le \frac{4M^2}{n} + 0,
\end{align}
where the cross terms are zero by independence of the $Z_i$'s.

Putting everything together, we obtain the desired bound:
\begin{align}
\left\|\frac{1}{n}\sum_{i=1}^n Z_i \right\|
&\le \frac{2M}{\sqrt{n}} + M \sqrt{\frac{2\log(1/\delta)}{n}}.
\end{align}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Remark}: The above result can be directly
applied to the Frobenius norm of a matrix
$M$ because $\|M\|_F = \|\vvec(M)\|_2$.

\begin{proposition}[Perturbation Bounds on Whitening Matrices]
  \label{prop:white}
  Let $A$ be a rank-k $d\times d$ matrix, $\Wp$ be a $d \times k$ matrix that
  whitens $\hat A$, i.e. $\Wp^T \Ap \Wp = I$.  Suppose $\Wp^T A \Wp
  = U D U^T$, then define $W = \hat{W} U D^{-\half} U^T$. Note that $W$
  is also a $d \times k$ matrix that whitens $A$. Let $\serr{A}
  = \frac{\aerr{A}}{\sigma_k(A)}$. 

  Then, 
  \begin{align*}
    \|\hat W\|_\op 
      &\le \frac{\|W\|_\op}{\sqrt{1 - \serr{A}} } \\
  \|\Whinv\|_\op 
    &\le \|\Winv\|_\op \sqrt{1 + \serr{A}} \\
    \aerr{W} 
      &\le 2 \|W\|_\op \frac{\serr{A}}{1 - \serr{A}} \\
    \aerr{\Winv} 
      &\le 2 \|\Winv\|_\op \sqrt{1 + \serr{A}} \frac{\serr{A}}{1 - \serr{A}}.
  \end{align*}
\end{proposition}
\begin{proof}
  First, note that for a matrix $W$ that whitens $A = V \Sigma V^T$,
  $W = V \Sigma^{-\half} V^T$ and $\Winv = V \Sigma^{-\half} V^T$.
  This allows us to bound the operator norms of $\hat W$ and $\Whinv$ in
  terms of $W$ and $\Winv$,
  \begin{align*}
    \|\hat W\|_\op &= \frac{1}{\sqrt{\sigma_k(\hat A)}} \\
    &\le \frac{1}{\sqrt{\sigma_k({A}) - \aerr{A}} } \comment{By Weyl's Theorem} \\
    &\le \frac{\|W\|_\op}{\sqrt{1 - \serr{A}} } \\
    \|\Whinv\|_\op &= \sqrt{\sigma_1(\hat A)} \\
    &\le \sqrt{\sigmamax({A}) + \aerr{A}}  \comment{By Weyl's Theorem} \\
    &\le \sqrt{1 + \serr{A}} \|\Winv\|_\op.
  \end{align*}

  To find $\aerr{W}$, we will exploit the rotational invariance of the operator norm. 
  \begin{align*}
    \aerr{W} &= \| \Wp - W \|_\op \\
    &= \| W U D^{\half} U^T - W \|_\op  \comment{$W = U D^{-\half} U^T$}\\
    &\le \|W\|_\op \| I - U D^{\half} U^T \|_\op \comment{Sub-multiplicativity}\\
    &\le \|W\|_\op \| I - D \|_\op \\
    &= \|W\|_\op \| I - U D U^T \|_\op \comment{Rotational invariance}\\
    &\le \|W\|_\op \| \Wp^T \Ap_k \Wp - \Wp^T A \Wp \|_\op \comment{By definition}\\
    &\le \|W\|_\op ( \| \Wp^T (\Ap_k - \Ap) \Wp \|_\op + \| \Wp^T (\Ap - A) \Wp \|_\op) \\
    &\le \|W\|_\op \|\Wp\|_\op^2 (\sigma_{k+1}(\Ap) + \aerr{A}) \\
    &\le 2 \|W\|_\op \|\Wp\|_\op^2 \aerr{A}  \comment{Since $\sigma_{k+1}(A) = 0$}\\
    &\le 2 \|W\|_\op \frac{\serr{A}}{1 - \serr{A}} \comment{Using bound on $\|\hat W\|_\op$}.
  \end{align*}

  Similarly, we can bound the error on the un-whitening transform, $\Winv$,
  \begin{align*}
    \aerr{\pinv{W}} &= \| \pinv{\Wp} - \pinv{W} \|_\op \\
    &= \| \pinv{\Wp} U D^{\half} U^T - \pinv{W} \|_\op \\
    &\le \|\pinv{\Wp}\|_\op \| I - U D^{\half} U^T \|_\op \\
    &\le 2 \|\pinv{\Wp}\|_\op \|\Wp\|_\op^2 \aerr{A} \comment{From derivation of $\aerr{W}$}\\
    &\le 2 \|\Winv\|_\op \sqrt{1 + \serr{A}} \frac{\serr{A}}{1 - \serr{A}}.
  \end{align*}
\end{proof}



