\section{Spectral Experts Algorithm}
\label{sec:algo}

In this section, we outline our Spectral Experts algorithm
for recover the model parameters $\theta = (\pi, B, \sigma^2)$.
Our algorithm consists of two parts:
(i) low-rank regression to estimate certain symmetric tensors;
and (ii) tensor factorzation to recover the parameters.
% Motivate how we can recover $B$ from regression by lower order moments
%ideally from lower order moments.
%Conventionally, when using the method of moments, the
%moments of observed variables are expressed algebraically in terms of
%the model parameters. Using these equations, we attempt to solve for the
%model parameters using empirical estimates of the moments. 

%%% first moment
To warm-up, let us consider performing linear regression
on the response $y$ given $x$:
\begin{align}
  y &= \innerp{\bar \beta}{x} + (\innerp{\bar\beta - \beta_h}{x} + \epsilon),
\end{align}
where $\bar\beta \eqdef \sum_{h=1}^K \pi_h \beta_h$ is the average regression coefficient.
Here, there are two noise terms:
(i) the \emph{mixing noise} $\innerp{\bar\beta - \beta_h}{x}$
due to the choice of the mixture component $h$
and (ii) the \emph{observation noise} $\epsilon \sim \normal{0}{\sigma^2}$.
Performing regression on the data $\{\xni,
\yni\}_{i=1}^{N}$ would provide a consistent estimate of $\bar\beta$,
but this is clearly insufficient for identifying all the parameters $\theta$.

%%% second moment
Regressing on $x$ provides only first-order
information.  The key insight is that
since higher-order moments usually yield more information,
let us consider regressing $y^2$ on $x^{\otimes 2}$:
\begin{align}
  y^2 &= \innerp{M_2}{x\tp{2}} + \epsilon \innerp{\beta_h\tp{2} - M_2}{x\tp{2}} + \epsilon^2 \label{eq:y2}.
\end{align}

Let us consider some more low powers of $y$, noting
that taking the expectation gives us the corresponding low-order
moments,
\begin{align}
  y^2 &= \innerp{\beta_h\tp{2}}{x\tp{2}} + \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}} + \epsilon^2 \label{eq:y2} \\ 
  y^3 &= \innerp{\beta_h\tp{3}}{x\tp{3}} + 3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}} \notag \\
  &+ 3 \epsilon^2 \innerp{\beta_h}{x} + \epsilon^3. \label{eq:y3} 
\end{align}
Regression with the second order moments would give us $M_2 = \E[\beta_h
\tp{2}] = \sum_h \pi_h \beta_h\tp{2}$ which can identify $\pi$ and the
subspace spanned the $\beta_h$, but not the $\beta_h$ themselves. To see
this, note that the eigen-decomposition of $M_2$ is not unique.
\todo{explain better}.

This brings us to regression with the third order moments, giving us the
tensor $M_3 = \E[\beta_h \tp{3}] = \sum_h \pi_h \beta_h\tp{3}$.
\citet{AnandkumarGeHsu} show that these three moments suffice to
identify $\pi$ and $B$ and provide an efficient algorithm to recover the
parameters.

We note that the eigendecomposition of $M_2$ is not sufficient to
recover the $\beta_k$ because they are not necessarily orthogonal to
each other. \citet{AnandkumarGeHsu2012} describes an approach that uses
rotates $M_3$ to an orthogonal basis by using the whitening transform of
$M_2$. The eigenvectors and eigenvalues recovered from the
eigendecomposition of $M_3(W, W, W)$ can be de-whitened to recover the
$\beta_k$ and $\pi_k$.

% Describe the rest of the algorithm.
This description completes a sketch of the algorithm, described in
\algorithmref{algo:spectral-experts}. Going ahead, we have yet to show
that the regression is well-behaved which we will do in
\sectionref{sec:regression}. This is of concern because the regression
problem we have has variance introduced from component selection,
independent of any observation noise. We will show that we can indeed
efficiently recover $M_2$ and $M_3$ using ideas from low-rank
regression. Finally, we will outline the tensor power method to recover
$B$ and $\pi$ given these two quantities, $M_2$ and $M_3$ in
\sectionref{sec:tensor-power}. 

\cite{candesPhaseLift} regresses on the square of the response,
because that's the observation.

\begin{theorem}
  Recovery analysis of mixture of linear regressions.
\end{theorem}
\begin{proof}
\end{proof}

\begin{algorithm}[t]
  \caption{Spectral Experts}
  \label{algo:spectral-experts}
  \begin{algorithmic}[1]
    \REQUIRE $\mathcal{D} = \{ (\xn{1}, \yn{1}), \cdots, (\xn{N}, \yn{N}) \}$
    \REQUIRE $K$, the number of clusters, $\sigma^2$ 
    \STATE Recover $M_1$ using least-squares regression on $\mathcal{D}$.
    \STATE Recover $M_2, M_3$ using low-rank regression on $\mathcal{D}$.
    \STATE Return the eigenvectors $\beta_k$ and eigenvalues $\pi_k$ of $M_3$ using $M_2$.
  \end{algorithmic}
\end{algorithm}

We will conclude this section by
discussing some identifiability concerns for our algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Identifiability from moments}

The mixture of linear regressions model is identifiable subject to
permutations of the components\citationneeded. However, our approach
employs regression on $x\tp{2}$ and $x\tp{3}$, and thus requires
$\mathcal{X} \otimes \mathcal{X}$ and $\mathcal{X} \otimes \mathcal{X}
\otimes \mathcal{X}$ to be linearly independent modulo symmetry. We call
these condition {\em quadratic} and {\em cubic} independence.

This condition trivially holds when the space $\mathcal{X}$ consists of
independent vectors $\{ x_1, x_2, \cdots, x_d \}$. Unfortunately, it
places some constraints on non-linear featurizations. For example, the
common polynomial basis expansion, $1, x_1, x_1^2, \dots, x_1^p$ is
neither quadratically nor cubically independent because the product
space is strictly a subset of $\Re^{\frac{d (d-1)}{2}}$ since it
contains $x_1 \times x_1 = x_1^2 \times 1$, and $x_1 \times x_1 \times
x_1 = x_1^2 \times x_1 \times 1 = x_1^3 \times 1 \times 1$. In fact, the
number of unique terms in $x\tp{p}$ is only of the order of $p \times
d$, and hence we will not have enough equations to solve for $M_2$ or
$M_3$ unless we consider $p = d^2$ moments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Recovering $M_2$ and $M_3$}

% In the case of the mixture of linear regressions, we have two observed
% quantities, $x$ and $y$. Unfortunately, because $y$ is a scalar
% quantity, we have only one equation for every moment of $y$, and with
% this naive approach, would have to consider $O( (K+1) D)$ moments to
% recover $B \in \Re{K \times D}$ and $\pi \in \Re^K$. 

% To keep the discussion simple, we will consider the case where there is
% no noise, i.e.  $\sigma^2 = 0$. In this case, the moments of $y$ are,
% \begin{eqnarray*}
%   \E[ y ] &=& (\sum_{k=1}^{K} \pi_{k} \beta_k)^T x \\
%           &=& M_1^T \E[x] \\
%   \E[ y^2 ] &=& \sum_{k=1}^{K} \pi_{k} \E[(\beta_k^T x)]^2 \\
%   &=& \sum_{k=1}^{K} \pi_{k} (\beta_k\tp{2} \odot \E[x\tp{2}]) \\
%   &=& M_2 \odot \E[x\tp{2}] \\
%   \E[ y^p ] &=& M_p \odot \E[x\tp{p}].
% \end{eqnarray*}
% 
% We note that joint moments with $x$ will not help either; consider
% \begin{eqnarray}
%   \E[ y^p x^q ] &=& M_p \odot E[ x\tp{p+q} ].
% \end{eqnarray}
% The elements of $\E[ y^p x^q ]$ are linear combinations of $\E[y^p]$,
% and hence we do not get any new equations; \todo{This isn't exactly
% correct; should we even be presenting this line of reasoning?}. This is
% to be expected, since any information about the $\beta$s is really
% encoded in $y$, and not in the $X$.

