\section{Spectral Experts algorithm}
\label{sec:algo}

In this section, we describe our Spectral Experts algorithm
for estimating model parameters $\theta = (\pi, B)$.
The algorithm consists of two steps:
(i) low-rank regression to estimate certain symmetric tensors;
and (ii) tensor factorization to recover the parameters.
The two steps can be performed efficiently using
convex optimization and tensor power method, respectively.

%%% first moment
To warm up, let us consider linear regression
on the response $y$ given $x$.
From the model definition, we have $y = \beta_h^\top x + \epsilon$ where
$\epsilon \sim \normal{0}{\sigma^2}$, and $h$ is a random quantity
independent of $x$.
We can average over this randomness by defining
the average regression coefficients
$M_1 \eqdef \sum_{h=1}^k \pi_h \beta_h$.
Now we can express $y$ as a linear function of $x$ with coefficients $M_1$
plus some noise $\eta_1(x)$:
\begin{align}
  y &= \innerp{M_1}{x} +
  \underbrace{(\innerp{\beta_h - M_1}{x} + \epsilon)}_{\eqdef \eta_1(x)}. \label{eqn:y1}
\end{align}
The noise $\eta_1(x)$ is the sum of two terms:
(i) the \emph{mixing noise} $\innerp{M_1 - \beta_h}{x}$
due to the random choice of the mixture component $h$,
and (ii) the \emph{observation noise} $\epsilon \sim \normal{0}{\sigma^2}$.
Although the noise depends on $x$,
it still has zero mean conditioned on $x$.
We will later show that we can
perform linear regression on the data $\{\xni,
\yni\}_{i=1}^{n}$ to produce a consistent estimate of $M_1$
under identifiability conditions.
But clearly, knowing $M_1$ is insufficient
for identifying all the parameters $\theta$.

%%% second moments
Intuitively, performing regression on only $y$ given $x$ provides only first-order
information.  The key insight is that we can perform regression
on higher order powers to obtain more information about the parameters.
Specifically, for an integer $p \ge 1$, let us define the average
$p$-th order tensor power of the parameters as follows:
\begin{align}
M_p &\eqdef \sum_{h=1}^k \pi_h \beta_h\tp{p}. \label{eqn:Mp} % \in (\Re^{d})\tp{p}.
\end{align}
Now consider performing regression on $y^2$ given $x\tp{2}$.
Expanding $y^2 = (\innerp{\beta_h}{x} + \epsilon)^2$,
using the fact that $\innerp{\beta_h}{x}^p = \innerp{\beta_h\tp{p}}{x\tp{p}}$,
we have:
\begin{align}
y^2 &= \innerp{M_2}{x\tp{2}} + \sigma^2 + \eta_2(x), \label{eqn:y2} \\
\eta_2(x) &= \innerp{\beta_h\tp{2} - M_2}{x\tp{2}} + 2 \epsilon \innerp{\beta_h}{x} + (\epsilon^2 - \sigma^2). \nonumber
\end{align}
Again, we have expressed $y^2$ has a linear function of $x\tp{2}$
with regression coefficients $M_2$, plus a known bias $\sigma^2$ and noise.\footnote{If $\sigma^2$ were not known,
we could treat it as another coefficient
to be estimated.  The coefficients $M_2$ and $\sigma^2$ can be estimated jointly
provided that $x$ does not already contain a bias ($x_j$ must be non-constant for every $j \in [d]$).}
Importantly, the noise has mean zero; 
in fact each of the three terms has mean zero
by (i) definition of $M_2$, (ii) independence of $\epsilon$ and $h$,
and (iii) the fact that $\E[\epsilon^2] = \sigma^2$.

Performing regression yields a consistent estimate of $M_2$,
but does not identify all the parameters $\theta$.
In particular, $B$ is only identified up to rotation:
if $B = [\beta_1 \mid \cdots \mid \beta_k]$ satisfies
$B \diag(\pi) B^\top = M_2$, then $(B Q) \diag(\pi) (Q B^\top)$
for any orthogonal matrix $Q$.

%%% third moment
Let us look to the third moment for additional information.
We can write $y^3$ as a linear function of $x\tp{3}$ with coefficients $M_3$,
a bias $3 \sigma^2 \innerp{M_1}{x}$ that can be estimated from \refeqn{y1}, and zero mean noise $\eta_3(x)$:
\begin{align}
y^3 &= \innerp{M_3}{x\tp{3}} + 3\sigma^2 \innerp{M_1}{x} + \eta_3(x), \label{eqn:y3} \\
\eta_3(x) &= \innerp{\beta_h\tp{3} - M_3}{x\tp{3}}
+ 3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}} \nonumber \\
&\quad + 3(\epsilon^2 \innerp{\beta_h}{x} - \sigma^2 \innerp{M_1}{x})
+ \epsilon^3. \nonumber
\end{align}
Performing regression yields estimates for $M_3$.
It turns out that knowledge of $M_2$ and $M_3$ are sufficient to recover
all the parameters.

\begin{algorithm}[t]
  \caption{Spectral Experts}
  \label{algo:spectral-experts}
  \begin{algorithmic}[1]
    \INPUT Three datasets $\mathcal{D}_r = \{ (\xn{1}, \yn{1}), \cdots, (\xn{n}, \yn{n}) \}$ for $r = 1, 2, 3$;
    %regularization strengths $\lambda_n^{(2)} = \frac{c_2}{\sqrt{n}}$, $\lambda_n^{(3)} = \frac{c_3}{\sqrt{n}}$;
    regularization strengths $\lambda_n^{(2)}$, $\lambda_n^{(3)}$;
    observation noise variance $\sigma^2$.
    \OUTPUT Parameters $\hat\theta = (\hat \pi, [\hat \beta_1 \mid \cdots \mid \hat \beta_k])$.
    \STATE Estimate compound parameters $M_2, M_3$ using \textbf{low-rank regression}:
    \begin{align}
      &\hat M_1 = \arg\min_{M_1} \label{eqn:estimateM1} \\
      &\quad\quad\frac{1}{2n} \sum_{(x,y) \in \sD_1} (\innerp{M_1}{x} - y)^2, \nonumber \\
      &\hat M_2 = \arg\min_{M_2} \quad \lambda_n^{(2)} \|M_2\|_* + \label{eqn:estimateM2} \\
      &\quad\quad\frac{1}{2n} \sum_{(x,y) \in \sD_2} (\innerp{M_2}{x\tp{2}} + \sigma^2 - y^2)^2, \nonumber \\
      &\hat M_3 = \arg\min_{M_3} \quad \lambda_n^{(3)} \|M_3\|_* + \label{eqn:estimateM3} \\
      &\quad\quad\frac{1}{2n} \sum_{(x,y) \in \sD_3} (\innerp{M_3}{x\tp{3}} + 3 \sigma^2\innerp{\hat M_1}{x} - y^3)^2. \nonumber
    \end{align}
    \STATE Estimate the parameters $\theta = (\pi, B)$ using \textbf{tensor factorization}:
    \begin{enumerate}
      \item [(a)] Compute whitening matrix $\hat W \in \Re^{d \times k}$ (such that $\hat W^\top
      \hat M_2 \hat W = I$) using SVD.
      \item [(b)] Compute the eigenvalues $\{\hat a_h\}_{h=1}^k$
      and eigenvectors $\{\hat v_h\}_{h=1}^k$
      of the whitened tensor $\hat M_3(\hat W, \hat W, \hat W) \in \Re^{k \times k \times k}$
      by using the robust tensor power method.
    \item [(c)] Set parameter estimates $\hat\pi_h = \frac{1}{\hat a_h^2}$
    and $\hat\beta_h = (\hat W^{\top})^\dagger (\hat a_h \hat v_h)$.
    \end{enumerate}
  \end{algorithmic}
\end{algorithm}


% Full algorithm
Now we are ready to state our full algorithm, which we call Spectral Experts
(\algorithmref{algo:spectral-experts}).
First, we perform three regressions to recover the compound parameters
$M_1$ \refeqn{y1},
$M_2$ \refeqn{y2}, and
$M_3$ \refeqn{y3}.
Since $M_2$ and $M_3$ both only have rank $k$,
we can use nuclear norm regularization
\cite{Tomioka2011,NegahbanWainwright2009}
to exploit this low-rank structure and improve our compound parameter estimates.
In the algorithm, the regularization strengths $\lambda_n^{(2)}$ and $\lambda_n^{(3)}$
are set to $\frac{c}{\sqrt{n}}$ for some constant $c$.
The resulting semidefinite program is a standard one which has received
much attention in recent years.
% We use a standard off-the-shelf SDP solver, CVX, to solve .
% We use a rather simple proximal gradient-based approach,
% in which the nuclear norm is handled in closed form by taking an SVD
% and soft-thresholding the singular values \cite{donoho95soft,cai10soft}.

% Tensor factorization
Having estimated the compound parameters $M_1$, $M_2$ and $M_3$, it
remains to recover the original parameters $\theta$.
\citet{AnandkumarGeHsu2012} showed that for $M_2$ and $M_3$ of
the forms in \refeqn{Mp}, it is possible to efficiently accomplish this.
Specifically, we first compute a whitening matrix $W$ based on the SVD of $M_2$
and use that to construct a tensor $T = M_3(W, W, W)$ whose factors are orthogonal.
We can use the robust tensor power method to compute all the
eigenvalues and eigenvectors of $T$, from which it is easy to recover
the parameters $\pi$ and $\{\beta_h\}$.

\paragraph{Remarks}

% Spectral
In recent years, there has a been substantial interest in ``spectral'' methods
for learning latent-variable models.  One line of work has
focused on observable operator models \cite{hsu09spectral}
in which the true parameters are not recovered, but a reparametrization is used
which enables prediction.
Another line of work is based on the method of moments and uses eigendecomposition of a certain tensor
to recover the parameters \cite{anandkumar12svd,anandkumar12moments,AnandkumarHsuKakade2012}.
Our work extends this line of work into the space of discriminative models,
which requires regression to obtain the desired tensor.

% Unmixing
In spirit, Spectral Experts bears some resemblance to the unmixing
algorithm for estimation of restricted PCFGs
\cite{hsu12identifiability}.
In that work, the observations (moments) provided a linear combination over
the compound parameters.  ``Unmixing'' involves solving for the compound
parameters by inverting a mixing matrix.  In this work,
each data point (appropriately transformed) provides a different noisy projection of
the compound parameters.

% Signal
The idea of performing low-rank regression on $y^2$ has been explored
in the context of signal recovery from magnitude measurements
\cite{candes11phaselift,ohlsson12phase}.
There, the actual observed response was $y^2$,
whereas in our case, we deliberately construct powers $y,y^2,y^3$
to identify the underlying parameters.

%\citet{AnandkumarGeHsu2012} describes an approach that uses
%rotates $M_3$ to an orthogonal basis by using the whitening transform of
%$M_2$. The eigenvectors and eigenvalues recovered from the
%eigendecomposition of $M_3(W, W, W)$ can be de-whitened to recover the
%$\beta_k$ and $\pi_k$.

% Describe the rest of the algorithm.
%This description completes a sketch of the algorithm, described in
%\algorithmref{algo:spectral-experts}. Going ahead, we have yet to show
%that the regression is well-behaved which we will do in
%\sectionref{sec:regression}. This is of concern because the regression
%problem we have has variance introduced from component selection,
%independent of any observation noise. We will show that we can indeed
%efficiently recover $M_2$ and $M_3$ using ideas from low-rank
%regression. Finally, we will outline the tensor power method to recover
%$B$ and $\pi$ given these two quantities, $M_2$ and $M_3$ in
%\sectionref{sec:tensor-power}. 

