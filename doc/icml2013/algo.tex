\section{Spectral Experts Algorithm}
\label{sec:algo}

In this section, we describe our Spectral Experts algorithm
for estimating model parameters $\theta = (\pi, B)$.
The algorithm consists of two steps:
(i) low-rank regression to estimate certain symmetric tensors;
and (ii) tensor factorzation to recover the parameters.
The two steps can be performed efficiently using
convex optimization and tensor power method, respectively.
% Motivate how we can recover $B$ from regression by lower order moments
%ideally from lower order moments.
%Conventionally, when using the method of moments, the
%moments of observed variables are expressed algebraically in terms of
%the model parameters. Using these equations, we attempt to solve for the
%model parameters using empirical estimates of the moments. 

%%% first moment
To warm up, let us consider linear regression
on the response $y$ given $x$.
From the model definition, we have $y = \beta_h^\top x + \epsilon$ where
$\epsilon \sim \normal{0}{\sigma^2}$, and $h$ is a random quantity
independent of $x$.
We can average over this randomness by defining
the average regression coefficients
$M_1 \eqdef \sum_{h=1}^k \pi_h \beta_h$.
Now we can express $y$ as a linear function of $x$ with coefficients $M_1$
plus some noise $\eta_1(x)$:
\begin{align}
  y &= \innerp{M_1}{x} +
  \underbrace{(\innerp{M_1 - \beta_h}{x} + \epsilon)}_{\eqdef \eta_1(x)}. \label{eqn:y1}
\end{align}
The noise $\eta_1(x)$ is the sum of two terms:
(i) the \emph{mixing noise} $\innerp{M_1 - \beta_h}{x}$
due to the random choice of the mixture component $h$,
and (ii) the \emph{observation noise} $\epsilon \sim \normal{0}{\sigma^2}$.
Although the noise depends on $x$,
it still has zero mean conditioned on $x$.
We will later show that we can
perform linear regression on the data $\{\xni,
\yni\}_{i=1}^{n}$ to produce a consistent estimate of $M_1$
under identifiability conditions.
But clearly, knowing $M_1$ is insufficient
for identifying all the parameters $\theta$.

%%% second moments
Intuitively, performing regression on only $y$ given $x$ provides only first-order
information.  The key insight is that we can perform regression
on higher order powers to obtain more information about the parameters.
Specifically, for an integer $p \ge 1$, let us define the average
$p$-th order tensor power of the parameters as follows:
\begin{align}
M_p &\eqdef \sum_{h=1}^k \pi_h \beta_h\tp{p}. \label{eqn:Mp} % \in (\Re^{d})\tp{p}.
\end{align}
Now consider performing regression on $y^2$ given $x\tp{2}$.
Expanding $y^2 = (\innerp{\beta_h}{x} + \epsilon)^2$,
using the fact that $\innerp{\beta_h}{x}^p = \innerp{\beta_h\tp{p}}{x\tp{p}}$,
we have:
\begin{align}
y^2 &= \innerp{M_2}{x\tp{2}} + \sigma^2 + \eta_2(x), \label{eqn:y2} \\
\eta_2(x) &= \innerp{\beta_h\tp{2} - M_2}{x\tp{2}} + 2 \epsilon \innerp{\beta_h}{x} + (\epsilon^2 - \sigma^2). \nonumber
\end{align}
Again, we have expressed $y^2$ has a linear function of $x\tp{2}$
with regression coefficients $M_2$, plus a known bias $\sigma^2$ and noise.\footnote{If $\sigma^2$ were not known,
we could treat it as another coefficient
to be estimated.  The coefficients $M_2$ and $\sigma^2$ can be estimated jointly
provided that $x$ does not already contain a bias ($x_j$ must be non-constant for every $j \in [d]$).}
Importantly, the noise has mean zero; 
in fact each of the three terms has mean zero
by (i) definition of $M_2$, (ii) independence of $\epsilon$ and $h$,
and (iii) the fact that $\E[\epsilon^2] = \sigma^2$.

Performing regression yields a consistent estimate of $M_2$,
but does not identify all the parameters $\theta$.
In particular, $B$ is only identified up to rotation:
if $B = [\beta_1 \mid \cdots \mid \beta_k]$ satisfies
$B \diag(\pi) B^\top = M_2$, then $(B Q) \diag(\pi) (Q B^\top)$
for any orthogonal matrix $Q$.

%%% third moment
Let us look to the third moment for additional information.
We can write $y^3$ as a linear function of $x\tp{3}$ with coefficients $M_3$,
a bias $3 \sigma^2 \innerp{M_1}{x}$ that can be estimated from \refeqn{y1}, and zero mean noise $\eta_3(x)$:
\begin{align}
y^3 &= \innerp{M_3}{x\tp{3}} + 3\sigma^2 \innerp{M_1}{x} + \eta_3(x), \label{eqn:y3} \\
\eta_3(x) &= \innerp{\beta_h\tp{3} - M_3}{x\tp{3}}
+ 3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}} \nonumber \\
&\quad + 3(\epsilon^2 \innerp{\beta_h}{x} - \sigma^2 \innerp{M_1}{x})
+ \epsilon^3. \nonumber
\end{align}
Performing regression yields estimates for $M_3$.
It turns out that knowledge of $M_2$ and $M_3$ are sufficient to recover
all the parameters.

% Full algorithm
Now we are ready to state our full algorithm, which we call Spectral Experts
(\algorithmref{sec:algo}).
First, we perform three regressions to recover the compound parameters
$M_1$ \refeqn{y1},
$M_2$ \refeqn{y2}, and
$M_3$ \refeqn{y3}.
Since $M_2$ and $M_3$ both only have rank $k$,
we can use nuclear norm regularization
\cite{Tomioka2011,NegahbanWainwright2009}
to exploit this low-rank structure and improve our compound parameter estimates.
In the algorithm, the regularization strengths $\lambda_n^{(2)}$ and $\lambda_n^{(3)}$
are set to $\frac{1}{\sqrt{n}}$.
The resulting semidefinite program is a standard one which has received
much attention in recent years.
We use a rather simple gradient-based approach,
where the nuclear norm is handled in closed form by taking an SVD
and soft-thresholding the eigenvalues \cite{donoho95soft,cai10soft}.

% Tensor factorization
Having estimated the compound parameters $M_2$ and $M_3$,
it remains to recover the original parameters $\theta$.
\citet{AnandkumarGeHsu2012} showed that for $M_2$ and $M_3$ of
the forms in \refeqn{Mp}, it is possible to efficiently accomplish this.
Specifically, we first compute a whitening matrix $W$ based on the SVD of $M_2$
and use that to construct a tensor $T = M_3(W, W, W)$ whose factors are orthogonal.
We can use the robust tensor power method to compute all the
eigenvalues and eigenvectors of $T$, from which it is easy to recover
the parameters $\pi$ and $\{\beta_h\}$.

\paragraph{Remarks}

% Spectral
In recent years, there has a been substantial interest in ``spectral'' methods
for learning latent-variable models.  One line of work has
focused on observerable operator models \cite{hsu09spectral}
in which the true parameters are not recovered, but a reparametrization is used
which enables prediction.
Another line of work is based on the method of moments and uses eigendecomposition of a certain tensor
to recover the parameters \cite{anandkumar12moments,anandkumar12svd,AnandkumarHsuKakade2012}.
Our work extends this line of work into the space of discriminative models,
which requires regression to obtain the desired tensor.

% Unmixing
In spirit, Spectral Experts also bears some resemblence to the unmixing
algorithm for estimation of restricted PCFGs
\cite{hsu12identifiability}.
In that work, the observations (moments) provided a linear combination over
the compound parameters.  ``Unmixing'' involves solving for the compound
parameters by inverting a mixing matrix.  In this work,
each data point (appropriately transformed) provides a noisy slice of the
compound parameters; retrieving the compound parameters is more involved.

% Signal
The idea of performing low-rank regression on $y^2$ has been explored
in the context of signal recovery from magnitude measurements
\cite{candes11phaselift,ohlsson12phase}.
There, the actual observed response was $y^2$,
whereas in our case, we delibrately construct powers $y,y^2,y^3$
to identify the parameters.

%\citet{AnandkumarGeHsu2012} describes an approach that uses
%rotates $M_3$ to an orthogonal basis by using the whitening transform of
%$M_2$. The eigenvectors and eigenvalues recovered from the
%eigendecomposition of $M_3(W, W, W)$ can be de-whitened to recover the
%$\beta_k$ and $\pi_k$.

% Describe the rest of the algorithm.
%This description completes a sketch of the algorithm, described in
%\algorithmref{algo:spectral-experts}. Going ahead, we have yet to show
%that the regression is well-behaved which we will do in
%\sectionref{sec:regression}. This is of concern because the regression
%problem we have has variance introduced from component selection,
%independent of any observation noise. We will show that we can indeed
%efficiently recover $M_2$ and $M_3$ using ideas from low-rank
%regression. Finally, we will outline the tensor power method to recover
%$B$ and $\pi$ given these two quantities, $M_2$ and $M_3$ in
%\sectionref{sec:tensor-power}. 

\begin{algorithm}[t]
  \caption{Spectral Experts}
  \label{algo:spectral-experts}
  \begin{algorithmic}[1]
    \INPUT Three independent datasets $\mathcal{D}_r = \{ (\xn{1}, \yn{1}), \cdots, (\xn{n}, \yn{n}) \}$ for $r = 1, 2, 3$;
    regularization strengths $\lambda_n^{(2)} = \frac{c_2}{\sqrt{n}}$, $\lambda_n^{(3)} = \frac{c_3}{\sqrt{n}}$;
    observation noise variance $\sigma^2$.
    \OUTPUT Parameters $\hat\theta = (\hat \pi, [\hat \beta_1 \mid \cdots \mid \hat \beta_k])$.
    \STATE Estimate compound parameters $M_2, M_3$ using \textbf{low-rank regression}:
    \begin{align}
      &\hat M_1 = \arg\min_{M_1} \label{eqn:estimateM1} \\
      &\quad\quad\frac{1}{2n} \sum_{(x,y) \in \sD_1} (\innerp{M_1}{x} - y)^2, \nonumber \\
      &\hat M_2 = \arg\min_{M_2} \quad \lambda_n^{(2)} \|M_2\|_* + \label{eqn:estimateM2} \\
      &\quad\quad\frac{1}{2n} \sum_{(x,y) \in \sD_2} (\innerp{M_2}{x\tp{2}} + \sigma^2 - y^2)^2, \nonumber \\
      &\hat M_3 = \arg\min_{M_3} \quad \lambda_n^{(3)} \|M_3\|_* + \label{eqn:estimateM3} \\
      &\quad\quad\frac{1}{2n} \sum_{(x,y) \in \sD_3} (\innerp{M_3}{x\tp{3}} + 3 \sigma^2\innerp{\hat M_1}{x} - y^3)^2. \nonumber
    \end{align}
    \STATE Estimate the parameters $\theta = (\pi, B)$ using \textbf{tensor factorzation}:
    \begin{enumerate}
      \item [(a)] Compute whitening matrix $\hat W \in \Re^{d \times k}$ (such that $\hat W^\top
      \hat M_2 \hat W = I$) using SVD.
      \item [(b)] Compute the eigenvalues $\{\hat a_h\}_{h=1}^k$
      and eigenvectors $\{\hat v_h\}_{h=1}^k$
      of the whitened tensor $\hat M_3(\hat W, \hat W, \hat W) \in \Re^{k \times k \times k}$
      by using the robust tensor power method.
    \item [(c)] Set parameter estimates $\hat\pi_h = \frac{1}{\hat a_h^2}$
    and $\hat\beta_h = (\hat W^{\top})^\dagger (\hat a_h \hat v_h)$.
    \end{enumerate}
  \end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical results}

%Define $\sM_p = \{ \sum_{h=1}^k \pi_h \beta_h\tp{p} : \pi_h \in \R^k, \beta_h
%\in \R^d \}$ be the set of all $p$-th order compound parameters.
%Let $\bM_p$ be the set of 

%Define the symmetric rank
%\begin{align}
%\E[\cvec(x\tp{p})\tp{2}] \succ 0.
%\end{align}

In this section, we conduct a theoretical analysis of Spectral Experts.
In particular, we prove that it enjoys consistent parameter estimates.
\begin{theorem}[Convergence of Spectral Experts]
  \label{thm:convergence}
Each dataset $\sD_p$ (for $p = 1, 2, 3$) consists of $n$ i.i.d. points drawn from the mixture
of linear regressions model with parameter $\theta^*$
(having three independent copies simplifies the analysis).
Assume that $\E[\cvec(x\tp{p})\tp{2}] \succ 0$ for each $p \in \{1,2,3\}$.
Assume $d \ge k$.
Let $\|x\|_2 \le R$ with probability $1$
and let $\|\beta_h\|_2 \le L$ for all $h \in [k]$.
There exists universal constants $c_1,c_2$ such that
if the number of samples is
$n \ge \frac{c_1 k d^2 L R}{\epsilon}$
and $\lambda_n^{(2)} \ge \frac{c_2 L R}{\sqrt{n}}$,
and $\lambda_n^{(3)} \ge \frac{c_3 L R}{\sqrt{n}}$,
then the parameter estimates $\hat\theta$ returned by
\algorithmref{algo:spectral-experts} (with the columns appropriate permuted)
satisfies 
$\|\hat\pi - \pi^*\|_{\infty} \le \frac{\epsilon}{???}$
and for all $h \in [k]$,
$\|\hat\beta_h - \beta^*_h\|_2 \le \epsilon ???$,
\end{theorem}
\begin{proof}
Let $T^* = M_3^*(W^*, W^*, W^*)$.
By Theorem 5.1 of \cite{AnandkumarGeHsu2012},
Need to convert error from $\epsilon$
hit by $(W^\top)^\dagger$
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Identifiability from moments}

In ordinary linear regression, the regression coefficients $\beta \in \Re^d$ are
identifiable if and only if the data has full
rank: $\E[x\tp{2}] \succ 0$,
and furthermore, identifying $\beta$ requires only moments
$\E[xy]$ and $\E[x\tp{2}]$ (by observing the optimality conditions for \refeqn{y1}).
However, in mixture of linear regressions, these two moments only allow us to recover $M_1$.
\refthm{convergence} shows that if we have the higher order analogues,
$\E[x\tp{p}y\tp{p}]$ and $\E[x\tp{2p}]$ for $p \in \{1,2,3\}$,
we can then identify the parameters $\theta = (\pi, B)$,
provided $\E[\cvec(x\tp{p})\tp{2}] \succ 0$ for $p \in \{1,2,3\}$.

We note that the positive definite constraints are more stringent.
For example, suppose $x = (1, t, t^2)$,
the common polynomial basis expansion, so that all the coordinates are deterministically related.
Then $\E[x\tp{2}] \succ 0$ can easily be satisfied, say with $x \sim \normal{0}{I}$.
However, one can verify that $\E[\cvec(x\tp{2})\tp{2}]$ is singular,
since $\cvec(x\tp{2}) = [1 \cdot 1, t\cdot t, 2(1 \cdot t^2), 2(t \cdot t^2), (t^2 \cdot t^2)]$ contains
components $t \cdot t$ and $2(1 \cdot t^2)$, which are linearly dependent.
So our method would not be able to identify the parameters of a mixture of
linear regressions for this distribution over $x$.

We now show that some amount of unidentifiability is intrinsic to the model,
not just our estimation procedure.
Suppose $x = (t, \dots, t^d)$.
Even if we observed all moments
$\E[x\tp{p}y\tp{p}]$ and $\E[x\tp{2p}]$ for $p \in [r]$,
all the coordinates would be monomials up to degree $2dr$,
the therefore the moments live in a $2(dr)$-dimensional subspace.
The parameters live in a subspace of at least dimension $dk$.
Therefore, at least $r \ge k/2$ moments are required for identifiability of any
algorithm for this monomial example.

%In fact, considering moments up to power $p
%Regressing on all powers up to $p$ yield at most $3$.

%$1, x_1, x_1^2, \dots, x_1^p$ is
%neither quadratically nor cubically independent because the product
%space is strictly a subset of $\Re^{\frac{d (d-1)}{2}}$ since it
%contains $x_1 \times x_1 = x_1^2 \times 1$, and $x_1 \times x_1 \times
%x_1 = x_1^2 \times x_1 \times 1 = x_1^3 \times 1 \times 1$.
%In fact, the
%number of unique terms in $x\tp{p}$ is only of the order of $p \times
%d$, and hence we will not have enough equations to solve for $M_2$ or
%$M_3$ unless we consider $p = d^2$ moments.

%The mixture of linear regressions model is identifiable subject to
%permutations of the components. However, our approach
%employs regression on $x\tp{2}$ and $x\tp{3}$, and thus requires
%$\mathcal{X} \otimes \mathcal{X}$ and $\mathcal{X} \otimes \mathcal{X}
%\otimes \mathcal{X}$ to be linearly independent modulo symmetry. We call
%these condition {\em quadratic} and {\em cubic} independence.

%This condition trivially holds when the space $\mathcal{X}$ consists of
%independent vectors $\{ x_1, x_2, \cdots, x_d \}$. Unfortunately, it
%places some constraints on non-linear featurizations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Recovering $M_2$ and $M_3$}

% In the case of the mixture of linear regressions, we have two observed
% quantities, $x$ and $y$. Unfortunately, because $y$ is a scalar
% quantity, we have only one equation for every moment of $y$, and with
% this naive approach, would have to consider $O( (k+1) D)$ moments to
% recover $B \in \Re{k \times D}$ and $\pi \in \Re^k$. 

% To keep the discussion simple, we will consider the case where there is
% no noise, i.e.  $\sigma^2 = 0$. In this case, the moments of $y$ are,
% \begin{eqnarray*}
%   \E[ y ] &=& (\sum_{k=1}^{k} \pi_{k} \beta_k)^T x \\
%           &=& M_1^T \E[x] \\
%   \E[ y^2 ] &=& \sum_{k=1}^{k} \pi_{k} \E[(\beta_k^T x)]^2 \\
%   &=& \sum_{k=1}^{k} \pi_{k} (\beta_k\tp{2} \odot \E[x\tp{2}]) \\
%   &=& M_2 \odot \E[x\tp{2}] \\
%   \E[ y^p ] &=& M_p \odot \E[x\tp{p}].
% \end{eqnarray*}
% 
% We note that joint moments with $x$ will not help either; consider
% \begin{eqnarray}
%   \E[ y^p x^q ] &=& M_p \odot E[ x\tp{p+q} ].
% \end{eqnarray}
% The elements of $\E[ y^p x^q ]$ are linear combinations of $\E[y^p]$,
% and hence we do not get any new equations; \todo{This isn't exactly
% correct; should we even be presenting this line of reasoning?}. This is
% to be expected, since any information about the $\beta$s is really
% encoded in $y$, and not in the $X$.

