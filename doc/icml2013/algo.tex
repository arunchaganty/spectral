\section{Spectral Experts Algorithm}
\label{sec:algo}

In this section, we describe our Spectral Experts algorithm
for estimating model parameters $\theta = (\pi, B)$.
The algorithm consists of two steps:
(i) low-rank regression to estimate certain symmetric tensors;
and (ii) tensor factorization to recover the parameters.
The two steps can be performed efficiently using
convex optimization and tensor power method, respectively.

%%% first moment
To warm up, let us consider linear regression
on the response $y$ given $x$.
From the model definition, we have $y = \beta_h^\top x + \epsilon$ where
$\epsilon \sim \normal{0}{\sigma^2}$, and $h$ is a random quantity
independent of $x$.
We can average over this randomness by defining
the average regression coefficients
$M_1 \eqdef \sum_{h=1}^k \pi_h \beta_h$.
Now we can express $y$ as a linear function of $x$ with coefficients $M_1$
plus some noise $\eta_1(x)$:
\begin{align}
  y &= \innerp{M_1}{x} +
  \underbrace{(\innerp{M_1 - \beta_h}{x} + \epsilon)}_{\eqdef \eta_1(x)}. \label{eqn:y1}
\end{align}
The noise $\eta_1(x)$ is the sum of two terms:
(i) the \emph{mixing noise} $\innerp{M_1 - \beta_h}{x}$
due to the random choice of the mixture component $h$,
and (ii) the \emph{observation noise} $\epsilon \sim \normal{0}{\sigma^2}$.
Although the noise depends on $x$,
it still has zero mean conditioned on $x$.
We will later show that we can
perform linear regression on the data $\{\xni,
\yni\}_{i=1}^{n}$ to produce a consistent estimate of $M_1$
under identifiability conditions.
But clearly, knowing $M_1$ is insufficient
for identifying all the parameters $\theta$.

%%% second moments
Intuitively, performing regression on only $y$ given $x$ provides only first-order
information.  The key insight is that we can perform regression
on higher order powers to obtain more information about the parameters.
Specifically, for an integer $p \ge 1$, let us define the average
$p$-th order tensor power of the parameters as follows:
\begin{align}
M_p &\eqdef \sum_{h=1}^k \pi_h \beta_h\tp{p}. \label{eqn:Mp} % \in (\Re^{d})\tp{p}.
\end{align}
Now consider performing regression on $y^2$ given $x\tp{2}$.
Expanding $y^2 = (\innerp{\beta_h}{x} + \epsilon)^2$,
using the fact that $\innerp{\beta_h}{x}^p = \innerp{\beta_h\tp{p}}{x\tp{p}}$,
we have:
\begin{align}
y^2 &= \innerp{M_2}{x\tp{2}} + \sigma^2 + \eta_2(x), \label{eqn:y2} \\
\eta_2(x) &= \innerp{\beta_h\tp{2} - M_2}{x\tp{2}} + 2 \epsilon \innerp{\beta_h}{x} + (\epsilon^2 - \sigma^2). \nonumber
\end{align}
Again, we have expressed $y^2$ has a linear function of $x\tp{2}$
with regression coefficients $M_2$, plus a known bias $\sigma^2$ and noise.\footnote{If $\sigma^2$ were not known,
we could treat it as another coefficient
to be estimated.  The coefficients $M_2$ and $\sigma^2$ can be estimated jointly
provided that $x$ does not already contain a bias ($x_j$ must be non-constant for every $j \in [d]$).}
Importantly, the noise has mean zero; 
in fact each of the three terms has mean zero
by (i) definition of $M_2$, (ii) independence of $\epsilon$ and $h$,
and (iii) the fact that $\E[\epsilon^2] = \sigma^2$.

Performing regression yields a consistent estimate of $M_2$,
but does not identify all the parameters $\theta$.
In particular, $B$ is only identified up to rotation:
if $B = [\beta_1 \mid \cdots \mid \beta_k]$ satisfies
$B \diag(\pi) B^\top = M_2$, then $(B Q) \diag(\pi) (Q B^\top)$
for any orthogonal matrix $Q$.

%%% third moment
Let us look to the third moment for additional information.
We can write $y^3$ as a linear function of $x\tp{3}$ with coefficients $M_3$,
a bias $3 \sigma^2 \innerp{M_1}{x}$ that can be estimated from \refeqn{y1}, and zero mean noise $\eta_3(x)$:
\begin{align}
y^3 &= \innerp{M_3}{x\tp{3}} + 3\sigma^2 \innerp{M_1}{x} + \eta_3(x), \label{eqn:y3} \\
\eta_3(x) &= \innerp{\beta_h\tp{3} - M_3}{x\tp{3}}
+ 3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}} \nonumber \\
&\quad + 3(\epsilon^2 \innerp{\beta_h}{x} - \sigma^2 \innerp{M_1}{x})
+ \epsilon^3. \nonumber
\end{align}
Performing regression yields estimates for $M_3$.
It turns out that knowledge of $M_2$ and $M_3$ are sufficient to recover
all the parameters.

% Full algorithm
Now we are ready to state our full algorithm, which we call Spectral Experts
(\algorithmref{algo:spectral-experts}).
First, we perform three regressions to recover the compound parameters
$M_1$ \refeqn{y1},
$M_2$ \refeqn{y2}, and
$M_3$ \refeqn{y3}.
Since $M_2$ and $M_3$ both only have rank $k$,
we can use nuclear norm regularization
\cite{Tomioka2011,NegahbanWainwright2009}
to exploit this low-rank structure and improve our compound parameter estimates.
In the algorithm, the regularization strengths $\lambda_n^{(2)}$ and $\lambda_n^{(3)}$
are set to $\frac{c}{\sqrt{n}}$ for some constant $c$.
The resulting semidefinite program is a standard one which has received
much attention in recent years.
We use a rather simple proximal gradient-based approach,
in which the nuclear norm is handled in closed form by taking an SVD
and soft-thresholding the singular values \cite{donoho95soft,cai10soft}.

% Tensor factorization
Having estimated the compound parameters $M_2$ and $M_3$,
it remains to recover the original parameters $\theta$.
\citet{AnandkumarGeHsu2012} showed that for $M_2$ and $M_3$ of
the forms in \refeqn{Mp}, it is possible to efficiently accomplish this.
Specifically, we first compute a whitening matrix $W$ based on the SVD of $M_2$
and use that to construct a tensor $T = M_3(W, W, W)$ whose factors are orthogonal.
We can use the robust tensor power method to compute all the
eigenvalues and eigenvectors of $T$, from which it is easy to recover
the parameters $\pi$ and $\{\beta_h\}$.

\paragraph{Remarks}

% Spectral
In recent years, there has a been substantial interest in ``spectral'' methods
for learning latent-variable models.  One line of work has
focused on observable operator models \cite{hsu09spectral}
in which the true parameters are not recovered, but a reparametrization is used
which enables prediction.
Another line of work is based on the method of moments and uses eigendecomposition of a certain tensor
to recover the parameters \cite{anandkumar12svd,anandkumar12moments,AnandkumarHsuKakade2012}.
Our work extends this line of work into the space of discriminative models,
which requires regression to obtain the desired tensor.

% Unmixing
In spirit, Spectral Experts bears some resemblance to the unmixing
algorithm for estimation of restricted PCFGs
\cite{hsu12identifiability}.
In that work, the observations (moments) provided a linear combination over
the compound parameters.  ``Unmixing'' involves solving for the compound
parameters by inverting a mixing matrix.  In this work,
each data point (appropriately transformed) provides a different noisy projection of
the compound parameters.

% Signal
The idea of performing low-rank regression on $y^2$ has been explored
in the context of signal recovery from magnitude measurements
\cite{candes11phaselift,ohlsson12phase}.
There, the actual observed response was $y^2$,
whereas in our case, we deliberately construct powers $y,y^2,y^3$
to identify the underlying parameters.

%\citet{AnandkumarGeHsu2012} describes an approach that uses
%rotates $M_3$ to an orthogonal basis by using the whitening transform of
%$M_2$. The eigenvectors and eigenvalues recovered from the
%eigendecomposition of $M_3(W, W, W)$ can be de-whitened to recover the
%$\beta_k$ and $\pi_k$.

% Describe the rest of the algorithm.
%This description completes a sketch of the algorithm, described in
%\algorithmref{algo:spectral-experts}. Going ahead, we have yet to show
%that the regression is well-behaved which we will do in
%\sectionref{sec:regression}. This is of concern because the regression
%problem we have has variance introduced from component selection,
%independent of any observation noise. We will show that we can indeed
%efficiently recover $M_2$ and $M_3$ using ideas from low-rank
%regression. Finally, we will outline the tensor power method to recover
%$B$ and $\pi$ given these two quantities, $M_2$ and $M_3$ in
%\sectionref{sec:tensor-power}. 

\begin{algorithm}[t]
  \caption{Spectral Experts}
  \label{algo:spectral-experts}
  \begin{algorithmic}[1]
    \INPUT Three independent datasets $\mathcal{D}_r = \{ (\xn{1}, \yn{1}), \cdots, (\xn{n}, \yn{n}) \}$ for $r = 1, 2, 3$;
    %regularization strengths $\lambda_n^{(2)} = \frac{c_2}{\sqrt{n}}$, $\lambda_n^{(3)} = \frac{c_3}{\sqrt{n}}$;
    regularization strengths $\lambda_n^{(2)}$, $\lambda_n^{(3)}$;
    observation noise variance $\sigma^2$.
    \OUTPUT Parameters $\hat\theta = (\hat \pi, [\hat \beta_1 \mid \cdots \mid \hat \beta_k])$.
    \STATE Estimate compound parameters $M_2, M_3$ using \textbf{low-rank regression}:
    \begin{align}
      &\hat M_1 = \arg\min_{M_1} \label{eqn:estimateM1} \\
      &\quad\quad\frac{1}{2n} \sum_{(x,y) \in \sD_1} (\innerp{M_1}{x} - y)^2, \nonumber \\
      &\hat M_2 = \arg\min_{M_2} \quad \lambda_n^{(2)} \|M_2\|_* + \label{eqn:estimateM2} \\
      &\quad\quad\frac{1}{2n} \sum_{(x,y) \in \sD_2} (\innerp{M_2}{x\tp{2}} + \sigma^2 - y^2)^2, \nonumber \\
      &\hat M_3 = \arg\min_{M_3} \quad \lambda_n^{(3)} \|M_3\|_* + \label{eqn:estimateM3} \\
      &\quad\quad\frac{1}{2n} \sum_{(x,y) \in \sD_3} (\innerp{M_3}{x\tp{3}} + 3 \sigma^2\innerp{\hat M_1}{x} - y^3)^2. \nonumber
    \end{align}
    \STATE Estimate the parameters $\theta = (\pi, B)$ using \textbf{tensor factorization}:
    \begin{enumerate}
      \item [(a)] Compute whitening matrix $\hat W \in \Re^{d \times k}$ (such that $\hat W^\top
      \hat M_2 \hat W = I$) using SVD.
      \item [(b)] Compute the eigenvalues $\{\hat a_h\}_{h=1}^k$
      and eigenvectors $\{\hat v_h\}_{h=1}^k$
      of the whitened tensor $\hat M_3(\hat W, \hat W, \hat W) \in \Re^{k \times k \times k}$
      by using the robust tensor power method.
    \item [(c)] Set parameter estimates $\hat\pi_h = \frac{1}{\hat a_h^2}$
    and $\hat\beta_h = (\hat W^{\top})^\dagger (\hat a_h \hat v_h)$.
    \end{enumerate}
  \end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical results}

In this section, we provide theoretical guarantees for our Spectral Experts algorithm.
In particular, the following theorem provides a rate of convergence:

\begin{theorem}[Convergence of Spectral Experts]
\label{thm:convergence}
Each dataset $\sD_p$ (for $p = 1, 2, 3$) consists of $n$ i.i.d. points drawn from a mixture
of linear regressions model with parameter $\theta^*$
(having three independent copies simplifies the analysis).
Let $\|x\|_2 \le R$ with probability $1$
and let $\|\beta_h^*\|_2 \le L$ for all $h \in [k]$.
Let $\Sigma_p \eqdef \E[\cvec(x\tp{p})\tp{2}]$, 
and assume $\Sigma_p \succ 0$ for each $p \in \{1,2,3\}$.
Assume $d \ge k$.
Suppose the number of samples is
$n = \max(n_1,n_2)$
where $n_1 = \Omega\left(d^6 R^12 (\max_{p \in [3]} \sigmamin(\Sigma_p)^{-2}) \log(d) \log(1/\delta)\right)$ and
$n_2 = \Omega\left( \sigma^6 L^6 R^{12} k (\max_{p \in [3]} \sigmamin(\Sigma_p)^{-1}) \frac{\log^3 (1/\delta)}{\epsilon^2} \right)$.
If the regularization strengths $\lambda_n^{(2)}$ and $\lambda_n^{(3)}$ are
set to $\Omega(\sigma^3 L^3 R^6 \sqrt{\frac{\log(1/\delta)}{n}})$,
then the parameter estimates $\hat\theta = (\hat\pi, \hat B)$ returned by
\algorithmref{algo:spectral-experts} (with the columns appropriate permuted)
satisfies 
$\|\hat\pi - \pi^*\|_{\infty} \le \epsilon$
and for all $h \in [k]$,
$\|\hat\beta_h - \beta^*_h\|_2 \le \frac{\epsilon}{\sqrt{\pi_h^*}}$.
\end{theorem}

The proof of the theorem has two parts.
First, we bound the error in the compound parameters estimates $\hat M_2,\hat M_3$
using results from \citet{Tomioka2011}.
Then, we use results from \cite{AnandkumarGeHsu2012} to convert this error
into a bound on the actual parameter estimates $\hat\theta = (\hat\pi, \hat B)$
derived from the robust tensor power method.
But first, let us study a more basic property: identifiability.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Identifiability from moments}

In ordinary linear regression, the regression coefficients $\beta \in \Re^d$ are
identifiable if and only if the data has full
rank: $\E[x\tp{2}] \succ 0$,
and furthermore, identifying $\beta$ requires only moments
$\E[xy]$ and $\E[x\tp{2}]$ (by observing the optimality conditions for \refeqn{y1}).
However, in mixture of linear regressions, these two moments only allow us to recover $M_1$.
\refthm{convergence} shows that if we have the higher order analogues,
$\E[x\tp{p}y\tp{p}]$ and $\E[x\tp{2p}]$ for $p \in \{1,2,3\}$,
we can then identify the parameters $\theta = (\pi, B)$,
provided $\E[\cvec(x\tp{p})\tp{2}] \succ 0$ for $p \in \{1,2,3\}$.

We note that the positive definite constraints are more stringent.
For example, suppose $x = (1, t, t^2)$,
the common polynomial basis expansion, so that all the coordinates are deterministically related.
Then $\E[x\tp{2}] \succ 0$ can easily be satisfied, say with $x \sim \normal{0}{I}$.
However, one can verify that $\E[\cvec(x\tp{2})\tp{2}]$ is singular,
since $\cvec(x\tp{2}) = [1 \cdot 1, t\cdot t, 2(1 \cdot t^2), 2(t \cdot t^2), (t^2 \cdot t^2)]$ contains
components $t \cdot t$ and $2(1 \cdot t^2)$, which are linearly dependent.
Therefore, Spectral Experts would not be able to identify the parameters of a mixture of
linear regressions for this distribution over $x$.

We now show that some amount of unidentifiability is intrinsic to estimation from low-order moments,
not just an artifact of our estimation procedure.
Suppose $x = (t, \dots, t^d)$.
Even if we observed all moments
$\E[x\tp{p}y\tp{p}]$ and $\E[x\tp{2p}]$ for $p \in [r]$,
all the resulting coordinates would be monomials up to degree $2dr$,
and thus the moments live in a $2(dr)$-dimensional subspace.
On the other hand, the parameters $\theta$ live in a subspace of at least dimension $dk$.
Therefore, at least $r \ge k/2$ moments are required for identifiability of any
algorithm for this monomial example.
