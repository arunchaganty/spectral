\documentclass[tablecaption=bottom]{jmlr}

\usepackage{booktabs}
\usepackage{ctable}
\usepackage{soul}

\title{Detailed Proofs for Spectal Experts}

\author{\Name{Arun Tejasvi Chaganty} \Email{chaganty@stanford.edu}}

\input{macros}
\input{spectral-macros}


\newcommand\eqdef{\ensuremath{\stackrel{\textrm{def}}{=}}} % Equal by definition
\newcommand\refeqn[1]{(\ref{eqn:#1})}
\newcommand\sD{\ensuremath{\mathcal{D}}}
\newcommand\sM{\ensuremath{\mathcal{M}}}
\newcommand\refapp[1]{Appendix~\ref{sec:#1}}
\newcommand\refthm[1]{Theorem~\ref{thm:#1}}
\newcommand\refprop[1]{Proposition~\ref{prop:#1}}
\newcommand\sigmamin{\sigma_\text{\rm min}}
\newcommand\sigmamax{\sigma_\text{\rm max}}
\newcommand\op{{\text{\rm op}}}
\newcommand\BP{\ensuremath{\mathbb{P}}}
\newcommand\reflem[1]{Lemma~\ref{lem:#1}}
\newcommand{\Lop}{\textrm{op}}
\DeclareMathOperator{\cvec} {cvec}
\DeclareMathOperator{\vvec} {vec}

\begin{document}

\maketitle

\todo{Refresh notation.}

\section{Regression}

Let us review the regression problem set up in \cite[Section
3]{ChagantyLiang2013}. We assume we are given data $(x_i,y_i) \in \sD_p$
generated by the following process,
\begin{align}
  y_i &= \innerp{M_p}{x_i\tp{p}} + \eta_p(x_i),
\end{align}
where $M_p = \sum_{h=1}^k \pi_h \beta_h\tp{p}$, the p-th order moments
of $\beta_h$ and $\eta_p(x)$ is zero mean noise. In particular, for $p \in \{1,2,3\}$,
we showed that $\eta_p(x)$ were defined to be,
\begin{align*}
  \eta_1(x) &= (\innerp{\beta_h - M_1}{x} + \epsilon) \\
  \eta_2(x) &= \innerp{\beta_h\tp{2} - M_2}{x\tp{2}} + 2 \epsilon \innerp{\beta_h}{x} + (\epsilon^2 - \E[\epsilon^2]) \\
  \eta_3(x) &= \innerp{\beta_h\tp{3} - M_3}{x\tp{3}}
        + 3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}} 
        + 3(\epsilon^2 \innerp{\beta_h}{x} - \E[\epsilon^2] \innerp{M_1}{x})
        + (\epsilon^3 - \E[\epsilon^3]).
\end{align*}
We assume that $\|x_i\| \le R$, $\| \beta_h \| \le L$ and $|\epsilon| \le S$.

We then defined the observation operator $\opX_p(M_p) : \Re^{d\tp{p}} \to \Re^{n}$,
\begin{align}
\opX_p(M_p; \sD_p)_i &\eqdef \innerp{M_p}{x\tp{p}_i}, & (x_i, y_i) \in \sD_p,
\end{align}
which let us succinctly represent the low-rank regression problem as follows,
\begin{align}
  \min_{M_p \in \Re^{d\tp{p}}} \frac{1}{2n} \| y - \opX_p(M_p; \sD_p) \|^2_2 + \lambda_p \|M_p\|_*.
\end{align}

Let us also recall the adjoint of the observation operator, $\opX_p^* : \Re^{n} \to \Re^{d^p}$,
\begin{align}
  \opX_p^*(\eta_p; \sD_p) &= \sum_{x \in \sD_p} \eta_p(x) x\tp{p},
\end{align}
where we have used $\eta_p$ to represent the vector $\left[\eta_p(x)\right]_{x \in \sD_p}$. 

\citet{Tomioka2011} showed that error in the estimated $\hat M_p$ can be
bounded as follows;

\begin{lemma}[\citet{Tomioka2011}, Theorem 1]
\label{lem:lowRank}
Suppose there exists a restricted strong convexity constant $\kappa(\opX_p)$ such that
$$\frac{1}{2n} \| \opX_p( \Delta )\|_2^2 \ge \kappa(\opX_p) \|\Delta\|^2_F \quad \text{and} \quad
\lambda_n \ge \frac{\|\opX_p^*(\eta_p)\|_\op}{n}.$$
Then the error of $\hat M_p$ is bounded as follows:
$\| \hat M_p - M_p^* \|_F \le \frac{\lambda_n \sqrt{k}}{\kappa(\opX_p)}$.
\end{lemma}

In this section, we will derive an upper bound on $\kappa(\opX_p)$ and
a lower bound on $\frac{1}{n} \| \opX_p^*(\eta_p) \|_\op$.

\begin{lemma}[Lower bound on restricted strong convexity]
\label{lem:lowRankLower}
Let $\Sigma_p \eqdef \E[\cvec(x\tp{p})\tp{2}]$.
If $$n \ge \frac{64 (p!)^2 R^{4p}}{\sigmamin(\Sigma_p)^2} \left(1 + \sqrt{\frac{\log(1/\delta)}{2}}\right)^2,$$
then, with probability at least $1-\delta$,
$$\kappa(\opX_p) \ge \frac{\sigmamin(\Sigma_p)}{2}.$$
\end{lemma}

\begin{proof}
  Recall the definition of $\kappa(\opX_p)$, 
  $$\frac{1}{n} \|\opX_p(\Delta)\|_2^2 \ge \kappa(\opX_p) \|\Delta\|^2_F.$$
Expanding the definition of the observation operator:
$$\frac{1}{n} \|\opX_p(\Delta)\|_2^2 = \frac{1}{n} \sum_{(x,y) \in \sD_p} \innerp{\Delta}{x\tp{p}}^2.$$
Unfolding the tensors, letting $\hat\Sigma_p \eqdef \frac{1}{n}
\sum_{(x,y) \in \sD_p} \cvec(x\tp{p})\tp{2}$, $\frac{1}{n}
\|\opX_p(\Delta)\|_2^2 = \trace(\cvec(\Delta)\tp{2} \hat\Sigma_p)$. 
We recall that each element of $\cvec(\Delta)$ aggregates elements with
permuted indices, so $\|\vvec(\Delta)\|_2 \le \|\cvec(\Delta)\|_2 \le p!
\|\vvec(\Delta)\|_2$. Then, we have 
\begin{align}
\frac{1}{n} \|\opX_p(\Delta)\|_2^2 
  &= \trace(\cvec(\Delta)\tp{2} \hat\Sigma_p) \\
  &\ge \sigmamin(\hat\Sigma_p) \|\Delta\|_F^2 .
\end{align}
By Weyl's theorem, $$\sigmamin(\hat\Sigma_p) \ge
\sigmamin(\Sigma_p) - \|\hat\Sigma_p - \Sigma_p\|_\Lop.$$ Thus, it
remains to show that $$\|\hat\Sigma_p - \Sigma_p\|_\Lop \le
\|\hat\Sigma_p - \Sigma_p\|_{F}$$ is small. Applying \reflem{conc-norms}, with
probability at least $1 - \delta$, $$\| \hat\Sigma_p - \Sigma_p \|_F
\le \frac{2 \|\Sigma_p\|_F}{\sqrt n} \left( 1 + \sqrt{\frac{\log(1/\delta)}{2}} \right).$$ Since $\|x\|_2 \le R$, we can use the
bound $\| \Sigma_p \|_F \le p! \| \vvec(x\tp{p})\tp{2} \|_F \le p!
R^{2p}$. Then, $\|\hat\Sigma_p - \Sigma_p\|_\op \le \sigmamin(\Sigma_p)/2$ with probability at least $1 - \delta$ if,
$$n \ge \frac{64 (p!)^2 R^{4p}}{\sigmamin(\Sigma_p)^2} \left(1 + \sqrt{\frac{\log(1/\delta)}{2}}\right)^2.$$

%To do this, we first apply Hoeffding's inequality elementwise.
%Since $\|x\|_2 \le R$, we have that for each element $(i,j)$,
%$|(\hat\Sigma_p)_{ij} - (\Sigma_p)_{ij}| = O(R^{2p}\sqrt{\frac{\log (1/\delta)}{n}})$.
%Applying the union bound over the $d^{2p}$ elements of $\hat\Sigma_p - \Sigma_p$,
%we have that the max norm is bounded:
%$\|\hat\Sigma_p - \Sigma_p\|_\text{\rm max} = O(R^{2p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}})$.
%The max norm times $d^p$ upper bounds the Frobenius norm, which upper bounds the operator norm, so we have that
%$\|\hat\Sigma_p - \Sigma_p\|_\text{\rm op} = O(d^p R^{2p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}})$.
%Using the fact that $\sigmamin(\hat\Sigma_p) \ge \sigmamin(\Sigma_p) - \|\hat\Sigma_p - \Sigma_p\|_\text{\rm op}$
%yields the result.
\end{proof}

\begin{lemma}[Upper bound on adjoint noise]
\label{lem:lowRankUpper}
Let $\opX_p^* : \Re^{n} \to \Re^{d^p}$ be the adjoint defined as follows,
\begin{align}
  \opX_p^*(\eta_p; \sD_p) &= \sum_{x \in \sD_p} \eta_p(x) x\tp{p},
\end{align}
where we have used $\eta_p$ to represent the vector $\left[\eta_p(x)\right]_{x \in \sD_p}$. For $p \in \{1,2,3\}$, define $\eta_p$ to be, 
\begin{align*}
  \eta_1(x) &= (\innerp{\beta_h - M_1}{x} + \epsilon) \\
  \eta_2(x) &= \innerp{\beta_h\tp{2} - M_2}{x\tp{2}} + 2 \epsilon \innerp{\beta_h}{x} + (\epsilon^2 - \E[\epsilon^2]) \\
  \eta_3(x) &= \innerp{\beta_h\tp{3} - M_3}{x\tp{3}}
        + 3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}} 
        + 3(\epsilon^2 \innerp{\beta_h}{x} - \E[\epsilon^2] \innerp{M_1}{x})
        + (\epsilon^3 - \E[\epsilon^3]).
\end{align*}

Then, with probability at least $1-\delta$,
$$\frac1{n} \|\opX_p^*(\eta_p)\|_\op = O\left(S^p L^3 R^{2p} \sqrt{\frac{\log(1/\delta)}{n}}\right).$$
for each $p \in \{1,2,3\}$.
\end{lemma}

\begin{proof}
Let $\hat\E_p[f(x,\epsilon,h)]$ denote the empirical expectation over
the examples in dataset $\sD_p$ (recall the $\sD_p$'s are independent to
simplify the analysis).  By definition,
$$\frac1n \|\opX_p^*(\eta_p)\|_\op = \hat\E_p[\eta_p(x) x\tp{p}]$$
for $p \in \{1,2,3\}$ defined in \todo{ref equations}.
%\refeqn{y1}, \refeqn{y2}, and \refeqn{y3}. 
Each $\eta_p(x)$ is composed of several zero-mean random
variables, and since $\|A + B\|_\op \le \|A\|_\op + \|B\|_\op$, it
suffices to consider each term in turn. 
For each term, we will
condition on $x$ and use standard concentration inequalities to bound
the noise. 

\paragraph{Bounding $\eta_1(x)$.}
Consider $\eta_1(x)$, which has two terms. 
The first term is $\innerp{M_1 - \beta_h}{x}$.  We have that $\|M_1 - \beta_h\|_2 \le 2L$, so by Hoeffding's inequality, and using the fact that $\|x\|_2 \le R$ a.s., with probability $1-\delta_1$, $$\|\hat\E_1[\innerp{M_1 - \beta_h}{x} x]\|_2 = O(R^2 L \sqrt{\frac{\log(1/\delta_1)}{n}}).$$ 
The second term is $\epsilon \sim \normal{0}{S^2}$, thus with probability $1-\delta_1$, $$\|\hat\E_1[\epsilon x]\|_2 = O(R S \sqrt{\frac{\log(1/\delta_1)}{n}}).$$ 
The bounds on the terms presented below hold with probability at least $1-\delta_1$ and are proved similarly, using union bounds to bound the products of independent zero-mean variables,
{
\begin{align*}
%   \| \hat\E_1[ \eta_1(x) x ] \|_\Lop &\le 
%             \underbrace{ \|\hat\E_1[\innerp{M_1 - \beta_h}{x} x]\|_\op  }_{ O( L R^2     \sqrt{\frac{\log(1/\delta_1)}{n}} ) } +
%             \underbrace{ \|\hat\E_1[\epsilon x]\|_\op                   }_{ O( R S  \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
  %  &= O( S L R^2 \sqrt{\frac{\log(1/\delta_1)}{n}} ) \\
    \| \hat\E_2[ \eta_2(x) x\tp{2} ] \|_\Lop &\le 
            \underbrace{ \|\hat\E_2[\innerp{\beta_h\tp{2} - M_2}{x\tp{2}} x\tp{2}]\|_\op }_{ O( L^2 R^4      \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
   &\quad + \underbrace{ \|\hat\E_2[2 \epsilon \innerp{\beta_h}{x} x\tp{2}]\|_\op        }_{ O( S L R^3 \sqrt{\frac{\log(1/\delta_1)}{n}} ) } 
          + \underbrace{ \|\hat\E_2[(\epsilon^2 - S^2) x\tp{2}]\|_\op               }_{ O( S^2 R^2 \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
  %&= O( S^2 L^2 R^4 \sqrt{\frac{\log^2(1/\delta_1)}{n}} ) \\
  \| \hat\E_3[ \eta_3(x) x\tp{3} ] \|_\Lop &\le 
            \underbrace{ \|\hat\E_3[\innerp{\beta_h\tp{3} - M_3}{x\tp{3}}                      x\tp{3}]\|_\op }_{O( L^3 R^6        \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
   &\quad + \underbrace{ \|\hat\E_3[3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}}                 x\tp{3}]\|_\op }_{O( S L^2 R^5 \sqrt{\frac{\log(1/\delta_1)}{n}} ) } 
          + \underbrace{ \|\hat\E_3[\epsilon^3                                                 x\tp{3}]\|_\op }_{O( S^3 R^3   \sqrt{\frac{\log (1/\delta_1)}{n}} ) } \\
   &\quad + \underbrace{ \|\hat\E_3[3(\epsilon^2 \innerp{\beta_h}{x} -S^2 \innerp{\hat M_1}{x}) x\tp{3}]\|_\op }_{O( (S^2 L R^4 + S^2 R^4) \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
  %&= O( S^3 L^3 R^6 \sqrt{\frac{\log^3(1/\delta_1)}{n}} ).
\end{align*}
}
There are two additional considerations when bounding the terms in $\eta_3$.
First, to bound $\hat\E_3[\epsilon^3]$, we use Lemma 19 of \cite{hsu13spherical}.
Second, there is an additional bias since the estimator uses $\hat M_1$ instead of $M_1$; this bias is small by the previous bound (and is constructed on an independent dataset, $\sD_1$).
Therefore, we incur another $O(S^2 \cdot R^4 \sqrt{\frac{\log(1/\delta_1)}{n}})$.
%We can handle the other terms using conventional means, the details of which can be found in \appendixref{sec:proofs}.

Finally, taking $\delta_1 = \delta/3$, and taking the union bound over the bounds for $p \in \{1,2,3\}$,
we get our result.
Note that we are bounding quantities fairly crudely for the sake of simplicity.
\end{proof}

%%%%%%%%%%%%

\section{Tensor Decomposition}

Once we have estimated the moments from the data through regression, we apply the robust tensor eigen-decomposition algorithm to recover the parameters, $\beta_h$ and $\pi$. However, the algorithm is guaranteed to work only for symmetric matrices with (nearly) orthogonal eigenvectors, so as a first step, we will need to whiten the third-order moment tensor using the second moments. Once we get the eigenvalues and eigenvectors from this orthogonal tensor, we have to undo the transformation by applying an un-whitening step. In this section, we present error bounds for each step, and combine them to prove the following lemma,
\begin{lemma}[Tensor Decomposition with Whitening]
  \label{lem:tensorPower} Let $\|\hat M_2 - M_2\|_\op \le \epsilon$ and
  $\|\hat M_3 - M_3\|_\op \le \epsilon$, for some $\epsilon$ such that 
  $$\epsilon < 
    \min\left\{
      \frac{\sigmamin(M_2)}{2},
      \frac{1}{10 k \pi_1 \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)}
    \right\}.$$ 

  Then, there exists a permutation of indices such that  the parameter
  estimates found in step 2 of \citet[Algorithm 1]{ChagantyLiang2013}
  satisfy the following with probability at least $1 - \delta$,
  \begin{align*}
  \|\hat \pi - \pi \|_{\infty}
  &\le \frac{10}{3} \pi_1^{5/2} k \pi_1 
    \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)
    \epsilon \\
  \|\hat \beta_h - \beta_h\|_2
    &\le 4\sqrt{3/2} \|M_2\|_\op^{1/2} \sigma_k(M_2)^{-1} \epsilon  \\
  &\quad + 8 \|M_2\|_\op^{1/2} k \pi_1 
    \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)
    \epsilon.
  \end{align*}
  for all $h \in [k]$.
\end{lemma}

\begin{proof}
We will use the general notation, $\aerr{X} \eqdef \|\hat X - X\|_\Lop$
to represent the error of the estimate, $\hat X$, of $X$ in the operator
norm. 
\paragraph{Step 1: Whitening}
Let $W$ and $\hat W$ be the whitening matrices for $M_2$ and $\hat M_2$
respectively. Also define $\Winv$ and $\Whinv$ to be their
pseudo-inverses.

We will first show that the whitened tensors $T = M_3(W,W,W)$ and $\hat
T = \hat M_3(\hat W, \hat W, \hat W)$ are symmetric with orthogonal
eigenvectors. Recall that $M_2 = \sum_h \pi_h \beta_h\tp{2}$, and thus
$W \beta_h = \frac{v_h}{\sqrt{\pi_h}}$, where $v_h$ form an orthonormal
basis. Applying the whitening transform to $M_3$, we get, 
\begin{align}
  M_3 &= \sum_h \pi_h \beta_h\tp{3} \\
  M_3(W,W,W) &= \sum_h \pi_h (W\beta_h)\tp{3} \\
  &= \sum_h \frac{1}{\sqrt{\pi_h}} v_h\tp{3}.
\end{align}
Consequently, $T$ has orthogonal eigenvectors, with eigenvalues $1/\sqrt{\pi_h}$.

Let us now study how far $\hat T$ differs from $T$, in terms of the
errors of $M_2$ and $M_3$. To do so, we use the triangle inequality to
break the difference into a number of simple terms,
\begin{align*}
  \aerr{T} &= \|M_3(W,W,W) - \hat M_3(\hat W,\hat W, \hat W)\|_\op \\
           &\le 
           \| {M_3}(W,W,W) - {M_3}(W,W,\hat W) \|_\op
           + \| {M_3}(W,W,\hat W) - {M_3}(W, \hat W, \hat W)\|_\op \\
           &\quad 
           + \|{M_3}(W,\hat W,\hat W) - {M_3}(\hat W, \hat W, \hat W)\|_\op 
           + \|{M_3}(\hat W,\hat W,\hat W) - - \hat {M_3}(\hat W,\hat W, \hat W)\|_\op \\
           &\le
           \| {M_3} \|_\op \|W\|^2_\op \aerr{W} +
            \| {M_3} \|_\op \|\hat W\|_\op \|W\|_\op \aerr{W} +
            \| {M_3} \|_\op \|\hat W\|^2_\op \aerr{W} +
            \aerr{M_3} \|\hat W\|^3_\op  \\
           &\le
           \| {M_3} \|_\op (\|W\|^2_\op + \|\hat W\|_\op \|W\|_\op + \|\hat W\|^2_\op) \aerr{W} +
            \aerr{M_3} \|\hat W\|^3_\op 
\end{align*}
We can relate $\|\hat W\|$ and $\aerr{W}$ to $\aerr{M_2}$ using 
using \refprop{white}. By the assumption that $\epsilon < \sigma_k/2$, we have,
\begin{align*}
  \|\hat W\|_\op &\le \sqrt{2} \sigma_k(M_2)^{-1/2} \\
  \aerr{W} &\le 4 \sigma_k(M_2)^{-3/2} \aerr{M_2}.
\end{align*}
Thus,
\begin{align*}
  \aerr{T} &\le 
  6 \| {M_3} \|_\op \|W\|^2_\op (4 \sigma_k(M_2)^{-3/2}) \aerr{M_2} +
  \aerr{M_3} 2\sqrt{2} \|W\|^3_\op \\
  &\le 
  24 \| {M_3} \|_\op \sigma_k(M_2)^{-5/2} \aerr{M_2} +
  2\sqrt{2} \sigma_k(M_2)^{-3/2} \aerr{M_3} \\
  &\le 
    \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)
    \epsilon.
\end{align*}

\paragraph{Step 2: Decomposition}

We have constructed $T$ to be a symmetric tensor with orthogonal
eigenvectors. We can now apply the results of \citet[Theorem
5.1]{AnandkumarGeHsu2012} to bound the error in the eigenvalues,
$\lambda_W$, and eigenvectors, $\omega$, returned by the robust tensor
power method;
\newcommand{\lW}{\lambda_W}
\newcommand{\lhW}{{\hat\lambda}_W}
\newcommand{\mW}{\omega}
\newcommand{\mhW}{{\hat\omega}}
\begin{align}
  \|\lW - \lhW \|_{\infty} 
  &\le \frac{5 k \epsilon_T}{(\lW)_{\min}} \\
\|\mW_h -\mhW_h \|_2 
  &\le \frac{8 k \epsilon_T}{(\lW)_{\min}^2},
\end{align}
for all $h \in [k]$, where $(\lW)_{min}$ is the smallest
eigenvalue of $T$. 

\paragraph{Step 3: Unwhitening}

Finally, we need to invert the whitening transformation to recover $\pi$ and
$\beta_h$ from $\lW$ and $\mW_h$. Let us complete the proof by
studying how this inversion relates the error in $\pi$ and $\beta$ to
the error in $\lW$ and $\mW$.

First, we will bound the error in the $\beta$s,
\begin{align*}
  \|\hat \beta_h - \beta_h\|_2
  &= \| \Whinv \mhW - \Winv \mW \|_2 \\
  &\le \aerr{\Winv} \|\mhW_h\|_2 + \|\Winv\|_2 \|\mhW_h - \mW_h \|_2. \comment{Triangle inequality}
\end{align*}

Once more, we can apply the results of \refprop{white}, with the assumptions on $\epsilon$, to get, 
\begin{align*}
  \|\Whinv\|_\op &\le \sqrt{3/2} \|M_2\|_\op^{1/2} \\
  \aerr{\Winv} &\le 4\sqrt{3/2} \|M_2\|_\op^{1/2} \sigma_k(M_2)^{-1} \aerr{M_2}.
\end{align*}

Thus,
\begin{align*}
  \|\hat \beta_h - \beta_h\|_2
  &\le 4\sqrt{3/2} \|M_2\|_\op^{1/2} \sigma_k(M_2)^{-1} \epsilon 
    + 8 \|M_2\|_\op^{1/2} \frac{k \epsilon_{T}}{(\lW)_{min}^2} \\
  &\le 4\sqrt{3/2} \|M_2\|_\op^{1/2} \sigma_k(M_2)^{-1} \epsilon  \\
  &\quad + 8 \|M_2\|_\op^{1/2} k \pi_1 
    \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)
   \epsilon.
\end{align*}

Next, let us bound the error in $\pi$,
\begin{align*}
  |\hat \pi_h - \pi_h |
  &= \left| \frac{1}{(\lW)_h^2} - \frac{1}{(\lhW)_h^2} \right| \\
  &= \left| \frac{\left( (\lW)_h + (\lhW)_h \right) \left( (\lW)_h - (\lhW)_h \right)}
  {(\lW)_h^2(\lhW)_h^2} \right| \\
  &\le \frac{( 2(\lW)_h - \|\lW - \lhW\|_{\infty} )}{(\lW)_h^2 \left( (\lW)_h + \|\lW - \lhW\|_{\infty} \right)^2} \|\lW - \lhW\|_{\infty}.
\end{align*}
Recall that $(\lW)_h = \pi_h^{-1/2}$, so the
assumptions that $\epsilon$ imply that $\aerr{T} \le \frac{1}{10 k \pi_1}$, and thus, $\|\lW - \lhW \|_{\infty} \le (\lW)_{\min}/2$. This allows us to simplify the above expression as follows,
\begin{align*}
  |\hat \pi_h - \pi_h |
  &\le \frac{(3/2)(\lW)_h}{(3/2)^2 (\lW)_h^4}
  \|\lW - \lhW\|_{\infty} \\
  &\le \frac{2}{3(\lW)_h^3} \frac{5 k \aerr{T}}{(\lW)_{\min}^2} \\
 &\le \frac{2 \pi_1^{3/2}}{3} 5 k \pi_1 
    \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)  \epsilon.
\end{align*}

\end{proof}

\section{Basic Lemmas}

\begin{lemma}[Concentration of vector norms]
  \label{lem:conc-norms}
  Let $X, X_1, \cdots, X_n \in \Re^d$ be i.i.d.\ samples
  from some distribution with bounded support
  ($\|X\|_2 \le M$ with probability 1).
  Then with probability at least $1 - \delta$,
  \begin{align}
    \left\| \frac{1}{n} \sum_{i=1}^{n} X_i - \E[X] \right\|_2 &
    \le \frac{2M}{\sqrt{n}} \left(1 + \sqrt{\frac{\log(1/\delta)}{2}}\right).
  \end{align}
\end{lemma}
\begin{proof}
  Define $Z_i = X_i - \E[X]$.

%  First, $\|Z_i\|_2 \le \|X_i\|_2 + \|\E[X_i]\|_2 \le 2M$,
%  where the first inequality is due to the triangle inequality,
%  and the second follows by Jensen's inequality on $\|\cdot\|_2$
%  and the boundedness assumption on $X_i$.

The quantity we want to bound can be expressed as follows:
  \begin{align}
  f(Z_1, Z_2, \cdots, Z_n) = \left\| \frac1n \sum_{i=1}^n Z_i \right\|_2.
  \end{align}

Let us check that $f$ satisfies the bounded differences inequality:
  \begin{align}
|f(Z_1, \cdots, Z_i, \cdots, Z_n) - f(Z_1, \cdots, Z_i', \cdots, Z_n)|
& \le \frac1n \|Z_i - Z_i'\|_2 \\
& = \frac1n \|X_i - X_i'\|_2 \\
&\le \frac{2M}{n},
  \end{align}
  by the bounded assumption of $X_i$ and the triangle inequality.

By McDiarmid's inequality,
with probability at least $1 - \delta$,
we have:
\begin{align}
\Pr[f - \E[f] \ge \epsilon] \le
\exp\left(\frac{-2 \epsilon^2}{\sum_{i=1}^n (2M/n)^2}\right).
\end{align}
Re-arranging:
\begin{align}
  \left\|\frac{1}{n}\sum_{i=1}^n Z_i\right\|_2
  &\le \E\left[ \left\| \frac{1}{n} \sum_{i=1}^n Z_i \right\|_2 \right]
  + M\sqrt{\frac{2\log(1/\delta)}{n}}.
\end{align}

Now it remains to bound $\E[f]$.
By Jensen's inequality, $\E[f] \le \sqrt{\E[f^2]}$,
so it suffices to bound $\E[f^2]$:
\begin{align}
  \E\left[ \frac1{n^2} \left\| \sum_{i=1}^n Z_i \right\|^2 \right]
  &= \E\left[ \frac1{n^2} \sum_{i=1}^n \|Z_i\|_2^2 \right] +
\E\left[ \frac1{n^2} \sum_{i\neq j} \innerp{Z_i}{Z_j} \right] \\ 
& \le \frac{4M^2}{n} + 0,
\end{align}
where the cross terms are zero by independence of the $Z_i$'s.

Putting everything together, we obtain the desired bound:
\begin{align}
\left\|\frac{1}{n}\sum_{i=1}^n Z_i \right\|
&\le \frac{2M}{\sqrt{n}} + M \sqrt{\frac{2\log(1/\delta)}{n}}.
\end{align}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Remark}: The above result can be directly
applied to the Frobenius norm of a matrix
$M$ because $\|M\|_F = \|\vvec(M)\|_2$.

\begin{proposition}[Perturbation Bounds on Whitening Matrices]
  \label{prop:white}
  Let $A$ be a rank-k $d\times d$ matrix, $\Wp$ be a $d \times k$ matrix that
  whitens $\hat A$, i.e. $\Wp^T \Ap \Wp = I$.  Suppose $\Wp^T A \Wp
  = U D U^T$, then define $W = \hat{W} U D^{-\half} U^T$. Note that $W$
  is also a $d \times k$ matrix that whitens $A$. Let $\serr{A}
  = \frac{\aerr{A}}{\sigma_k(A)}$. 

  Then, 
  \begin{align*}
    \|\hat W\|_\op 
      &\le \frac{\|W\|_\op}{\sqrt{1 - \serr{A}} } \\
  \|\Whinv\|_\op 
    &\le \|\Winv\|_\op \sqrt{1 + \serr{A}} \\
    \aerr{W} 
      &\le 2 \|W\|_\op \frac{\serr{A}}{1 - \serr{A}} \\
    \aerr{\Winv} 
      &\le 2 \|\Winv\|_\op \sqrt{1 + \serr{A}} \frac{\serr{A}}{1 - \serr{A}}.
  \end{align*}
\end{proposition}
\begin{proof}
  First, note that for a matrix $W$ that whitens $A = V \Sigma V^T$,
  $W = V \Sigma^{-\half} V^T$ and $\Winv = V \Sigma^{-\half} V^T$.
  This allows us to bound the operator norms of $\hat W$ and $\Whinv$ in
  terms of $W$ and $\Winv$,
  \begin{align*}
    \|\hat W\|_\op &= \frac{1}{\sqrt{\sigma_k(\hat A)}} \\
    &\le \frac{1}{\sqrt{\sigma_k({A}) - \aerr{A}} } \comment{By Weyl's Theorem} \\
    &\le \frac{\|W\|_\op}{\sqrt{1 - \serr{A}} } \\
    \|\Whinv\|_\op &= \sqrt{\sigma_1(\hat A)} \\
    &\le \sqrt{\sigmamax({A}) + \aerr{A}}  \comment{By Weyl's Theorem} \\
    &\le \sqrt{1 + \serr{A}} \|\Winv\|_\op.
  \end{align*}

  To find $\aerr{W}$, we will exploit the rotational invariance of the operator norm. 
  \begin{align*}
    \aerr{W} &= \| \Wp - W \|_\op \\
    &= \| W U D^{\half} U^T - W \|_\op  \comment{$W = U D^{-\half} U^T$}\\
    &\le \|W\|_\op \| I - U D^{\half} U^T \|_\op \comment{Sub-multiplicativity}\\
    &\le \|W\|_\op \| I - D \|_\op \\
    &= \|W\|_\op \| I - U D U^T \|_\op \comment{Rotational invariance}\\
    &\le \|W\|_\op \| \Wp^T \Ap_k \Wp - \Wp^T A \Wp \|_\op \comment{By definition}\\
    &\le \|W\|_\op ( \| \Wp^T (\Ap_k - \Ap) \Wp \|_\op + \| \Wp^T (\Ap - A) \Wp \|_\op) \\
    &\le \|W\|_\op \|\Wp\|_\op^2 (\sigma_{k+1}(\Ap) + \aerr{A}) \\
    &\le 2 \|W\|_\op \|\Wp\|_\op^2 \aerr{A}  \comment{Since $\sigma_{k+1}(A) = 0$}\\
    &\le 2 \|W\|_\op \frac{\serr{A}}{1 - \serr{A}} \comment{Using bound on $\|\hat W\|_\op$}.
  \end{align*}

  Similarly, we can bound the error on the un-whitening transform, $\Winv$,
  \begin{align*}
    \aerr{\pinv{W}} &= \| \pinv{\Wp} - \pinv{W} \|_\op \\
    &= \| \pinv{\Wp} U D^{\half} U^T - \pinv{W} \|_\op \\
    &\le \|\pinv{\Wp}\|_\op \| I - U D^{\half} U^T \|_\op \\
    &\le 2 \|\pinv{\Wp}\|_\op \|\Wp\|_\op^2 \aerr{A} \comment{From derivation of $\aerr{W}$}\\
    &\le 2 \|\Winv\|_\op \sqrt{1 + \serr{A}} \frac{\serr{A}}{1 - \serr{A}}.
  \end{align*}
\end{proof}

\bibliography{../ref,../pliang}

\end{document}
