\documentclass[tablecaption=bottom]{jmlr}

\usepackage{booktabs}
\usepackage{ctable}

\title{Detailed Proofs for Spectal Experts}

\author{\Name{Arun Tejasvi Chaganty} \Email{chaganty@stanford.edu}}

\input{macros}
\input{spectral-macros}


\newcommand\eqdef{\ensuremath{\stackrel{\textrm{def}}{=}}} % Equal by definition
\newcommand\refeqn[1]{(\ref{eqn:#1})}
\newcommand\sD{\ensuremath{\mathcal{D}}}
\newcommand\sM{\ensuremath{\mathcal{M}}}
\newcommand\refapp[1]{Appendix~\ref{sec:#1}}
\newcommand\refthm[1]{Theorem~\ref{thm:#1}}
\newcommand\sigmamin{\sigma_\textrm{min}}
\newcommand\sigmamax{\sigma_\textrm{max}}
\newcommand\op{{\textrm{op}}}
\newcommand\BP{\ensuremath{\mathbb{P}}}
\newcommand\reflem[1]{Lemma~\ref{lem:#1}}
\newcommand{\Lop}{\textrm{op}}
\DeclareMathOperator{\cvec} {cvec}
\DeclareMathOperator{\vvec} {vec}

\begin{document}

\maketitle

\begin{lemma}[Concentration of vector norms]
  \label{lem:conc-norms}
  Let $X, X_1, \cdots, X_n \in \Re^d$ be i.i.d.\ samples
  from some distribution with bounded support
  ($\|X\|_2 \le M$ with probability 1).
  Then with probability at least $1 - \delta$,
  \begin{align}
    \left\| \frac{1}{n} \sum_{i=1}^{n} X_i - \E[X] \right\|_2 &
    \le \frac{2 M (1 + \sqrt{\log(1/\delta)})}{\sqrt{n}}.
  \end{align}
\end{lemma}
\begin{proof}
  Define $Z_i = X_i - \E[X]$.

%  First, $\|Z_i\|_2 \le \|X_i\|_2 + \|\E[X_i]\|_2 \le 2M$,
%  where the first inequality is due to the triangle inequality,
%  and the second follows by Jensen's inequality on $\|\cdot\|_2$
%  and the boundedness assumption on $X_i$.

The quantity we want to bound can be expressed as follows:
  \begin{align}
  f(Z_1, Z_2, \cdots, Z_n) = \left\| \frac1n \sum_{i=1}^n Z_i \right\|_2.
  \end{align}

Let us check that $f$ satisfies the bounded differences inequality:
  \begin{align}
|f(Z_1, \cdots, Z_i, \cdots, Z_n) - f(Z_1, \cdots, Z_i', \cdots, Z_n)|
& \le \frac1n \|Z_i - Z_i'\|_2 \\
& = \frac1n \|X_i - X_i'\|_2 \\
&\le \frac{2M}{n},
  \end{align}
  by the bounded assumption of $X_i$ and the triangle inequality.

By McDiarmid's inequality,
with probability at least $1 - \delta$,
we have:
\begin{align}
\Pr[f - \E[f] \ge \epsilon] \le
\exp\left(\frac{-2 \epsilon^2}{\sum_{i=1}^n (2M/n)^2}\right).
\end{align}
Re-arranging:
\begin{align}
  \left\|\frac{1}{n}\sum_{i=1}^n Z_i\right\|_2
  &\le \E\left[ \left\| \frac{1}{n} \sum_{i=1}^n Z_i \right\|_2 \right]
  + M\sqrt{\frac{2\log(1/\delta)}{n}}.
\end{align}

Now it remains to bound $\E[f]$.
By Jensen's inequality, $\E[f] \le \sqrt{\E[f^2]}$,
so it suffices to bound $\E[f^2]$:
\begin{align}
  \E\left[ \frac1{n^2} \left\| \sum_{i=1}^n Z_i \right\|^2 \right]
  &= \E\left[ \frac1{n^2} \sum_{i=1}^n \|Z_i\|_2^2 \right] +
\E\left[ \frac1{n^2} \sum_{i\neq j} \innerp{Z_i}{Z_j} \right] \\ 
& \le \frac{4M^2}{n} + 0,
\end{align}
where the cross terms are zero by independence of the $Z_i$'s.

Putting everything together, we obtain the desired bound:
\begin{align}
\|\frac{1}{n}\sum_{i=1}^n Z_i\|
&\le \frac{2M}{\sqrt{n}} + M \sqrt{\frac{2\log(1/\delta)}{n}}.
\end{align}
\end{proof}

Note that the above result holds for the Frobenius norm of a matrix
$M$ because $\|M\|_F = \|\vvec(M)\|_2$.

\begin{lemma}(Concentration of Independent Products)
  \label{lemma:conc-prod}
  Let $X$ and $Y$ be independent random variables, such that $X$ and $Y^2 - \E[Y^2]$ are subgaussian with parameters $\sigma^2_X$ and $\sigma^2_{Y^2}$ respectively.
  Then, 
  \begin{align}
    \Pr[ X - \E[X] \ge \epsilon ] 
      &\le \exp( -\frac{3}{4} (\frac{2\epsilon^4}{\sigma^4_X \sigma^2_{Y^2}} )^{1/3} ).
  \end{align}
\end{lemma}
\begin{proof}
  We begin by bounding the moment generating function of $XY$, using the property that $X$ and $Y$ are independent,
  \begin{align}
    \E[ \exp(t X Y) ] 
      &= \E[ \E[ \exp(t X Y) | Y ] ] \\
      &\le \E[ \exp(\frac{(tY)^2 \sigma^2_X}{2}) ] \\
      &\le \E[ \exp(\frac{(tY)^2 \sigma^2_X}{2} + \frac{t^2 \E[Y^2] \sigma^2_X}{2}) ] \\
      &\le \exp(\frac{t^4 \sigma^4_X \sigma^2_{Y^2} }{8}  + \frac{t^2 \E[Y^2] \sigma^2_X}{2}) \\
      &\le \exp(\frac{t^4 \sigma^4_X \sigma^2_{Y^2} }{8}  + \frac{t^2 R^2 \sigma^2_X}{2}).
  \end{align}

  By Markov's inequality, we have that,
  \begin{align}
    \Pr[ XY \ge \epsilon ] 
      &\le \inf_t \frac{\E[\exp(t XY)]}{\exp{t\epsilon}} \\
      &\le \inf_t \exp(\frac{t^4 \sigma^4_X \sigma^2_{Y^2} }{8} + \frac{t^2 R^2 \sigma^2_X}{2} - t\epsilon).
  \end{align}
  %Setting $t = (\frac{2\epsilon}{\sigma^4_X \sigma^2_{Y^2}})^{1/3}$ and
  %$t = \frac{\epsilon}{\sigma^2_X R^2 }$, we get two upper bounds on the
  %above expression, thus,
  %\begin{align}
  %  \Pr[ XY \ge \epsilon ] 
  %    &\le \min\{
  %      \exp( -(\frac{54}{64 (\sigma^4_X \sigma^4_{Y^2})})^{1/3} \epsilon^{4/3} 
  %          + (\frac{1}{2 (\sigma^2_X \sigma^4_{Y^2})})^{1/3} R^2 \epsilon^{2/3} ), \\
  %    &\quad
  %      \exp( - \frac{1}{\sigma^2_X R^2} \epsilon^{2} 
  %        + \frac{\sigma^2_{Y^2}}{8 (\sigma^4_X R^8)} \epsilon^{4} ) \}.
  %\end{align}

  Setting $t = \frac{\epsilon}{\sigma^2_X R^2 }$, we get the bound,
  \begin{align}
    \Pr[ XY \ge \epsilon ] 
      &\le 
        \exp( - \frac{1}{\sigma^2_X R^2} \epsilon^{2} 
          + \frac{\sigma^2_{Y^2}}{8 (\sigma^4_X R^8)} \epsilon^{4} ).
  \end{align}

\end{proof}

\begin{corollary}
  Let $\{X_i\}_{i=1}^n$ and $\{Y_i\}_{i=1}^n$ be $n$ iid
  samples of $X$ and $Y$, such that $X$ is subgaussian with parameter
  $\sigma^2$ and $Y \le R$ with probability 1. Then, if $\epsilon \le 4 \sigma_X R$,
  \begin{align}
    \Pr[ \hat\E[ X Y ] - \E[ X Y ] \ge \epsilon ] 
    &\le \exp( - \frac{n \epsilon^2}{2 \sigma^2_X R^2 } ).
  \end{align}
\end{corollary}
\begin{proof}
  Observe that $Y^2 - \E[Y^2]$ is subgaussian with parameter $\frac{R^4}{4}$. Using the assumed bound on $\epsilon$, we get,
  $\frac{\sigma^2_{Y^2}}{8 (\sigma^4_X R^8)} \epsilon^{4} \le \frac{1}{2\sigma^2_X R^2} \epsilon^{2}$.
\end{proof}

\begin{lemma}[lower bound on restricted strong convexity]
\label{lem:lowRankLower}
Let $\Sigma_p \eqdef \E[\cvec(x\tp{p})\tp{2}]$.
If $$n \ge \frac{12^2 (p!)^2 R^{4p} \log(1/\delta)}{\sigmamin(\Sigma_p)^2},$$
then, with probability at least $1-\delta$,
$$\kappa(\opX_p) \ge \frac{\sigmamin(\Sigma_p)}{2}.$$
\end{lemma}

\begin{proof}
  Recall the definition of $\kappa(\opX_p)$, 
  $$\frac{1}{n} \|\opX_p(\Delta)\|_2^2 \ge \kappa(\opX_p) \|\Delta\|^2_F.$$
Expanding the definition of the observation operator:
$$\frac{1}{n} \|\opX_p(\Delta)\|_2^2 = \frac{1}{n} \sum_{(x,y) \in \sD} \innerp{\Delta}{x\tp{p}}^2.$$
Unfolding the tensors, letting $\hat\Sigma_p \eqdef \frac{1}{n}
\sum_{(x,y) \in \sD} \cvec(x\tp{p})\tp{2}$, $\frac{1}{n}
\|\opX_p(\Delta)\|_2^2 = \trace(\cvec(\Delta)\tp{2} \hat\Sigma_p)$. 
We recall that each element of $\cvec(\Delta)$ aggregates elements with
permuted indices, so $\|\vvec(\Delta)\|_2 \le \|\cvec(\Delta)\|_2 \le p!
\|\vvec(\Delta)\|_2$. Then, we have 
\begin{align}
\frac{1}{n} \|\opX_p(\Delta)\|_2^2 
  &= \trace(\cvec(\Delta)\tp{2} \hat\Sigma_p) \\
  &\ge \sigmamin(\hat\Sigma_p) \|\Delta\|_F^2 .
\end{align}
By Weyl's theorem, $$\sigmamin(\hat\Sigma_p) \ge
\sigmamin(\Sigma_p) - \|\hat\Sigma_p - \Sigma_p\|_\Lop.$$ Thus, it
remains to show that $$\|\hat\Sigma_p - \Sigma_p\|_\Lop \le
\|\hat\Sigma_p - \Sigma_p\|_{F}$$ is small. Applying \reflem{conc-norms}, with
probability at least $1 - \delta$, $$\| \hat\Sigma_p - \Sigma_p \|_F
\le 6 \|\Sigma_p\|_F \sqrt{\frac{\log(1/\delta)}{n}}.$$ Since $\|x\|_2 \le R$, we can use the
bound $\| \Sigma_p \|_F \le p! \| \vvec(x\tp{p})\tp{2} \|_F \le p!
R^{2p}$ in the above to get the stated result. 

%To do this, we first apply Hoeffding's inequality elementwise.
%Since $\|x\|_2 \le R$, we have that for each element $(i,j)$,
%$|(\hat\Sigma_p)_{ij} - (\Sigma_p)_{ij}| = O(R^{2p}\sqrt{\frac{\log (1/\delta)}{n}})$.
%Applying the union bound over the $d^{2p}$ elements of $\hat\Sigma_p - \Sigma_p$,
%we have that the max norm is bounded:
%$\|\hat\Sigma_p - \Sigma_p\|_\text{\rm max} = O(R^{2p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}})$.
%The max norm times $d^p$ upper bounds the Frobenius norm, which upper bounds the operator norm, so we have that
%$\|\hat\Sigma_p - \Sigma_p\|_\text{\rm op} = O(d^p R^{2p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}})$.
%Using the fact that $\sigmamin(\hat\Sigma_p) \ge \sigmamin(\Sigma_p) - \|\hat\Sigma_p - \Sigma_p\|_\text{\rm op}$
%yields the result.
\end{proof}

\begin{lemma}[upper bound on adjoint noise]
\label{lem:lowRankUpper}
Let $\opX_p$ be the linear operator previously defined. Then with
probability at least $1-\delta$,
$$\frac1{n} \|\opX_p^*(\eta_p)\|_\op \le O(\sigma^3 L^3 R^6 \sqrt{\frac{\log^3(1/\delta)}{n}}).$$
for each $p \in \{1,2,3\}$.
\end{lemma}

\begin{proof}
Let $\hat\E_p[f(x,\epsilon,h)]$ denote the empirical expectation over
the examples in dataset $\sD_p$ (recall the $\sD_p$'s are independent to
simplify the analysis).  By definition,
$$\frac1n \|\opX_p^*(\eta_p)\|_\op = \hat\E_p[\eta_p(x) x\tp{p}]$$
for $p \in \{1,2,3\}$ defined in \refeqn{y1}, \refeqn{y2}, and
\refeqn{y3}. Each $\eta_p(x)$ is composed of several zero-mean random
variables, and since $\|A + B\|_\op \le \|A\|_\op + \|B\|_\op$, it
suffices to consider each term in turn. 
For each term, we will
condition on $x$ and use standard concentration inequalities to bound
the noise. 

For example, consider $\eta_1(x)$, which has two terms. 
The first term is $\innerp{M_1 - \beta_h}{x}$.  We have that $\|M_1 - \beta_h\|_2 \le 2L$, so by Hoeffding's inequality, and using the fact that $\|x\|_2 \le R$ a.s., with probability $1-\delta_1$, $$\|\hat\E_1[\innerp{M_1 - \beta_h}{x} x]\|_2 = O(R^2 L \sqrt{\frac{\log(1/\delta_1)}{n}})$$. 
The second term is $\epsilon \sim \normal{0}{\sigma^2}$, thus with probability $1-\delta_1$, $$\|\hat\E_1[\epsilon x]\|_2 = O(R \sigma \sqrt{\frac{\log(1/\delta_1)}{n}})$$. 
The bounds on the terms presented below hold with probability at least $1-\delta_1$ and are proved similarly, using union bounds to bound the products of independent zero-mean variables,
{
\begin{align*}
%   \| \hat\E_1[ \eta_1(x) x ] \|_\Lop &\le 
%             \underbrace{ \|\hat\E_1[\innerp{M_1 - \beta_h}{x} x]\|_\op  }_{ O( L R^2     \sqrt{\frac{\log(1/\delta_1)}{n}} ) } +
%             \underbrace{ \|\hat\E_1[\epsilon x]\|_\op                   }_{ O( R \sigma  \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
  %  &= O( \sigma L R^2 \sqrt{\frac{\log(1/\delta_1)}{n}} ) \\
    \| \hat\E_2[ \eta_2(x) x\tp{2} ] \|_\Lop &\le 
            \underbrace{ \|\hat\E_2[\innerp{\beta_h\tp{2} - M_2}{x\tp{2}} x\tp{2}]\|_\op }_{ O( L^2 R^4      \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
   &\quad + \underbrace{ \|\hat\E_2[2 \epsilon \innerp{\beta_h}{x} x\tp{2}]\|_\op        }_{ O( \sigma L R^3 \sqrt{\frac{\log(1/\delta_1)}{n}} ) } 
          + \underbrace{ \|\hat\E_2[(\epsilon^2 - \sigma^2) x\tp{2}]\|_\op               }_{ O( \sigma^2 R^2 \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
  %&= O( \sigma^2 L^2 R^4 \sqrt{\frac{\log^2(1/\delta_1)}{n}} ) \\
  \| \hat\E_3[ \eta_3(x) x\tp{3} ] \|_\Lop &\le 
            \underbrace{ \|\hat\E_3[\innerp{\beta_h\tp{3} - M_3}{x\tp{3}}                      x\tp{3}]\|_\op }_{O( L^3 R^6        \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
   &\quad + \underbrace{ \|\hat\E_3[3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}}                 x\tp{3}]\|_\op }_{O( \sigma L^2 R^5 \sqrt{\frac{\log(1/\delta_1)}{n}} ) } 
          + \underbrace{ \|\hat\E_3[\epsilon^3                                                 x\tp{3}]\|_\op }_{O( \sigma^3 R^3   \sqrt{\frac{\log (1/\delta_1)}{n}} ) } \\
   &\quad + \underbrace{ \|\hat\E_3[3(\epsilon^2 \innerp{\beta_h}{x} -\sigma^2 \innerp{\hat M_1}{x}) x\tp{3}]\|_\op }_{O( (\sigma^2 L R^4 + \sigma^2 R^4) \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
  %&= O( \sigma^3 L^3 R^6 \sqrt{\frac{\log^3(1/\delta_1)}{n}} ).
\end{align*}
}
There are two additional considerations when bounding the terms in $\eta_3$.
First, to bound $\hat\E_3[\epsilon^3]$, we use Lemma 19 of \cite{hsu13spherical}.
Second, there is an additional bias since the estimator uses $\hat M_1$ instead of $M_1$; this bias is small by the previous bound (and is constructed on an independent dataset, $\sD_1$).
Therefore, we incur another $O(\sigma^2 \cdot R^4 \sqrt{\frac{\log(1/\delta_1)}{n}})$.
%We can handle the other terms using conventional means, the details of which can be found in \appendixref{sec:proofs}.

Finally, taking $\delta_1 = \delta/3$, and taking the union bound over the bounds for $p \in \{1,2,3\}$,
we get our result.
Note that we are bounding quantities fairly crudely for the sake of simplicity.
\end{proof}

\paragraph{Lemma 4}

\begin{proof}
Let 
\begin{align}
\epsilon_{X} &\eqdef \|\hat X - X\|_\Lop \\
\epsilon_W &\eqdef \|\hat W - W\|_\Lop \\
\epsilon_{\Winv} &\eqdef \|\hat \Winv - \Winv\|_\Lop,
\end{align}
where $\hat W$ is the whitening
matrix for $\hat M_2$, and $\hat \Winv$ its inverse. 
Basic perturbation theory gives that, if $\epsilon \le \frac{\sigmamin(M_2)}{2}$,
\begin{align}
\epsilon_W &= O(\sigmamin(M_2)^{-0.5} \epsilon) \\
\epsilon_{\Winv} &= O(\|M_2\|_\Lop^{0.5} \epsilon) \\
\|\hat W\|_2 &\le \sqrt{2} \|W\| \\
\|\hat \Winv\|_2 &\le \sqrt\frac{3}{2} \|\Winv\|.
\end{align}

The error of the whitened tensor $T = M_3(W,W,W)$ is then, 

\begin{align}
\epsilon_T &= O( \|\hat W\|_\Lop \epsilon_{M_3} + 3 \|T\|_\Lop \|\hat W\|_\Lop^2 \epsilon_{W}  )  \\
  &= O(\sigmamin(M_2)^{-0.5} (1 + \|M_3\|_\Lop \sigmamin(M_2)^{-1} )~\epsilon).
\end{align}

By construction, $T$ is a symmetric tensor with orthogonal eigenvectors;
thus we can apply the results Theorem 5.1 of \citet{AnandkumarGeHsu2012}
to bound the error in the eigenvalues, $\tilde \lambda$, and eigenvectors, $\tilde \beta_h$, returned by the
robust tensor power method; 
$$\|\tilde \lambda - \hat {\tilde \lambda} \|_{\infty} \le \frac{5 k \epsilon_T}{\tilde \lambda_{\min}}$$
and
$$\|\tilde \beta_h - \hat {\tilde \beta}_h \|_2 \le \frac{8 k \epsilon_T}{\tilde \lambda_{\min}^2}$$,
for all $h \in [k]$, where $\tilde \lambda_{min}$ is the smallest
eigenvalue of $T$. Note that $\tilde \lambda_h = \pi_h^{-0.5}$ and that
the assumptions on $\epsilon$ imply that $$\|\tilde \lambda - \hat
{\tilde \lambda} \|_{\infty} < \frac{\tilde \lambda_{min}}{2}$$

Finally, we need to invert the whitening transformation to get bounds on
the original $\pi$ and $\beta_h$,
\begin{align*}
  \|\hat \beta_h - \beta_h\|_2
  &= \| \pinv{\hat W} \hat {\tilde \beta}_h - \pinv{W} {\tilde \beta}_h \|_2 \\
  &= O( \epsilon_{\pinv{W}} \|\hat{\tilde \beta}_h\|_2 + \|\pinv{W}\|_2 \|\hat {\tilde \beta}_h - {\tilde \beta}_h\|_2 )\\
  &= O(\|M_2\|_\Lop^{0.5} \epsilon + \|M_2\|_\Lop^{0.5} \frac{k \epsilon_{T}}{\tilde \lambda_{min}^2}) \\
  &= O( k \pi_{max} \sigmamin(M_2)^{-1.5} \|M_2\|_\Lop^{0.5} \|M_3\|_\Lop \epsilon ) \\
  |\hat \pi_h - \pi_h |
  &= |\frac{1}{(\hat{\tilde{\lambda}}_h)^2} - \frac{1}{({\tilde{\lambda}}_h)^2}| \\
  &= O( \frac{|2 {\tilde \lambda}_h \epsilon_{\tilde{\lambda}} - \epsilon_{\tilde{\lambda}}^2 |}{({\tilde \lambda}_{\min}^4 - {\tilde \lambda}_{\min}^2 \epsilon_{\tilde \lambda})} ) \\
  &= O( \frac{{\tilde \lambda}_{max} \epsilon_{\tilde \lambda}}{{\tilde\lambda}_{min}^4} ) \\
  &= O( k \pi_{max}^{2.5} \pi_{min}^{-0.5} \sigmamin(M_2)^{-1.5} \|M_3\|_\Lop \epsilon ).
\end{align*}

\end{proof}

\end{document}
