\documentclass[tablecaption=bottom]{jmlr}

\usepackage{booktabs}
\usepackage{ctable}
\usepackage{soul}

\title{Detailed Proofs for Spectal Experts}

\author{\Name{Arun Tejasvi Chaganty} \Email{chaganty@stanford.edu}}

\input{macros}
\input{spectral-macros}


\newcommand\eqdef{\ensuremath{\stackrel{\textrm{def}}{=}}} % Equal by definition
\newcommand\refeqn[1]{(\ref{eqn:#1})}
\newcommand\sD{\ensuremath{\mathcal{D}}}
\newcommand\sM{\ensuremath{\mathcal{M}}}
\newcommand\refapp[1]{Appendix~\ref{sec:#1}}
\newcommand\refthm[1]{Theorem~\ref{thm:#1}}
\newcommand\refprop[1]{Proposition~\ref{prop:#1}}
\newcommand\sigmamin{\sigma_\text{\rm min}}
\newcommand\sigmamax{\sigma_\text{\rm max}}
\newcommand\op{{\text{\rm op}}}
\newcommand\BP{\ensuremath{\mathbb{P}}}
\newcommand\reflem[1]{Lemma~\ref{lem:#1}}
\newcommand{\Lop}{\textrm{op}}
\DeclareMathOperator{\cvec} {cvec}
\DeclareMathOperator{\vvec} {vec}

\begin{document}

\maketitle

\section{Notation}
Let $[n] = \{ 1, \dots, n \}$ denote the first $n$ positive integers.

We use $x\tp{p}$ to represent the $p$-th order tensor formed by taking
the outer product of $x \in \Re^d$; i.e. $x\tp{p}_{i_1 \ldots i_p}
= x_{i_1} \cdots x_{i_p}$. We will use $\innerp{\cdot}{\cdot}$ to denote
the generalized dot product between two $p$-th order tensors:
$\innerp{X}{Y} = \sum_{i_1, \ldots i_p} X_{i_1, \ldots i_p} Y_{i_1,
\ldots i_p}$.  A tensor $X$ is symmetric if for all $i,j \in [d]^p$
which are permutations of each other, $X_{i_1 \cdots i_p}$ = $X_{j_1
\cdots j_p}$ (all tensors in this paper will be symmetric).  For
a $p$-th order tensor $X \in (\Re^d)\tp{p}$, the mode-$i$ unfolding of
$X$ is a matrix, $X_{(i)} \in \Re^{d \times d^{p-1}}$, whose $j$-th row contains all the elements of $X$ whose
$i$-th index is equal to $j$. 
%For $X \in (\Re^d)\tp{p}$ and $Y \in (\Re^d)\tp{(p+q)}$,% $p, q > 0$,
%$\innerp{X}{Y} \in \Re^{d^q}$ is the projection of $X$ onto the first
%$p$ modes of $Y$, i.e. $\innerp{X}{Y}_{i_1, \ldots, i_q} = \sum_{j_1,
%\ldots, j_p} X_{j_1, \ldots, j_p} Y_{j_1, \ldots, j_p, i_1, \ldots,
%i_q}$.

% Norms
For a vector $X$,
let $\|X\|_\op$ denote the 2-norm.
For a matrix $X$,
let $\|X\|_*$ denote the nuclear (trace) norm (sum of singular values),
let $\|X\|_F$ denote the Frobenius norm (square root of sum of squares of singular values),
let $\|X\|_{\max}$ denote the max norm (elementwise maximum),
let $\|X\|_\op$ denote the operator norm (largest singular value),
let $\sigma_\text{min}(X)$ be the smallest singular value of $X$.
For a tensor $X$,
let $\|X\|_* = \frac{1}{p} \sum_{i=1}^p \|X_{(i)}\|_*$ denote
the average nuclear norm over all $p$ unfoldings,
and let $\|X\|_\op = \frac{1}{p} \sum_{i=1}^p \|X_{(i)}\|_\op$
denote the average operator norm over all $p$ unfoldings.

For a symmetric tensor $X \in (\Re^d)\tp{p}$, let $\cvec(X) \in
\Re^{C_{d,p}}, C_{d,p} = \binom{d + p + 1}{p}$ be the collapsed vectorization of distinct
elements in $X$, for example, for $X \in \Re^{d \times d}$, $\cvec(X)
= (X_{ii} : i \in [d]; X_{ij} + X_{ji} : i,j \in [d], i<j)$.
In general, each component of $\cvec(X)$ is indexed by a vector of
counts $(c_1, \dots, c_d)$ with total sum $\sum_i c_i = p$.  The value
of that component is $\sum_{k \in K(c)} X_{k_1 \cdots k_p}$, where $K(c)
= \{ k \in [d]^p : \forall i \in [d], c_i = |\{ j \in [p] : k_j = i \}|
\}$ are the set of index vectors $k$ with that count profile.  
%For symmetric tensors, $\innerp{X}{Y} = \innerp{\cvec(X)}{\cvec(Y)}$.

\section{Regression}

Let us review the regression problem set up in \cite[Section
3]{ChagantyLiang2013}. We assume we are given data $(x_i,y_i) \in \sD_p$
generated by the following process,
\begin{align}
  y_i &= \innerp{M_p}{x_i\tp{p}} + \eta_p(x_i),
\end{align}
where $M_p = \sum_{h=1}^k \pi_h \beta_h\tp{p}$, the p-th order moments
of $\beta_h$ and $\eta_p(x)$ is zero mean noise. In particular, for $p \in \{1,2,3\}$,
we showed that $\eta_p(x)$ were defined to be,
\begin{align}
  \eta_1(x) &= \innerp{\beta_h - M_1}{x} + \epsilon \label{eqn:eta1} \\
  \eta_2(x) &= \innerp{\beta_h\tp{2} - M_2}{x\tp{2}} + 2 \epsilon \innerp{\beta_h}{x} + (\epsilon^2 - \E[\epsilon^2]) \label{eqn:eta2}\\
  \eta_3(x) &= \innerp{\beta_h\tp{3} - M_3}{x\tp{3}}
        + 3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}} 
        + 3(\epsilon^2 \innerp{\beta_h}{x} - \E[\epsilon^2] \innerp{M_1}{x})
        + (\epsilon^3 - \E[\epsilon^3]). \label{eqn:eta3}
\end{align}
We assume that $\|x_i\| \le R$, $\| \beta_h \| \le L$ and $|\epsilon| \le S$.

We then defined the observation operator $\opX_p(M_p) : \Re^{d\tp{p}} \to \Re^{n}$,
\begin{align}
\opX_p(M_p; \sD_p)_i &\eqdef \innerp{M_p}{x\tp{p}_i}, & (x_i, y_i) \in \sD_p,
\end{align}
which let us succinctly represent the low-rank regression problem as follows,
\begin{align}
  \min_{M_p \in \Re^{d\tp{p}}} \frac{1}{2n} \| y - \opX_p(M_p; \sD_p) \|^2_2 + \lambda_p \|M_p\|_*.
\end{align}

Let us also recall the adjoint of the observation operator, $\opX_p^* : \Re^{n} \to \Re^{d^p}$,
\begin{align}
  \opX_p^*(\eta_p; \sD_p) &= \sum_{x \in \sD_p} \eta_p(x) x\tp{p},
\end{align}
where we have used $\eta_p$ to represent the vector $\left[\eta_p(x)\right]_{x \in \sD_p}$. 

\citet{Tomioka2011} showed that error in the estimated $\hat M_p$ can be
bounded as follows;

\begin{lemma}[\citet{Tomioka2011}, Theorem 1]
\label{lem:lowRank}
Suppose there exists a restricted strong convexity constant $\kappa(\opX_p)$ such that
$$\frac{1}{2n} \| \opX_p( \Delta )\|_2^2 \ge \kappa(\opX_p) \|\Delta\|^2_F \quad \text{and} \quad
\lambda_n \ge \frac{\|\opX_p^*(\eta_p)\|_\op}{n}.$$
Then the error of $\hat M_p$ is bounded as follows:
$\| \hat M_p - M_p^* \|_F \le \frac{\lambda_n \sqrt{k}}{\kappa(\opX_p)}$.
\end{lemma}

In this section, we will derive an upper bound on $\kappa(\opX_p)$ and
a lower bound on $\frac{1}{n} \| \opX_p^*(\eta_p) \|_\op$.

\begin{lemma}[Lower bound on restricted strong convexity]
\label{lem:lowRankLower}
Let $\Sigma_p \eqdef \E[\cvec(x\tp{p})\tp{2}]$.
If $$n \ge \frac{64 (p!)^2 R^{4p}}{\sigmamin(\Sigma_p)^2} \left(1 + \sqrt{\frac{\log(1/\delta)}{2}}\right)^2,$$
then, with probability at least $1-\delta$,
$$\kappa(\opX_p) \ge \frac{\sigmamin(\Sigma_p)}{2}.$$
\end{lemma}

\begin{proof}
  Recall the definition of $\kappa(\opX_p)$, 
  $$\frac{1}{n} \|\opX_p(\Delta)\|_2^2 \ge \kappa(\opX_p) \|\Delta\|^2_F.$$
Expanding the definition of the observation operator:
$$\frac{1}{n} \|\opX_p(\Delta)\|_2^2 = \frac{1}{n} \sum_{(x,y) \in \sD_p} \innerp{\Delta}{x\tp{p}}^2.$$
Unfolding the tensors, letting $\hat\Sigma_p \eqdef \frac{1}{n}
\sum_{(x,y) \in \sD_p} \cvec(x\tp{p})\tp{2}$, $\frac{1}{n}
\|\opX_p(\Delta)\|_2^2 = \trace(\cvec(\Delta)\tp{2} \hat\Sigma_p)$. 
We recall that each element of $\cvec(\Delta)$ aggregates elements with
permuted indices, so $\|\vvec(\Delta)\|_2 \le \|\cvec(\Delta)\|_2 \le p!
\|\vvec(\Delta)\|_2$. Then, we have 
\begin{align}
\frac{1}{n} \|\opX_p(\Delta)\|_2^2 
  &= \trace(\cvec(\Delta)\tp{2} \hat\Sigma_p) \\
  &\ge \sigmamin(\hat\Sigma_p) \|\Delta\|_F^2 .
\end{align}
By Weyl's theorem, $$\sigmamin(\hat\Sigma_p) \ge
\sigmamin(\Sigma_p) - \|\hat\Sigma_p - \Sigma_p\|_\Lop.$$ Thus, it
remains to show that $$\|\hat\Sigma_p - \Sigma_p\|_\Lop \le
\|\hat\Sigma_p - \Sigma_p\|_{F}$$ is small. Applying \reflem{conc-norms}, with
probability at least $1 - \delta$, $$\| \hat\Sigma_p - \Sigma_p \|_F
\le \frac{2 \|\Sigma_p\|_F}{\sqrt n} \left( 1 + \sqrt{\frac{\log(1/\delta)}{2}} \right).$$ Since $\|x\|_2 \le R$, we can use the
bound $\| \Sigma_p \|_F \le p! \| \vvec(x\tp{p})\tp{2} \|_F \le p!
R^{2p}$. Then, $\|\hat\Sigma_p - \Sigma_p\|_\op \le \sigmamin(\Sigma_p)/2$ with probability at least $1 - \delta$ if,
$$n \ge \frac{64 (p!)^2 R^{4p}}{\sigmamin(\Sigma_p)^2} \left(1 + \sqrt{\frac{\log(1/\delta)}{2}}\right)^2.$$

%To do this, we first apply Hoeffding's inequality elementwise.
%Since $\|x\|_2 \le R$, we have that for each element $(i,j)$,
%$|(\hat\Sigma_p)_{ij} - (\Sigma_p)_{ij}| = O(R^{2p}\sqrt{\frac{\log (1/\delta)}{n}})$.
%Applying the union bound over the $d^{2p}$ elements of $\hat\Sigma_p - \Sigma_p$,
%we have that the max norm is bounded:
%$\|\hat\Sigma_p - \Sigma_p\|_\text{\rm max} = O(R^{2p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}})$.
%The max norm times $d^p$ upper bounds the Frobenius norm, which upper bounds the operator norm, so we have that
%$\|\hat\Sigma_p - \Sigma_p\|_\text{\rm op} = O(d^p R^{2p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}})$.
%Using the fact that $\sigmamin(\hat\Sigma_p) \ge \sigmamin(\Sigma_p) - \|\hat\Sigma_p - \Sigma_p\|_\text{\rm op}$
%yields the result.
\end{proof}

\begin{lemma}[Upper bound on adjoint noise]
\label{lem:lowRankUpper}
With probability at least $1-\delta$, the following holds,
\begin{align*}
    \left\|\hat\E_1[\eta_1(x) x] \right\|_2
      &\le \frac{2R (2LR + S)}{\sqrt{n}} \left( 1 + \sqrt{\frac{\log(3/\delta)}{2}} \right) \\
  \left\|\hat\E_2[\eta_2(x) x\tp{2}] \right\|_F
      &\le \frac{(4L^2 R^2 + 2 S L R + 4S^2)R^2}{\sqrt{n}} \left( 1 + \sqrt{\frac{\log(3/\delta)}{2}} \right) \\
  \left\|\hat\E_3[\eta_3(x) x\tp{3}] \right\|_F
      &\le \frac{(8L^3 R^3 + 3 L^2 R^2 S + 6 L R S^2 + 2S^3) R^3}{\sqrt{n}} \left( 1 + \sqrt{\frac{\log(3/\delta)}{2}} \right).
\end{align*}
\end{lemma}

It follows that, with probability at least $1-\delta$,
\begin{align*}
  \left\|\hat\E_p[\eta_p(x) x\tp{p}] \right\|_2
  &= O\left( \frac{L^p S^p R^{2p}}{\sqrt{n}} \sqrt{\log(1/\delta)} \right),
\end{align*}
for each $p \in \{1,2,3\}$.

\begin{proof}
Let $\hat\E_p[f(x,\epsilon,h)]$ denote the empirical expectation over
the examples in dataset $\sD_p$ (recall the $\sD_p$'s are independent to
simplify the analysis).  By definition,
$$\frac1n \|\opX_p^*(\eta_p)\|_\op = \left\| \hat\E_p[\eta_p(x) x\tp{p}] \right\|_\op $$
for $p \in \{1,2,3\}$. To proceed, we will bound each $\eta_p(x)$, defined in \refeqn{eta1},
\refeqn{eta2} and \refeqn{eta3} and use \reflem{conc-norms} to bound $\|
\hat\E_p[\eta_p(x) x\tp{p}] \|_F$. The Frobenius norm to bounds the
operator norm, completing the proof.

%composed of several zero-mean random variables, and
%since $\|A + B\|_\op \le \|A\|_\op + \|B\|_\op$, it suffices to consider
%each term in turn. 

\paragraph{Bounding $\eta_p(x)$.}
Using the assumptions that $\|\beta_h\|_2 \le L$, $\|x\|_2 \le R$ and
$|\epsilon| \le S$, it is easy to bound each $\eta_p(x)$,
\begin{align*}
  \eta_1(x) &= \innerp{\beta_h - M_1}{x} + \epsilon \\
            &\le \|\beta_h - M_1\|_2 \|x\|_2 + |\epsilon| \\
            &\le 2LR + S \\
  \eta_2(x) 
    &= \innerp{\beta_h\tp{2} - M_2}{x\tp{2}} + 2 \epsilon \innerp{\beta_h}{x} + (\epsilon^2 - \E[\epsilon^2]) \\
    &\le \|\beta_h\tp{2} - M_2\|_F \|x\tp{2}\|_F + 2 |\epsilon| \|\beta_h\|_2\|x\|_2 + |\epsilon^2 - \E[\epsilon^2]| \\
    &\le (2L)^2 R^2 + 2 S L R + (2S)^2 \\
  \eta_3(x) &= \innerp{\beta_h\tp{3} - M_3}{x\tp{3}}
        + 3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}} \\
        &\quad + 3\left(\epsilon^2 \innerp{\beta_h}{x} - \E[\epsilon^2] \innerp{M_1}{x}\right)
        + (\epsilon^3 - \E[\epsilon^3]) \\
  &\le \|\beta_h\tp{3} - M_3\|_F\|x\tp{3}\|_F
        + 3 |\epsilon| \|\beta_h\tp{2}\|_F \|x\tp{2}\|_F  \\
        &\quad + 3 \left( |\epsilon^2|~\|\beta_h\|_F\|x\|_F + \left|\E[\epsilon^2]\right|~\|M_1\|_2\|x\|_2 \right)
        + |\epsilon^3| + \left| \E[\epsilon^3] \right| \\
  &\le (2L)^3 R^3 + 3 S L^2 R^2 + 3 ( S^2 L R + S^2 L R ) + 2S^3.
\end{align*}
We have used inequality $\|M_1 - \beta_h\|_2 \le 2L$ above. 

\paragraph{Bounding $\left\| \hat\E[\eta_p(x)x\tp{p}] \right\|_F$.}
We may now apply the above bounds on $\eta_p(x)$ to bound $\|\eta_p(x) x\tp{p}\|_F$, using the fact that $\|c X\|_F \le c\|X\|_F$.
By \reflem{conc-norms}, each of the following hold with probability at least $1-\delta_1$,
\begin{align*}
    \left\|\hat\E_1[\eta_1(x) x] \right\|_2
      &\le \frac{2R (2LR + S)}{\sqrt{n}} \left( 1 + \sqrt{\frac{\log(1/\delta_1)}{2}} \right) \\
  \left\|\hat\E_2[\eta_2(x) x\tp{2}] \right\|_F
      &\le \frac{(4L^2 R^2 + 2 S L R + 4S^2)R^2}{\sqrt{n}} \left( 1 + \sqrt{\frac{\log(1/\delta_1)}{2}} \right) \\
  \left\|\hat\E_3[\eta_3(x) x\tp{3}] \right\|_F
      &\le \frac{(8L^3 R^3 + 3 L^2 R^2 S + 6 L R S^2 + 2S^3) R^3}{\sqrt{n}} \left( 1 + \sqrt{\frac{\log(1/\delta_1)}{2}} \right).
\end{align*}

% {%
% \begin{align*}
% %   \| \hat\E_1[ \eta_1(x) x ] \|_\Lop &\le 
% %             \underbrace{ \|\hat\E_1[\innerp{M_1 - \beta_h}{x} x]\|_\op  }_{ O( L R^2     \sqrt{\frac{\log(1/\delta_1)}{n}} ) } +
% %             \underbrace{ \|\hat\E_1[\epsilon x]\|_\op                   }_{ O( R S  \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
%   %  &= O( S L R^2 \sqrt{\frac{\log(1/\delta_1)}{n}} ) \\
%     \| \hat\E_2[ \eta_2(x) x\tp{2} ] \|_\Lop &\le 
%             \underbrace{ \|\hat\E_2[\innerp{\beta_h\tp{2} - M_2}{x\tp{2}} x\tp{2}]\|_\op }_{ O( L^2 R^4      \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
%    &\quad + \underbrace{ \|\hat\E_2[2 \epsilon \innerp{\beta_h}{x} x\tp{2}]\|_\op        }_{ O( S L R^3 \sqrt{\frac{\log(1/\delta_1)}{n}} ) } 
%           + \underbrace{ \|\hat\E_2[(\epsilon^2 - S^2) x\tp{2}]\|_\op               }_{ O( S^2 R^2 \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
%   %&= O( S^2 L^2 R^4 \sqrt{\frac{\log^2(1/\delta_1)}{n}} ) \\
%   \| \hat\E_3[ \eta_3(x) x\tp{3} ] \|_\Lop &\le 
%             \underbrace{ \|\hat\E_3[\innerp{\beta_h\tp{3} - M_3}{x\tp{3}}                      x\tp{3}]\|_\op }_{O( L^3 R^6        \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
%    &\quad + \underbrace{ \|\hat\E_3[3 \epsilon \innerp{\beta_h\tp{2}}{x\tp{2}}                 x\tp{3}]\|_\op }_{O( S L^2 R^5 \sqrt{\frac{\log(1/\delta_1)}{n}} ) } 
%           + \underbrace{ \|\hat\E_3[\epsilon^3                                                 x\tp{3}]\|_\op }_{O( S^3 R^3   \sqrt{\frac{\log (1/\delta_1)}{n}} ) } \\
%    &\quad + \underbrace{ \|\hat\E_3[3(\epsilon^2 \innerp{\beta_h}{x} -S^2 \innerp{\hat M_1}{x}) x\tp{3}]\|_\op }_{O( (S^2 L R^4 + S^2 R^4) \sqrt{\frac{\log(1/\delta_1)}{n}} ) } \\
%   %&= O( S^3 L^3 R^6 \sqrt{\frac{\log^3(1/\delta_1)}{n}} ).
% \end{align*}
% }
% There are two additional considerations when bounding the terms in $\eta_3$.
% First, to bound $\hat\E_3[\epsilon^3]$, we use Lemma 19 of \cite{hsu13spherical}.
% Second, there is an additional bias since the estimator uses $\hat M_1$ instead of $M_1$; this bias is small by the previous bound (and is constructed on an independent dataset, $\sD_1$).
% Therefore, we incur another $O(S^2 \cdot R^4 \sqrt{\frac{\log(1/\delta_1)}{n}})$.
%We can handle the other terms using conventional means, the details of which can be found in \appendixref{sec:proofs}.
%Note that we are bounding quantities fairly crudely for the sake of simplicity.

Finally, taking $\delta_1 = \delta/3$, and taking the union bound over the bounds for $p \in \{1,2,3\}$,
we get our result.
\end{proof}

%%%%%%%%%%%%

\section{Tensor Decomposition}

Once we have estimated the moments from the data through regression, we apply the robust tensor eigen-decomposition algorithm to recover the parameters, $\beta_h$ and $\pi$. However, the algorithm is guaranteed to work only for symmetric matrices with (nearly) orthogonal eigenvectors, so as a first step, we will need to whiten the third-order moment tensor using the second moments. Once we get the eigenvalues and eigenvectors from this orthogonal tensor, we have to undo the transformation by applying an un-whitening step. In this section, we present error bounds for each step, and combine them to prove the following lemma,
\begin{lemma}[Tensor Decomposition with Whitening]
  \label{lem:tensorPower} Let $\|\hat M_2 - M_2\|_\op \le \epsilon$ and
  $\|\hat M_3 - M_3\|_\op \le \epsilon$, for some $\epsilon$ such that 
  $$\epsilon < 
    \min\left\{
      \frac{\sigmamin(M_2)}{2},
      \frac{1}{10 k \pi_1 \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)}
    \right\}.$$ 

  Then, there exists a permutation of indices such that  the parameter
  estimates found in step 2 of \citet[Algorithm 1]{ChagantyLiang2013}
  satisfy the following with probability at least $1 - \delta$,
  \begin{align*}
  \|\hat \pi - \pi \|_{\infty}
  &\le \frac{10}{3} \pi_1^{5/2} k \pi_1 
    \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)
    \epsilon \\
  \|\hat \beta_h - \beta_h\|_2
    &\le 4\sqrt{3/2} \|M_2\|_\op^{1/2} \sigma_k(M_2)^{-1} \epsilon  \\
  &\quad + 8 \|M_2\|_\op^{1/2} k \pi_1 
    \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)
    \epsilon.
  \end{align*}
  for all $h \in [k]$.
\end{lemma}

\begin{proof}
We will use the general notation, $\aerr{X} \eqdef \|\hat X - X\|_\Lop$
to represent the error of the estimate, $\hat X$, of $X$ in the operator
norm. 
\paragraph{Step 1: Whitening}
Let $W$ and $\hat W$ be the whitening matrices for $M_2$ and $\hat M_2$
respectively. Also define $\Winv$ and $\Whinv$ to be their
pseudo-inverses.

We will first show that the whitened tensors $T = M_3(W,W,W)$ and $\hat
T = \hat M_3(\hat W, \hat W, \hat W)$ are symmetric with orthogonal
eigenvectors. Recall that $M_2 = \sum_h \pi_h \beta_h\tp{2}$, and thus
$W \beta_h = \frac{v_h}{\sqrt{\pi_h}}$, where $v_h$ form an orthonormal
basis. Applying the whitening transform to $M_3$, we get, 
\begin{align}
  M_3 &= \sum_h \pi_h \beta_h\tp{3} \\
  M_3(W,W,W) &= \sum_h \pi_h (W\beta_h)\tp{3} \\
  &= \sum_h \frac{1}{\sqrt{\pi_h}} v_h\tp{3}.
\end{align}
Consequently, $T$ has orthogonal eigenvectors, with eigenvalues $1/\sqrt{\pi_h}$.

Let us now study how far $\hat T$ differs from $T$, in terms of the
errors of $M_2$ and $M_3$. To do so, we use the triangle inequality to
break the difference into a number of simple terms,
\begin{align*}
  \aerr{T} &= \|M_3(W,W,W) - \hat M_3(\hat W,\hat W, \hat W)\|_\op \\
           &\le 
           \| {M_3}(W,W,W) - {M_3}(W,W,\hat W) \|_\op
           + \| {M_3}(W,W,\hat W) - {M_3}(W, \hat W, \hat W)\|_\op \\
           &\quad 
           + \|{M_3}(W,\hat W,\hat W) - {M_3}(\hat W, \hat W, \hat W)\|_\op 
           + \|{M_3}(\hat W,\hat W,\hat W) - - \hat {M_3}(\hat W,\hat W, \hat W)\|_\op \\
           &\le
           \| {M_3} \|_\op \|W\|^2_\op \aerr{W} +
            \| {M_3} \|_\op \|\hat W\|_\op \|W\|_\op \aerr{W} +
            \| {M_3} \|_\op \|\hat W\|^2_\op \aerr{W} +
            \aerr{M_3} \|\hat W\|^3_\op  \\
           &\le
           \| {M_3} \|_\op (\|W\|^2_\op + \|\hat W\|_\op \|W\|_\op + \|\hat W\|^2_\op) \aerr{W} +
            \aerr{M_3} \|\hat W\|^3_\op 
\end{align*}
We can relate $\|\hat W\|$ and $\aerr{W}$ to $\aerr{M_2}$ using 
using \refprop{white}. By the assumption that $\epsilon < \sigma_k/2$, we have,
\begin{align*}
  \|\hat W\|_\op &\le \sqrt{2} \sigma_k(M_2)^{-1/2} \\
  \aerr{W} &\le 4 \sigma_k(M_2)^{-3/2} \aerr{M_2}.
\end{align*}
Thus,
\begin{align*}
  \aerr{T} &\le 
  6 \| {M_3} \|_\op \|W\|^2_\op (4 \sigma_k(M_2)^{-3/2}) \aerr{M_2} +
  \aerr{M_3} 2\sqrt{2} \|W\|^3_\op \\
  &\le 
  24 \| {M_3} \|_\op \sigma_k(M_2)^{-5/2} \aerr{M_2} +
  2\sqrt{2} \sigma_k(M_2)^{-3/2} \aerr{M_3} \\
  &\le 
    \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)
    \epsilon.
\end{align*}

\paragraph{Step 2: Decomposition}

We have constructed $T$ to be a symmetric tensor with orthogonal
eigenvectors. We can now apply the results of \citet[Theorem
5.1]{AnandkumarGeHsu2012} to bound the error in the eigenvalues,
$\lambda_W$, and eigenvectors, $\omega$, returned by the robust tensor
power method;
\newcommand{\lW}{\lambda_W}
\newcommand{\lhW}{{\hat\lambda}_W}
\newcommand{\mW}{\omega}
\newcommand{\mhW}{{\hat\omega}}
\begin{align}
  \|\lW - \lhW \|_{\infty} 
  &\le \frac{5 k \epsilon_T}{(\lW)_{\min}} \\
\|\mW_h -\mhW_h \|_2 
  &\le \frac{8 k \epsilon_T}{(\lW)_{\min}^2},
\end{align}
for all $h \in [k]$, where $(\lW)_{min}$ is the smallest
eigenvalue of $T$. 

\paragraph{Step 3: Unwhitening}

Finally, we need to invert the whitening transformation to recover $\pi$ and
$\beta_h$ from $\lW$ and $\mW_h$. Let us complete the proof by
studying how this inversion relates the error in $\pi$ and $\beta$ to
the error in $\lW$ and $\mW$.

First, we will bound the error in the $\beta$s,
\begin{align*}
  \|\hat \beta_h - \beta_h\|_2
  &= \| \Whinv \mhW - \Winv \mW \|_2 \\
  &\le \aerr{\Winv} \|\mhW_h\|_2 + \|\Winv\|_2 \|\mhW_h - \mW_h \|_2. \comment{Triangle inequality}
\end{align*}

Once more, we can apply the results of \refprop{white}, with the assumptions on $\epsilon$, to get, 
\begin{align*}
  \|\Whinv\|_\op &\le \sqrt{3/2} \|M_2\|_\op^{1/2} \\
  \aerr{\Winv} &\le 4\sqrt{3/2} \|M_2\|_\op^{1/2} \sigma_k(M_2)^{-1} \aerr{M_2}.
\end{align*}

Thus,
\begin{align*}
  \|\hat \beta_h - \beta_h\|_2
  &\le 4\sqrt{3/2} \|M_2\|_\op^{1/2} \sigma_k(M_2)^{-1} \epsilon 
    + 8 \|M_2\|_\op^{1/2} \frac{k \epsilon_{T}}{(\lW)_{min}^2} \\
  &\le 4\sqrt{3/2} \|M_2\|_\op^{1/2} \sigma_k(M_2)^{-1} \epsilon  \\
  &\quad + 8 \|M_2\|_\op^{1/2} k \pi_1 
    \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)
   \epsilon.
\end{align*}

Next, let us bound the error in $\pi$,
\begin{align*}
  |\hat \pi_h - \pi_h |
  &= \left| \frac{1}{(\lW)_h^2} - \frac{1}{(\lhW)_h^2} \right| \\
  &= \left| \frac{\left( (\lW)_h + (\lhW)_h \right) \left( (\lW)_h - (\lhW)_h \right)}
  {(\lW)_h^2(\lhW)_h^2} \right| \\
  &\le \frac{( 2(\lW)_h - \|\lW - \lhW\|_{\infty} )}{(\lW)_h^2 \left( (\lW)_h + \|\lW - \lhW\|_{\infty} \right)^2} \|\lW - \lhW\|_{\infty}.
\end{align*}
Recall that $(\lW)_h = \pi_h^{-1/2}$, so the
assumptions that $\epsilon$ imply that $\aerr{T} \le \frac{1}{10 k \pi_1}$, and thus, $\|\lW - \lhW \|_{\infty} \le (\lW)_{\min}/2$. This allows us to simplify the above expression as follows,
\begin{align*}
  |\hat \pi_h - \pi_h |
  &\le \frac{(3/2)(\lW)_h}{(3/2)^2 (\lW)_h^4}
  \|\lW - \lhW\|_{\infty} \\
  &\le \frac{2}{3(\lW)_h^3} \frac{5 k \aerr{T}}{(\lW)_{\min}^2} \\
 &\le \frac{2 \pi_1^{3/2}}{3} 5 k \pi_1 
    \sigma_k(M_2)^{-3/2}
      \left(24 \frac{\| {M_3} \|_\op}{\sigma_k(M_2)} + 2\sqrt{2} \right)  \epsilon.
\end{align*}

\end{proof}

\section{Basic Lemmas}

\begin{lemma}[Concentration of vector norms]
  \label{lem:conc-norms}
  Let $X, X_1, \cdots, X_n \in \Re^d$ be i.i.d.\ samples
  from some distribution with bounded support
  ($\|X\|_2 \le M$ with probability 1).
  Then with probability at least $1 - \delta$,
  \begin{align}
    \left\| \frac{1}{n} \sum_{i=1}^{n} X_i - \E[X] \right\|_2 &
    \le \frac{2M}{\sqrt{n}} \left(1 + \sqrt{\frac{\log(1/\delta)}{2}}\right).
  \end{align}
\end{lemma}
\begin{proof}
  Define $Z_i = X_i - \E[X]$.

%  First, $\|Z_i\|_2 \le \|X_i\|_2 + \|\E[X_i]\|_2 \le 2M$,
%  where the first inequality is due to the triangle inequality,
%  and the second follows by Jensen's inequality on $\|\cdot\|_2$
%  and the boundedness assumption on $X_i$.

The quantity we want to bound can be expressed as follows:
  \begin{align}
  f(Z_1, Z_2, \cdots, Z_n) = \left\| \frac1n \sum_{i=1}^n Z_i \right\|_2.
  \end{align}

Let us check that $f$ satisfies the bounded differences inequality:
  \begin{align}
|f(Z_1, \cdots, Z_i, \cdots, Z_n) - f(Z_1, \cdots, Z_i', \cdots, Z_n)|
& \le \frac1n \|Z_i - Z_i'\|_2 \\
& = \frac1n \|X_i - X_i'\|_2 \\
&\le \frac{2M}{n},
  \end{align}
  by the bounded assumption of $X_i$ and the triangle inequality.

By McDiarmid's inequality,
with probability at least $1 - \delta$,
we have:
\begin{align}
\Pr[f - \E[f] \ge \epsilon] \le
\exp\left(\frac{-2 \epsilon^2}{\sum_{i=1}^n (2M/n)^2}\right).
\end{align}
Re-arranging:
\begin{align}
  \left\|\frac{1}{n}\sum_{i=1}^n Z_i\right\|_2
  &\le \E\left[ \left\| \frac{1}{n} \sum_{i=1}^n Z_i \right\|_2 \right]
  + M\sqrt{\frac{2\log(1/\delta)}{n}}.
\end{align}

Now it remains to bound $\E[f]$.
By Jensen's inequality, $\E[f] \le \sqrt{\E[f^2]}$,
so it suffices to bound $\E[f^2]$:
\begin{align}
  \E\left[ \frac1{n^2} \left\| \sum_{i=1}^n Z_i \right\|^2 \right]
  &= \E\left[ \frac1{n^2} \sum_{i=1}^n \|Z_i\|_2^2 \right] +
\E\left[ \frac1{n^2} \sum_{i\neq j} \innerp{Z_i}{Z_j} \right] \\ 
& \le \frac{4M^2}{n} + 0,
\end{align}
where the cross terms are zero by independence of the $Z_i$'s.

Putting everything together, we obtain the desired bound:
\begin{align}
\left\|\frac{1}{n}\sum_{i=1}^n Z_i \right\|
&\le \frac{2M}{\sqrt{n}} + M \sqrt{\frac{2\log(1/\delta)}{n}}.
\end{align}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Remark}: The above result can be directly
applied to the Frobenius norm of a matrix
$M$ because $\|M\|_F = \|\vvec(M)\|_2$.

\begin{proposition}[Perturbation Bounds on Whitening Matrices]
  \label{prop:white}
  Let $A$ be a rank-k $d\times d$ matrix, $\Wp$ be a $d \times k$ matrix that
  whitens $\hat A$, i.e. $\Wp^T \Ap \Wp = I$.  Suppose $\Wp^T A \Wp
  = U D U^T$, then define $W = \hat{W} U D^{-\half} U^T$. Note that $W$
  is also a $d \times k$ matrix that whitens $A$. Let $\serr{A}
  = \frac{\aerr{A}}{\sigma_k(A)}$. 

  Then, 
  \begin{align*}
    \|\hat W\|_\op 
      &\le \frac{\|W\|_\op}{\sqrt{1 - \serr{A}} } \\
  \|\Whinv\|_\op 
    &\le \|\Winv\|_\op \sqrt{1 + \serr{A}} \\
    \aerr{W} 
      &\le 2 \|W\|_\op \frac{\serr{A}}{1 - \serr{A}} \\
    \aerr{\Winv} 
      &\le 2 \|\Winv\|_\op \sqrt{1 + \serr{A}} \frac{\serr{A}}{1 - \serr{A}}.
  \end{align*}
\end{proposition}
\begin{proof}
  First, note that for a matrix $W$ that whitens $A = V \Sigma V^T$,
  $W = V \Sigma^{-\half} V^T$ and $\Winv = V \Sigma^{-\half} V^T$.
  This allows us to bound the operator norms of $\hat W$ and $\Whinv$ in
  terms of $W$ and $\Winv$,
  \begin{align*}
    \|\hat W\|_\op &= \frac{1}{\sqrt{\sigma_k(\hat A)}} \\
    &\le \frac{1}{\sqrt{\sigma_k({A}) - \aerr{A}} } \comment{By Weyl's Theorem} \\
    &\le \frac{\|W\|_\op}{\sqrt{1 - \serr{A}} } \\
    \|\Whinv\|_\op &= \sqrt{\sigma_1(\hat A)} \\
    &\le \sqrt{\sigmamax({A}) + \aerr{A}}  \comment{By Weyl's Theorem} \\
    &\le \sqrt{1 + \serr{A}} \|\Winv\|_\op.
  \end{align*}

  To find $\aerr{W}$, we will exploit the rotational invariance of the operator norm. 
  \begin{align*}
    \aerr{W} &= \| \Wp - W \|_\op \\
    &= \| W U D^{\half} U^T - W \|_\op  \comment{$W = U D^{-\half} U^T$}\\
    &\le \|W\|_\op \| I - U D^{\half} U^T \|_\op \comment{Sub-multiplicativity}\\
    &\le \|W\|_\op \| I - D \|_\op \\
    &= \|W\|_\op \| I - U D U^T \|_\op \comment{Rotational invariance}\\
    &\le \|W\|_\op \| \Wp^T \Ap_k \Wp - \Wp^T A \Wp \|_\op \comment{By definition}\\
    &\le \|W\|_\op ( \| \Wp^T (\Ap_k - \Ap) \Wp \|_\op + \| \Wp^T (\Ap - A) \Wp \|_\op) \\
    &\le \|W\|_\op \|\Wp\|_\op^2 (\sigma_{k+1}(\Ap) + \aerr{A}) \\
    &\le 2 \|W\|_\op \|\Wp\|_\op^2 \aerr{A}  \comment{Since $\sigma_{k+1}(A) = 0$}\\
    &\le 2 \|W\|_\op \frac{\serr{A}}{1 - \serr{A}} \comment{Using bound on $\|\hat W\|_\op$}.
  \end{align*}

  Similarly, we can bound the error on the un-whitening transform, $\Winv$,
  \begin{align*}
    \aerr{\pinv{W}} &= \| \pinv{\Wp} - \pinv{W} \|_\op \\
    &= \| \pinv{\Wp} U D^{\half} U^T - \pinv{W} \|_\op \\
    &\le \|\pinv{\Wp}\|_\op \| I - U D^{\half} U^T \|_\op \\
    &\le 2 \|\pinv{\Wp}\|_\op \|\Wp\|_\op^2 \aerr{A} \comment{From derivation of $\aerr{W}$}\\
    &\le 2 \|\Winv\|_\op \sqrt{1 + \serr{A}} \frac{\serr{A}}{1 - \serr{A}}.
  \end{align*}
\end{proof}

\bibliography{../ref,../pliang}

\end{document}
