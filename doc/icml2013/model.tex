\section{Model}
\label{sec:model}

\newcommand{\xn}[1]{x^{(#1)}}
\newcommand{\xni}{\xn{i}}
\newcommand{\yn}[1]{y^{(#1)}}
\newcommand{\yni}{\yn{i}}

The mixture of linear regressions model\citep{VieleTong} describes
response variables $y \in \Re$ dependent on covariates $x \in \Re^d$
through $K$ different modes which occur with probability $\pi_1, \dots,
\pi_K$ respectively. Each mixture component is independently described
by regression coefficients $\beta_k$ and we consider a common variance
term $\sigma^2$. Given $x$, the generative process for the response
variables is,
\begin{eqnarray*}
  h &\sim& \mult(\pi) \\
  y \mid h &\sim& \normal{ \beta_{h}^T x}{ \sigma^2 },
\end{eqnarray*}
where $h$ is a latent variable corresponding to the mode.

Our learning framework can be described as follows; we observe $N$
samples, $(\xn{1}, \yn{1}), \dots, (\xn{N}, \yn{N})$ and would like to
recover the regressors, $B = [\beta_1 \mid \beta_2 \mid \cdots \mid
\beta_K]$, and the mixture probabilities, $\pi = (\pi_1, \pi_2, \dots,
\pi_K)$. Note that we do not observe $h$. If we did, the optimal
solution for $\beta_k$ would be the regressor found using least squares
on the set of points, $\{(x_i, y_i) | h_i = k \}$.

% Typically people use EM, but the maximum marginal likelihood is
% non-convex.
Typically, the parameters $\pi, B$ are learnt usig
expectation-maximization\cite{VieleTong}, but the maximum marginal
likelihood is non-convex. We provide several instances where this leads
to poor local optima in \sectionref{sec:evaluation}.

% Reference mixture of experts. 
% Acknowledge that mixing proportions % can't depend on $x$
The mixture of linear regressions model is closely related to the
mixture of experts model\citationneeded, wherein the components can be
SVMs, GPs, regression trees and other estimators. In practice, it is
desirable to have mixture proportions that are dependent on $x$. We have
not considered these extensions in our treatment of the model.

