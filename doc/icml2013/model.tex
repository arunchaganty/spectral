\section{Model}
\label{sec:model}

\newcommand{\xn}[1]{x^{(#1)}}
\newcommand{\xni}{\xn{i}}
\newcommand{\yn}[1]{y^{(#1)}}
\newcommand{\yni}{\yn{i}}

The mixture of linear regressions model \citep{VieleTong2002} defines
a conditional distribution over a response $y \in \Re$
given covariates $x \in \Re^d$.
The generation of $y$ given $x$ involves two steps:
(i) draw the mixture component $h \in [K]$ according to mixture proportions
$\pi = (\pi_1, \dots, \pi_K)$;
and (ii) generate $y$ from a Gaussian centered at $\beta_h^\top x$ with
variance $\sigma^2$.
%Given $x$, the generative process for the response
%variables is,
\begin{eqnarray*}
  h &\sim& \mult(\pi), \\
  y \mid h &\sim& \normal{ \beta_{h}^T x}{ \sigma^2 }.
\end{eqnarray*}
%where $h$ is a latent variable corresponding to the mode.
The parameters of the model are $\theta = (\pi, B, \sigma^2)$,
where $\pi \in \Re^d$ are the mixture proportions,
$B = [\beta_1 \mid \dots \mid \beta_K] \in \Re^{d \times K}$
are the regression coefficients,
and $\sigma^2$ is the variance of the observation noise.
The learning problem is stated as follows:
given $N$ i.i.d. samples $(\xn{1}, \yn{1}), \dots, (\xn{N}, \yn{N})$
drawn from the model with some unknown parameters $\theta^*$,
return an estimate of the parameters $\hat\theta$.

%and would like to
%recover the regressors, $B = [\beta_1 \mid \beta_2 \mid \cdots \mid
%\beta_K]$, and the mixture probabilities, $\pi = (\pi_1, \pi_2, \dots,
%\pi_K)$.

%Note that we do not observe $h$. If we did, the optimal
%solution for $\beta_k$ would be the regressor found using least squares
%on the set of points, $\{(x_i, y_i) | h_i = k \}$.

% Typically people use EM, but the maximum marginal likelihood is
% non-convex.
The estimation problem is difficult primarily because the mixture
components $h$ are unobserved, rendering the marginal likelihood
a non-convex function of $\theta$.
Typically, the parameters are learned using
expectation maximization (EM) \cite{VieleTong2002},
which suffers from local optima
(we show this empirically in \sectionref{sec:evaluation}).
%We provide several instances where this leads
%to poor local optima in \sectionref{sec:evaluation}.

This model was used for XXX.

TODO: move to later
% Reference mixture of experts.
% Acknowledge that mixture proportions % can't depend on $x$
The mixture of linear regressions model is an instance of
mixture of experts \cite{jacobs91},
%wherein the components can be SVMs, GPs, regression trees and other estimators.
%In practice, it is
in which the mixture proportions are allowed to depend on $x$.
We have not considered these extensions in our treatment of the model.
