\section{Model}
\label{sec:model}

\newcommand{\xn}[1]{x^{(#1)}}
\newcommand{\xni}{\xn{i}}
\newcommand{\yn}[1]{y^{(#1)}}
\newcommand{\yni}{\yn{i}}

The mixture of linear regressions model \citep{VieleTong2002} defines
a conditional distribution over a response $y \in \Re$
given covariates $x \in \Re^d$.
The generation of $y$ given $x$ involves two steps:
(i) draw the mixture component $h \in [K]$ according to mixture proportions
$\pi = (\pi_1, \dots, \pi_K)$;
and (ii) generate $y$ from a Gaussian centered at $\beta_h^\top x$ with
variance $\sigma^2$.
\begin{eqnarray*}
  h &\sim& \mult(\pi), \\
  y \mid h &\sim& \normal{ \beta_{h}^T x}{ \sigma^2 }.
\end{eqnarray*}
The parameters of the model are $\theta = (\pi, B)$,
where $\pi \in \Re^d$ are the mixture proportions and
$B = [\beta_1 \mid \dots \mid \beta_K] \in \Re^{d \times K}$
are the regression coefficients.
We assume the variance of the observation noise $\sigma^2$ is known and that the mixture components are independent.
The learning problem is stated as follows:
given $n$ i.i.d. samples $(\xn{1}, \yn{1}), \dots, (\xn{n}, \yn{n})$
drawn from the model with some unknown parameters $\theta^*$,
return an estimate of the parameters $\hat\theta$.

% History of model
The mixture of linear regressions model has been applied
in the statistics literature for modeling music perception, where $x$ is the
actual tone and $y$ is the tone perceived by a musician \cite{VieleTong2002}.
% Reference mixture of experts.
% Acknowledge that mixture proportions % can't depend on $x$
The model is an instance of hierarchical mixture of experts
\cite{jacobs91experts}, in which the mixture proportions are allowed to depend
on $x$, known as a gating function.
This dependence allow the experts to be localized in input space,
providing more flexibility, but we do not consider this dependence in our model.

% Typically people use EM, but the maximum marginal likelihood is
% non-convex.
The estimation problem for mixture of linear regressions is difficult because
the mixture components $h$ are unobserved,
resulting in a non-convex log marginal likelihood.
The parameters are typically learned using
expectation maximization (EM) or Gibbs sampling \cite{VieleTong2002},
which suffers from local optima.
In the next section, we present a new algorithm
that sidesteps the local optima problem entirely.
%(we show this empirically in \sectionref{sec:evaluation}).
%We provide several instances where this leads
%to poor local optima in \sectionref{sec:evaluation}.
