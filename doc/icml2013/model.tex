\section{Model}
\label{sec:model}

\newcommand{\xn}[1]{x^{(#1)}}
\newcommand{\xni}{\xn{i}}
\newcommand{\yn}[1]{y^{(#1)}}
\newcommand{\yni}{\yn{i}}

The mixture of linear regressions model \citep{VieleTong2002} defines
a conditional distribution over a response $y \in \Re$
given covariates $x \in \Re^d$.
Let $k$ be the number of mixture components.
The generation of $y$ given $x$ involves three steps:
(i) draw a mixture component $h \in [k]$ according to mixture proportions
$\pi = (\pi_1, \dots, \pi_k)$;
(ii) draw observation noise $\epsilon$ from a known zero-mean noise distribution $\sE$,
and (ii) set $y$ deterministically based on $h$ and $\epsilon$.
More compactly: %to $\beta_h^\top x$ plus some observation noise $\epsilon$.
\begin{eqnarray}
  h &\sim& \mult(\pi), \\
  \epsilon &\sim& \sE, \\
  y &=& \beta_{h}^T x + \epsilon.
\end{eqnarray}
The parameters of the model are $\theta = (\pi, B)$,
where $\pi \in \Re^d$ are the mixture proportions and
$B = [\beta_1 \mid \dots \mid \beta_k] \in \Re^{d \times k}$
are the regression coefficients.
Note that the choice of mixture component $h$ and the observation noise $\epsilon$ are independent.
%We also assume the distribution of the observation noise is known and has bounded support.
The learning problem is stated as follows:
given $n$ i.i.d. samples $(\xn{1}, \yn{1}), \dots, (\xn{n}, \yn{n})$
drawn from the model with some unknown parameters $\theta^*$,
return an estimate of the parameters $\hat\theta$.

% History of model
The mixture of linear regressions model has been applied
in the statistics literature for modeling music perception, where $x$ is the
actual tone and $y$ is the tone perceived by a musician \cite{VieleTong2002}.
% Reference mixture of experts.
% Acknowledge that mixture proportions % can't depend on $x$
The model is an instance of the hierarchical mixture of experts
\cite{jacobs91experts}, in which the mixture proportions are allowed to depend
on $x$, known as a gating function.
This dependence allow the experts to be localized in input space,
providing more flexibility, but we do not consider this dependence in our model.

% Typically people use EM, but the maximum marginal likelihood is
% non-convex.
The estimation problem for a mixture of linear regressions is difficult because
the mixture components $h$ are unobserved,
resulting in a non-convex log marginal likelihood.
The parameters are typically learned using
expectation maximization (EM) or Gibbs sampling \cite{VieleTong2002},
which suffers from local optima.
In the next section, we present a new algorithm
that sidesteps the local optima problem entirely.
%(we show this empirically in \sectionref{sec:evaluation}).
%We provide several instances where this leads
%to poor local optima in \sectionref{sec:evaluation}.
