\section{Empirical Evaluation}
\label{sec:evaluation}

\subsection{Setup}

\paragraph{Algorithms}

We ran expectation-maximization until the log likelihood converged
$10^-5$. Initialized the entries of $\beta$ with a random Gaussian, $\normal{0}{1}$.

In all our experiments, EM converged.

Describe spectral (briefly pointing to algorithm);
initialization, clipping.

Describe spectral+EM

\paragraph{Datasets}
Describe the Curves dataset.
$x = (1, t, t^4, t^9)$
$d = b^p$

Distribution over $x$.

Canonical settings:

Default values:
$n = 10^5$
$k = 5$

Dataset 1: visualization ($b = 1, p = 3$) 
Plot for visualization

Table on different settings: 
  (To show that we work comparably or better to EM always)
  (Spawned!)

Dataset 2: $b = 30$, $p = 1$
(Spawned many tasks to find spec performance. Will run EM on interesting cases)


\paragraph{Motion tracking data?}

\subsection{Results}

\paragraph{Basic results}

Punchline: spectral by itself is medicore, offers a good initialization to EM.
EM with random initialization is much less stable. (by table)

Using canonical settings, on the various datasets

Plot the parameters: the curves, three algorithms (DONE)

Plot: the histogram over errors (for EM?)

\paragraph{Effect on number of data points}

Curves dataset 2.
Plot: $n$ versus error for various algorithms

Point: tradeoff between statistical error and computational error;
not enough data, spectral is actually a lot worse
given enough data, spectral+EM is much better

\paragraph{Effect of dimensionality}

\paragraph{Effect of noise}

\paragraph{Effect of separation between components}
