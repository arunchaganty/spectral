\section{Introduction}
\label{sec:intro}

% Latent variables are important
% Local optima is problem
% Recent method of moments spectral methods sidestep local optima

Latent variable models have routinely been used in probabilistic
modelling, (examples). Parameters for these models have been learnt
through a variety of methods, EM, variational methods, etc. The local
nature of these algorithms lead them to converge to local optima.
Recently, there has been an interest in moment of methods estimators
with polynomial sample and computational complexities. These estimators
are statistically consistent, i.e. given sufficient data they are
guaranteed to converge to the global optimum in an identifiable model,
thus sidestepping the local optima issue altogether.

% Discriminative models work better in practice
% Discriminative models with latent variables add expressive power (examples)
This approach has been applied to a variety of generative models, e.g.
hidden Markov models \cite{AnandkumarHsuKakade2012}, probabilistic
context free grammars \cite{HsuKakadeLiang2012}, etc. However, in
practice it has been observed that discriminative models work better in
terms of parameter recovery\citationneeded, particularly in the large
sample regime. \todo{Examples! CRFs, etc.}

% This paper: first step in establishing provably correct estimation
% with simple mixture of linear regression
In this work, we provide a first step in establishing provably correct
estimation for such discriminative models by proposing an algorithm for
the simple mixture of linear regressions. The mixture of linear
regressions model \todo{models ?}. We describe the model formally in more
detail in \sectionref{sec:model}.

% Previous work: moment equations solvable by eigendecomposition or
% tensor factorization
While method of moment estimators were \todo{described by Fisher(?)},
they required as many moments as there were parameters, and hence deemed
impractical. Recently, \citet{AnandkumarHsuKakade2012} showed how using
eigendecomposition, the moment equations could be solved using just the
first three moments of data.

% Challenge: E[X^m Y^n] doesn't work
In the discriminative setting, we are trying to learn the parameters for
a model for $y | X$. However, as the response variables are just
scalars, it is not possible to recover the moments simply from $\E[ X^m
y^n ].$ At the same time, knowing the $X$ should give us more
information than the scalar $y$.

% Approach: Derive moment functions based on linear regression on Y^2
% and Y^3 Algorithm: trace norm regularization to recover tensor
% + tensor factorization
We propose an algorithm in \sectionref{sec:algo} that derives moment
functions based on linear regression on the moments of $y$, i.e.
$\E[y^2]$ and $\E[y^3]$. We exploit the low rank properties of the
second and third order moments to efficiently recover parameters, and
then recover the individual regressors, $\beta_k$, using the tensor
factorization method \cite{AnandkumarGeHsu2012}.

% Show some identifiability results; require quadratic and cubic
% independence
We prove that this approach is indeed statistically consistent with
polynomial sample complexity in \sectionref{sec:theory}. Aside from
standard identifiability results \citationneeded, we also show that
recovery using moments places some independence requirements on the
variables $X$. 

% Empirical evaluation
% It works
%  EM is sensitive to initialization (local procdure)
%  Spectral provides good initialization
% Intuition: basin of attraction, local methods like EM improve

We evaluate our algorithm on simulated linear and non-linear data to
understand how well the algorithm scales with the number of components,
dimensions and separation. To show how robust the algorithm is to model
misspecification, we evaluate it's performance on \todo{a motion
tracking dataset(?)}. 

% What properties of the problem make it hard on EM versus spectral?
%  Separation, dimensionality, number of clusters, etc.
% Real dataset?

We compare our approach with expectation-maximization; our results,
presented in \sectionref{sec:evaluation} shows that spectral methods do
indeed provide an answer close enough to the global optima that using
a local method from there recovers the true parameters. \todo{Describe
some of our other high-level findings.} 

\todo{What about talking about initialization? Generalizing and kmeans++} 

