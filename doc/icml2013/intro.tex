\section{Introduction}
\label{sec:intro}

% Discriminative models work better in practice Discriminative models
% with latent variables add expressive power (examples)
Discriminative latent-variable models,
which combine the high accuracy of discriminative models
with the compact expressiveness of latent-variable models,
have been widely applied to many tasks, including
object recognition \cite{quattoni04crf},
human action recognition \cite{wang09crf},
syntactic parsing \cite{petrov08discriminative},
and machine translation \cite{liang06discrimative}.
However, parameter estimation in these models is difficult;
past approaches have relied exclusively on local optimization (EM, 
gradient descent) and are vulnerable to local optima.

% Goal of this paper
% This paper: first step in establishing provably correct estimation
% with simple mixture of linear regression
Our broad goal is to develop efficient provably consistent estimators for
discriminative latent-variable models.
In this paper, we provide a first step in this 
direction by proposing a new algorithm for a simple model,
\emph{mixture of linear regressions} (MLG) \cite{VieleTong2002}.

% Recent method of moments spectral methods sidestep local optima
Recently, method of moments estimators have been developed for
\emph{generative} latent-variable models, including
mixture models, HMMs \cite{AnandkumarHsuKakade2012},
Latent Dirichlet Allocation \cite{anandkumar12lda},
and parsing models \cite{hsu12identifiability}.
The basic idea of this work is to express
the unknown model parameters as a tensor factorization
of the third-order moments of the model distribution, which
can be estimated from data.
The moments have a special symmetric structure
which permits the factorization to be computed efficiently using the robust
tensor power method \cite{AnandkumarGeHsu2012}.

% Twist
In mixture of linear regressions, using third-order moments does not
directly reveal the tensor structure of the problem, so we cannot
directly apply the above tensor factorization techniques.  Our solution
is to employ low-rank linear regression
\cite{NegahbanWainwright2009,Tomioka2011} on second and third powers of
the response to recover the appropriate symmetric tensors, from which we
can then apply the tensor power method to retrieve the final parameters.

% Approach: Derive moment functions based on linear regression on Y^2
% and Y^3 Algorithm: trace norm regularization to recover tensor
% + tensor factorization
%We propose an algorithm in \sectionref{sec:algo} that derives moment
%functions based on linear regression on the conditional moments of $y$,
%i.e. $\E[y^2 | X]$ and $\E[y^3 | X]$. We exploit the low rank properties
%of the second and third order moments to efficiently recover parameters,
%and then recover the individual regressors, $\beta_k$, using the tensor
%factorization method \cite{AnandkumarGeHsu2012}.


%These estimators feature polynomial
%sample and computational complexities while being statistically
%consistent, i.e. given sufficient data they are guaranteed to converge
%to the global optimum in an identifiable model, thus sidestepping the
%local optima issue altogether.

% Previous work: moment equations solvable by eigendecomposition or
% tensor factorization
%While method of moment estimators were \todo{described by Fisher(?)},
%they required as many moments as there were parameters, and hence deemed
%impractical. Recently, \citet{AnandkumarHsuKakade2012} showed how using
%eigendecomposition, the moment equations could be solved using just the
%first three moments of data.

% Challenge: E[X^m Y^n] doesn't work
%In the discriminative setting, we are trying to learn the parameters for
%a model for $y | X$. However, as the response variables are just
%scalars, it is not possible to recover the moments simply from $\E[ X^m
%y^n ].$ At the same time, knowing the $X$ should give us more
%information than the scalar $y$.

% Show some identifiability results; require quadratic and cubic
% independence
%We prove that this approach is indeed statistically consistent with
%polynomial sample complexity.
% PSL: leave out for now (too detailed)
%Aside from
%standard identifiability results \citationneeded, we also show that
%recovery using moments places some independence requirements on the
%variables $X$. 

% Empirical evaluation
% It works
%  EM is sensitive to initialization (local procdure)
%  Spectral provides good initialization
% Intuition: basin of attraction, local methods like EM improve

% Results
The result is a simple and efficient two-stage algorithm,
which we call Spectral Experts.
We prove that our algorithm yields consistent parameter estimates under certain
new identifiability conditions.  We also conduct an empirical evaluation
of our technique to understand its statistical properties (\sectionref{sec:evaluation}).
In particular, we find that in the low data regime,
Spectral Experts underperforms EM due to its weaker statistical efficiency,
but in the high data regime,
Spectral Experts outperforms EM.
We get the best results by using Spectral Experts as an initialization
to EM.

%We evaluate our algorithm on simulated linear and non-linear data to
%understand how well the algorithm scales with the number of components,
%dimensions and separation.

%To show how robust the algorithm is to model
%misspecification, we evaluate it's performance on \todo{a motion
%tracking dataset(?)}. 

% What properties of the problem make it hard on EM versus spectral?
%  Separation, dimensionality, number of clusters, etc.
% Real dataset?

%We compare our approach with expectation-maximization; our results,
%presented in \sectionref{sec:evaluation} shows that spectral methods do
%indeed provide an answer close enough to the global optima that using
%a local method from there recovers the true parameters. \todo{Describe
%some of our other high-level findings.} 

%PSL: not for this paper
%\todo{What about talking about initialization? Generalizing and kmeans++} 

\subsection{Notation}

%We have already defined $y$ to be the response variable and $x \in
%\Re^d$ to be the covariates it depends on. Note that the matrix of
%regressors $B$ is a $d \times K$ matrix, where the $k$-th column is
%$\beta_k$.

Let $[n] = \{ 1, \dots, n \}$ denote the first $n$ positive integers.
We use $O(f(n))$ to denote a function $g(n)$ such that $\lim_{n \to\infty} g(n)/f(n) < \infty$.
We will only hide universal constants, exposing all dependence on the number of samples,
dimension, norms, noise variance, etc.

We use $x\tp{p}$ to represent the $p$-th order tensor
formed by taking the outer product of $x \in \Re^d$; i.e. $x\tp{p}_{i_1 \ldots
i_p} = x_{i_1} \cdots x_{i_p}$. We will use
$\innerp{\cdot}{\cdot}$ to denote the generalized dot product between two $p$-th
order tensors: $\innerp{X}{Y} = \sum_{i_1, \ldots i_p} X_{i_1,
\ldots i_p} Y_{i_1, \ldots i_p}.$
A tensor $X$ is symmetric if for all $i,j \in [d]^p$ which are permutations of each other,
$X_{i_1 \cdots i_p}$ = $X_{j_1 \cdots j_p}$ (all tensors in this paper will be symmetric).
For a $p$-th order tensor $X \in (\Re^d)\tp{p}$, let $X_{(i)} \in \Re^{d \times d^{p-1}}$
be the mode-$i$ unfolding of $X$.
%For $X \in (\Re^d)\tp{p}$ and $Y \in (\Re^d)\tp{(p+q)}$,% $p, q > 0$,
%$\innerp{X}{Y} \in \Re^{d^q}$ is the projection of $X$ onto the first
%$p$ modes of $Y$, i.e. $\innerp{X}{Y}_{i_1, \ldots, i_q}
%= \sum_{j_1, \ldots, j_p} X_{j_1, \ldots, j_p} Y_{j_1, \ldots, j_p, i_1,
%\ldots, i_q}$.

% Norms
For a matrix $X$,
let $\|X\|_*$ denote the nuclear (trace) norm (sum of singular values),
let $\|X\|_F$ denote the Frobenius norm (square root of sum of squares of singular values),
let $\|X\|_\text{op}$ denote the operator norm (largest singular value).
For a rank-$K$ tensor $X$,
let $\|X\|_* = \frac{1}{p} \sum_{i=1}^p \|X_{(i)}\|_*$ denote
the average nuclear norm over all $p$ unfoldings,
and let $\|X\|_\text{op} = \frac{1}{p} \sum_{i=1}^p \|X_{(i)}\|_\text{op}$
denote the average operator norm over all $p$ unfoldings.
Let $\sigma_\text{min}(X)$ be the smallest singular value of $X$.

For a tensor $X \in (\Re^d)\tp{p}$,
let $\cvec(X) \in \Re^{\sum_{i=1}^p \binom{n}{i}}$ be
the collapsed vectorization of $X$: each component of $\cvec(X)$
is indexed by a vector of counts $(c_1, \dots, c_d)$ with total sum $\sum_i c_i = p$.
The value of that component is
$\sum_{k \in K(c)} X_{k_1 \cdots k_p}$,
where $K(c) = \{ k \in [d]^p : \forall i \in [d], c_i = |\{ j \in [p] : k_j = i \}| \}$ are the set of index vectors $k$ with that count profile.
For example, for $X \in \Re^{d \times d}$,
$\cvec(X) = (X_{ii} : i \in [d]; X_{ij} + X_{ji} : i,j \in [d], i<j)$.
%For symmetric tensors, $\innerp{X}{Y} = \innerp{\cvec(X)}{\cvec(Y)}$.
