%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical results}
\label{sec:theory}

In this section, we provide theoretical guarantees for the Spectral Experts algorithm.
Our main result shows that the parameter estimates $\hat\theta$ converge to $\theta$
at a $\frac{1}{\sqrt{n}}$ rate that depends polynomially on the bounds on the
parameters, covariates, and noise, as well the $k$-th smallest singular values
of the compound parameters and various covariance matrices.

\begin{theorem}[Convergence of Spectral Experts]
\label{thm:convergence}
Assume each dataset $\sD_p$ (for $p = 1, 2, 3$) consists of $n$ i.i.d.\ points independently drawn from a mixture
of linear regressions model with parameter $\theta^*$.\footnote{Having three independent copies simplifies the analysis.}
Further, assume 
$\|x\|_2 \le R$, 
$\|\beta_h^*\|_2 \le L$ for all $h \in [k]$,
$|\epsilon| \le S$
and $B$ is rank $k$.
Let $\Sigma_p \eqdef \E[\cvec(x\tp{p})\tp{2}]$, 
and assume $\Sigma_p \succ 0$ for each $p \in \{1,2,3\}$.
Let $\epsilon < \half$.
Suppose the number of samples is
$n = \max(n_1,n_2)$
where 
\begin{align*}
n_1 &= \Omega \left(\frac{R^{12} \log(1/\delta)}{\min_{p \in [3]} \sigmamin(\Sigma_p)^2} \right) \\
n_2 &= \Omega \left(\epsilon^{-2}~ \frac{k^2 \pi^2_{\max} \|M_2\|_\op^{1/2} \|M_3\|_\op^2 { L^{6} S^{6} R^{12}}}{\sigma_k(M_2)^{5} {\sigmamin(\Sigma_1)^2}} \log(1/\delta) \right).
\end{align*}
If each regularization strength $\lambda_n^{(p)}$ is set to 
$$\Theta\left( \frac{L^p S^p R^{2p}}{\sigmamin(\Sigma_1)^2} \sqrt{\frac{\log(1/\delta)}{n}} \right),$$
for $p \in 2, 3$,
%$O\left(\frac{\sigmamin(\Sigma_p)}{\sqrt{k}}~ \epsilon \right)$, 
%$\Omega\left(\sigma^3 L^3 R^6 \sqrt{\frac{\log(1/\delta)}{n}}\right)$,
then the parameter estimates $\hat\theta = (\hat\pi, \hat B)$ returned by
\algorithmref{algo:spectral-experts} (with the columns appropriately permuted)
satisfies 
  \begin{align*}
  \|\hat \pi - \pi \|_{\infty} \le \epsilon \quad\quad 
  %&= O\left(\frac{k \pi_{\max}^{5/2}\| {M_3} \|_\op}{\sigma_k(M_2)^{5/2}} ~ \epsilon \right) \\
  \|\hat \beta_h - \beta_h\|_2 \le \epsilon
  %&= O\left( \frac{k \pi_{\max} \|M_2\|_\op^{1/2} \| {M_3} \|_\op}{\sigma_k(M_2)^{5/2}}~ \epsilon \right),
  \end{align*}
  for all $h \in [k]$.
%$\|\hat\pi - \pi^*\|_{\infty} \le \epsilon$
%and for all $h \in [k]$,
%$\|\hat\beta_h - \beta^*_h\|_2 \le \frac{\epsilon}{\sqrt{\pi_h^*}}$.
\end{theorem}

While the dependence on some of the norms ($L^6,S^6,R^{12}$) looks formidable,
it is in some sense unavoidable, since we
need to perform regression on third-order moments.
Classically, the number of samples required is squared norm of the covariance matrix,
which itself is bounded by the squared norm of the data, $R^3$. This
third-order dependence also shows up in the regularization strengths;
the cubic terms bound each of $\epsilon^3$,
$\beta_h^3$ and $\|(x\tp{3})\tp{2}\|_F$ with high probability. 

The proof of the theorem has two parts.
First, we bound the error in the compound parameters estimates $\hat M_2,\hat M_3$
using results from \citet{Tomioka2011}.
Then we use results from \citet{AnandkumarGeHsu2012} to convert this error
into a bound on the actual parameter estimates $\hat\theta = (\hat\pi, \hat B)$
derived from the robust tensor power method.
But first, let us study a more basic property: identifiability.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Identifiability from moments}

In ordinary linear regression, the regression coefficients $\beta \in
\Re^d$ are identifiable if and only if the data has full rank:
$\E[x\tp{2}] \succ 0$, and furthermore, identifying $\beta$ requires
only moments $\E[xy]$ and $\E[x\tp{2}]$ (by observing the optimality
conditions for \refeqn{y1}).  However, in mixture of linear regressions,
these two moments only allow us to recover $M_1$.  \refthm{convergence}
shows that if we have the higher order analogues, $\E[x\tp{p}y\tp{p}]$
and $\E[x\tp{2p}]$ for $p \in \{1,2,3\}$, we can then identify the
parameters $\theta = (\pi, B)$,
provided the following \emph{identifiability condition} holds: $\E[\cvec(x\tp{p})\tp{2}] \succ
0$ for $p \in \{1,2,3\}$.

This identifiability condition warrants a little care,
as we can run into trouble when components of $x$ are dependent on each other
in a particular algebraic way.
For example, suppose $x = (1, t, t^2)$, the common polynomial
basis expansion, so that all the coordinates are deterministically
related.  While $\E[x\tp{2}] \succ 0$ might be satisfied (sufficient for ordinary linear regression),
$\E[\cvec(x\tp{2})\tp{2}]$ is singular for
any data distribution.
To see this, note that $\cvec(x\tp{2}) = [1 \cdot 1, t\cdot t, 2(1
\cdot t^2), 2(t \cdot t^2), (t^2 \cdot t^2)]$ contains components $t
\cdot t$ and $2(1 \cdot t^2)$, which are linearly dependent.  Therefore,
Spectral Experts would not be able to identify the parameters of
a mixture of linear regressions for this data distribution.

We can show that some amount of unidentifiability is intrinsic to
estimation from low-order moments, not just an artefact of our
estimation procedure.  Suppose $x = (t, \dots, t^d)$.  Even if we
observed all moments $\E[x\tp{p}y\tp{p}]$ and $\E[x\tp{2p}]$ for $p \in
[r]$ for some $r$, all the resulting coordinates would be monomials of $t$ up to only degree
$2dr$, and thus the moments live in a $2dr$-dimensional subspace.  On
the other hand, the parameters $\theta$ live in a subspace of at least
dimension $dk$.  Therefore, at least $r \ge k/2$ moments are required
for identifiability of any algorithm for this monomial example.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis of low-rank regression}
\label{sec:regression}

In this section, we will bound the error of
the compound parameter estimates $\|\Delta_2\|_F^2$ and $\|\Delta_3\|_F^2$,
where $\Delta_2 \eqdef \hat M_2 - M_2$
and $\Delta_3 \eqdef \hat M_3 - M_3$.
Our analysis is based on the low-rank regression framework of
\citet{Tomioka2011} for tensors, which builds on
\citet{NegahbanWainwright2009} for matrices.
The main calculation involved is controlling the noise $\eta_p(x)$,
which involves various polynomial combinations of the mixing noise and observation noise.

Let us first establish some notation that unifies the three regressions (\refeqn{estimateM1}, \refeqn{estimateM2}, and \refeqn{estimateM3}).
Define the observation operator $\opX_p(M_p) : \Re^{d\tp{p}} \to \Re^{n}$
mapping compound parameters $M_p$:
\begin{align}
\opX_p(M_p; \sD)_i &\eqdef \innerp{M_p}{x\tp{p}_i}, & (x_i, y_i) \in \sD.
\end{align}

Let $\kappa(\opX_p)$ be the restricted strong convexity constant,
and let $\opX^*_p(\eta_p; \sD) = \sum_{(x,y) \in \sD} \eta_p(x) x\tp{p}$
be the adjoint.

%\paragraph{Restricted strong convexity}

%Let us first lower bound the restricted strong convexity constant
%$\kappa(\opX_p)$:

%In the previous section, we described an algorithm for the mixture of
%linear regressions using regression to recover $M_2$ and $M_3$,
%described by \equationref{eqn:y2} and \equationref{eq:y3}, as
%a subroutine. In this section, we will characterize the rate of
%convergence of regression.

%Analysis for regression in the fixed and random design settings have
%been studied before \citep{HsuKakadeZhang}, however our setup differs
%substantially from the noise models assumed in the literature. In our
%scenario, the variance in the estimation comes not only from the
%Gaussian observation noise (which has been studied before), but also
%from the variance in the latent variable $h$.

%Let us now formally define the class of regression problems we wish to
%analyze, i.e. regression on the set $(x\tp{p}, y^p)$,
%\begin{align*}
%  y^p &= \innerp{x\tp{p}}{M_p} + (\innerp{x\tp{p}}{M_p - \beta_h\tp{p}} + \varepsilon).
%\end{align*}

%\todo{Describe/define the convex tensor stuff.}
%We would like to exploit
%the property that $M_p$ is low rank (as typically $k \ll D$). It has
%been shown that a convex relaxation for this problem regression with
%trace norm regularization, which can be solved using a proximal
%subgradient descent algorithm\citationneeded.
%The analogue of trace norm
%regularization for higher order tensors corresponds to the sum of the
%trace norms of the mode-k unfolding of the tensor \cite{Tomioka2011},
%$X_{(k)}$, is a $d_k \times (\prod_{k' \neq k} d_{k'})$ matrix obtained
%by concatenating the entries for all dimensions other than $k$. For
%example, the 1-mode unfolding of a 3rd order tensor has entries,
%$X_{(1)}^{i_1, (i_2, i_3)} = X_{i_1, i_2, i_3}$.

%In general, the optimization problem we'd like to solve is,
%\begin{align}
%  \hat M_p &= 
%  \arg\min_{M_p}& \frac{1}{2N} \| \vec y - \opX_p(M_p) \|_2^2 + \frac{\lambda_n}{k} \sum_{h=1}^k \| (M_p)_{(h)} \|_* \label{eq:regression}.
%\end{align}

\begin{lemma}[\citet{Tomioka2011}, Theorem 1]
\label{lem:lowRank}
Suppose there exists a restricted strong convexity constant $\kappa(\opX_p)$ such that
$$\frac{1}{n} \| \opX_p( \Delta )\|_2^2 \ge \kappa(\opX_p) \|\Delta\|^2_F \quad \text{and} \quad
\lambda^{(p)}_n \ge \frac{2 \|\opX_p^*(\eta_p)\|_\op}{n}.$$
Then the error of $\hat M_p$ is bounded as follows:
$$\| \hat M_p - M_p \|_F \le \frac{32 \lambda^{(p)}_n \sqrt{k}}{\kappa(\opX_p)}.$$
\end{lemma}

Going forward, we need to lower bound the restricted strong convexity
constant $\kappa(\opX_p)$ and upper bound the operator norm of the adjoint operator
$\|\opX_p^*(\eta_p)\|_\op$. The proofs of the following lemmas follow
from standard concentration inequalities and are detailed in 
\iftoggle{withappendix}{
\appendixref{sec:proofs:regression}.
}{
the supplementary material.
}
%have been deferred to \appendixref{sec:proofs}. 

%We will appeal to the random design framework that models the input $x$
%as random and show bounds that hold with high probability.
%\paragraph{Adjoint operator}
%In this section, we upper bound the operator norm of the adjoint
%$\|\opX_p(\eta_p)\|_\op$.

% First, let us lower bound the restricted strong convexity parameter $\kappa(\opX_p)$:

\begin{lemma}[lower bound on restricted strong convexity constant]
\label{lem:lowRankLower}
%Let $\Sigma_p \eqdef \E[\cvec(x\tp{p})\tp{2}]$.
If $$n = \Omega \left(\max_{p\in [3]} \frac{R^{4p} (p!)^2 \log(1/\delta)}{\sigmamin(\Sigma_p)^2} \right),$$
then with probability at least $1-\delta$:
$$\kappa(\opX_p) \ge \frac{\sigmamin(\Sigma_p)}{2},$$
for each $p \in [3]$.
\end{lemma}

% Next, let us upper bound the operator norm of the observation adjoint:

\begin{lemma}[upper bound on adjoint operator]
\label{lem:lowRankUpper}
% Let $\opX_p$ be the linear operator previously defined. 
If $$n = \Omega \left(\max_{p\in [3]} \frac{L^{2p} S^{2p} R^{4p} \log(1/\delta)}{ \sigmamin(\Sigma_1)^2 \left(\lambda_n^{(p)}\right)^2} \right),$$
then with probability at least $1-\delta$:
$$\lambda_n^{(p)} \ge \frac1{n} \|\opX_p^*(\eta_p)\|_\op,$$
for each $p \in [3]$.
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis of the tensor factorization} 
\label{sec:tensorError}

Having bounded the error of the compound parameter estimates $\hat M_2$ and $\hat M_3$,
we will now study how this error propagates through the tensor factorization step of 
\algorithmref{algo:spectral-experts},
which includes whitening, applying the robust tensor power method \cite{AnandkumarGeHsu2012},
and unwhitening.
\begin{lemma}
  \label{lem:tensorPower}
  Let $M_3 = \sum_{h=1}^{k} \pi_h \beta_h\tp{3}$.
  Let $\|\hat M_2 - M_2\|_\op$ and $\|\hat M_3 - M_3\|_\op$ both be less than
  \vspace{-0.5em}
  $$\frac{\sigma_k(M_2)^{5/2}}{k \pi_{\max} \|M_2\|_\op^{1/2} \| {M_3} \|_\op}~ \epsilon,$$
  for some $\epsilon < \half$. 
  Then, there exists a permutation of indices such that  the parameter
  estimates found in step 2 of \algorithmref{algo:spectral-experts}
  satisfy the following with probability at least $1 - \delta$:
  \begin{align*}
  \|\hat \pi - \pi \|_{\infty} &\le \epsilon \\
  %&= O\left(\frac{k \pi_{\max}^{5/2}\| {M_3} \|_\op}{\sigma_k(M_2)^{5/2}} ~ \epsilon \right) \\
  \|\hat \beta_h - \beta_h\|_2 &\le \epsilon.
  %&= O\left( \frac{k \pi_{\max} \|M_2\|_\op^{1/2} \| {M_3} \|_\op}{ \sigma_k(M_2)^{5/2} }~ \epsilon \right),
  \end{align*}
  for all $h \in [k]$.
\end{lemma}

The proof follows by applying standard matrix perturbation results for
the whitening and unwhitening operators and 
\iftoggle{withappendix}{
can be found in \appendixref{sec:proofs:tensors}.
}{
has again been deferred to the supplementary material.
}
%\appendixref{sec:proofs}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Synthesis}
Together, these lemmas allow us to control the compound parameter error
and the recovery error. We now apply them in the proof of
\refthm{convergence}:

\begin{proof}[Proof of Theorem 1 (sketch)]
By \reflem{lowRank}, \reflem{lowRankLower} and \reflem{lowRankUpper}, we
can control the Frobenius norm of the error in the moments, which
directly upper bounds the operator norm: If $n \ge \max\{n_1, n_2\}$,
then
\begin{align}
  \|\hat M_p - M_p\|_\op = O\left( \lambda_n^{(p)} \sqrt{k} \sigmamin(\Sigma_p)^{-1} \right).
  %\|\hat M_p - M_p\|_\op = O\left( \sigma^3 L^3 R^6 k^{\frac12} \sigmamin(\Sigma_p)^{-1} \sqrt{\frac{\log^3 (1/\delta)}{n}} \right).
  %\|\hat M_p - M_p\|_F = O\left( \frac{\sigma^3 L^3 R^6 \sqrt{k} \log^3 (1/\delta)}{\sqrt{n} (\sigmamin(\Sigma_p) - d^p R^p \sqrt{\frac{p \log(d) \log(1/\delta)}{n}}} \right).
\end{align}

We complete the proof by applying \reflem{tensorPower} with the above
bound on $\|\hat M_p - M_p\|_\op$.

\end{proof}

