%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical results}
\label{sec:theory}

In this section, we provide theoretical guarantees for our Spectral Experts algorithm.
In particular, the following theorem provides a rate of convergence:

\begin{theorem}[Convergence of Spectral Experts]
\label{thm:convergence}
Each dataset $\sD_p$ (for $p = 1, 2, 3$) consists of $n$ i.i.d.\ points drawn from a mixture
of linear regressions model with parameter $\theta^*$
(having three independent copies simplifies the analysis).
Let $\|x\|_2 \le R$ with probability $1$
and let $\|\beta_h^*\|_2 \le L$ for all $h \in [k]$.
Let $\Sigma_p \eqdef \E[\cvec(x\tp{p})\tp{2}]$, 
and assume $\Sigma_p \succ 0$ for each $p \in \{1,2,3\}$.
Assume $d \ge k$.
Suppose the number of samples is
$n = \max(n_1,n_2)$
where $n_1 = \Omega\left(d^6 R^{12} (\max_{p \in [3]} \sigmamin(\Sigma_p)^{-2}) \log(d) \log(1/\delta)\right)$ and
$n_2 = \Omega\left( \sigma^6 L^6 R^{12} k (\max_{p \in [3]} \sigmamin(\Sigma_p)^{-1}) \frac{\log^3 (1/\delta)}{\epsilon^2} \right)$.
If the regularization strengths $\lambda_n^{(2)}$ and $\lambda_n^{(3)}$ are
set to $\Omega\left(\sigma^3 L^3 R^6 \sqrt{\frac{\log(1/\delta)}{n}}\right)$,
then the parameter estimates $\hat\theta = (\hat\pi, \hat B)$ returned by
\algorithmref{algo:spectral-experts} (with the columns appropriate permuted)
satisfies 
$\|\hat\pi - \pi^*\|_{\infty} \le \epsilon$
and for all $h \in [k]$,
$\|\hat\beta_h - \beta^*_h\|_2 \le \frac{\epsilon}{\sqrt{\pi_h^*}}$.
\end{theorem}

The proof of the theorem has two parts.
First, we bound the error in the compound parameters estimates $\hat M_2,\hat M_3$
using results from \citet{Tomioka2011}.
Then, we use results from \cite{AnandkumarGeHsu2012} to convert this error
into a bound on the actual parameter estimates $\hat\theta = (\hat\pi, \hat B)$
derived from the robust tensor power method.
But first, let us study a more basic property: identifiability.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Identifiability from moments}

In ordinary linear regression, the regression coefficients $\beta \in
\Re^d$ are identifiable if and only if the data has full rank:
$\E[x\tp{2}] \succ 0$, and furthermore, identifying $\beta$ requires
only moments $\E[xy]$ and $\E[x\tp{2}]$ (by observing the optimality
conditions for \refeqn{y1}).  However, in mixture of linear regressions,
these two moments only allow us to recover $M_1$.  \refthm{convergence}
shows that if we have the higher order analogues, $\E[x\tp{p}y\tp{p}]$
and $\E[x\tp{2p}]$ for $p \in \{1,2,3\}$, we can then identify the
parameters $\theta = (\pi, B)$, provided $\E[\cvec(x\tp{p})\tp{2}] \succ
0$ for $p \in \{1,2,3\}$.

We note that this requires more stringent positive definite constraints
when using features. For example, suppose $x = (1, t, t^2)$, the common
polynomial basis expansion, so that all the coordinates are
deterministically related.  Then $\E[x\tp{2}] \succ 0$ can easily be
satisfied. 
%, say with $x \sim \normal{0}{I}$.
However, one can verify that $\E[\cvec(x\tp{2})\tp{2}]$ is singular for
any data distribution, since $\cvec(x\tp{2}) = [1 \cdot 1, t\cdot t, 2(1
\cdot t^2), 2(t \cdot t^2), (t^2 \cdot t^2)]$ contains components $t
\cdot t$ and $2(1 \cdot t^2)$, which are linearly dependent.  Therefore,
Spectral Experts would not be able to identify the parameters of
a mixture of linear regressions for this distribution over $x$.

We now show that some amount of unidentifiability is intrinsic to
estimation from low-order moments, not just an artifact of our
estimation procedure.  Suppose $x = (t, \dots, t^d)$.  Even if we
observed all moments $\E[x\tp{p}y\tp{p}]$ and $\E[x\tp{2p}]$ for $p \in
[r]$, all the resulting coordinates would be monomials up to degree
$2dr$, and thus the moments live in a $2(dr)$-dimensional subspace.  On
the other hand, the parameters $\theta$ live in a subspace of at least
dimension $dk$.  Therefore, at least $r \ge k/2$ moments are required
for identifiability of any algorithm for this monomial example.

\paragraph{Low-rank regression}
%\label{sec:regression}
\vspace{-0.5em}
We now bound the error of
the compound parameter estimates,
$\|\Delta_2\|_F^2$ and $\|\Delta_3\|_F^2$,
where $\Delta_2 \eqdef \hat M_2 - M_2$
and $\Delta_3 \eqdef \hat M_3 - M_3$.

Our analysis is based on the low-rank regression framework of
\citet{Tomioka2011} for tensors, which builds off of
\citet{NegahbanWainwright2009} for matrices.
The main challenge is to control the noise $\eta_p(x)$,
which in addition to containing the usual Gaussian part, is also dependent on
$x$, involves the mixing noise and various cross terms.
We first set up some notation that unifies all three regressions (\refeqn{estimateM1}, \refeqn{estimateM2}, and \refeqn{estimateM3}).
Define the observation operator $\opX_p(M_p) : \Re^{d\tp{p}} \to \Re^{n}$
mapping compound parameters $M_p$:
\begin{align}
\opX_p(M_p; \sD)_i &\eqdef \innerp{M_p}{x\tp{p}_i}, & i \in [n].
\end{align}

Let $\kappa(\opX_p)$ be the restricted strong convexity constant,
and let $\opX^*_p(\eta_p; \sD) = \sum_{(x,y) \in \sD} \eta_p(x) x\tp{p}$
be the adjoint.

%\paragraph{Restricted strong convexity}

%Let us first lower bound the restricted strong convexity constant
%$\kappa(\opX_p)$:

%In the previous section, we described an algorithm for the mixture of
%linear regressions using regression to recover $M_2$ and $M_3$,
%described by \equationref{eqn:y2} and \equationref{eq:y3}, as
%a subroutine. In this section, we will characterize the rate of
%convergence of regression.

%Analysis for regression in the fixed and random design settings have
%been studied before \citep{HsuKakadeZhang}, however our setup differs
%substantially from the noise models assumed in the literature. In our
%scenario, the variance in the estimation comes not only from the
%Gaussian observation noise (which has been studied before), but also
%from the variance in the latent variable $h$.

%Let us now formally define the class of regression problems we wish to
%analyze, i.e. regression on the set $(x\tp{p}, y^p)$,
%\begin{align*}
%  y^p &= \innerp{x\tp{p}}{M_p} + (\innerp{x\tp{p}}{M_p - \beta_h\tp{p}} + \varepsilon).
%\end{align*}

%\todo{Describe/define the convex tensor stuff.}
%We would like to exploit
%the property that $M_p$ is low rank (as typically $K \ll D$). It has
%been shown that a convex relaxation for this problem regression with
%trace norm regularization, which can be solved using a proximal
%subgradient descent algorithm\citationneeded.
%The analogue of trace norm
%regularization for higher order tensors corresponds to the sum of the
%trace norms of the mode-k unfolding of the tensor \cite{Tomioka2011},
%$X_{(k)}$, is a $d_k \times (\prod_{k' \neq k} d_{k'})$ matrix obtained
%by concatenating the entries for all dimensions other than $k$. For
%example, the 1-mode unfolding of a 3rd order tensor has entries,
%$X_{(1)}^{i_1, (i_2, i_3)} = X_{i_1, i_2, i_3}$.

%In general, the optimization problem we'd like to solve is,
%\begin{align}
%  \hat M_p &= 
%  \arg\min_{M_p}& \frac{1}{2N} \| \vec y - \opX_p(M_p) \|_2^2 + \frac{\lambda_n}{K} \sum_{h=1}^K \| (M_p)_{(h)} \|_* \label{eq:regression}.
%\end{align}

\begin{lemma}[\citet{Tomioka2011}, Theorem 1]
\label{lem:lowRank}
Suppose there exists a restricted strong convexity constant $\kappa(\opX_p)$ such that
$$\frac{1}{2n} \| \opX_p( \Delta )\|_2^2 \ge \kappa(\opX_p) \|\Delta\|^2_F \quad \text{and} \quad
\lambda_n \ge \frac{\|\opX_p^*(\eta_p)\|_{op}}{n}.$$
Then the error of $\hat M_p$ is bounded as follows:
$\| \hat M_p - M_p^* \|_F \le \frac{\lambda_n \sqrt{k}}{\kappa(\opX_p)}$.
\end{lemma}

Going forward, we need to lower bound the restricted strong convexity
parameter $\kappa(\opX_p)$ and upper bound the adjoint operator
$\|\opX_p^*(\eta_p)\|_{op}$.
%We
%will appeal to the random design framework that models the input $x$ as
%random and show bounds that hold with high probability.

%\paragraph{Adjoint operator}

%In this section, we upper bound the operator norm of the adjoint
%$\|\opX_p(\eta_p)\|_\text{op}$.

First, let us lower bound the restricted strong convexity parameter $\kappa(\opX_p)$:

\begin{lemma}[lower bound on restricted strong convexity]
\label{lem:lowRankLower}
Let $\Sigma_p \eqdef \E[\cvec(x\tp{p})\tp{2}]$.
If $$n = \Omega \left( p ~ d^{2p} \log(d) \frac{R^{4p} \log(1/\delta)}{\sigmamin(\Sigma_p)^2} \right),$$
with probability at least $1-\delta$,
$$\kappa(\opX_p) \ge \frac{\sigmamin(\Sigma_p)}{4}.$$
\end{lemma}
\begin{proof}
Expanding the definition of the observation operator:
$\frac{1}{2n} \|\opX_p(\Delta)\|_2^2
= \frac{1}{2n} \sum_{(x,y) \in \sD} \innerp{\Delta}{x\tp{p}}^2$.
Unfolding the tensor and letting $\hat\Sigma_p \eqdef \frac{1}{n} \sum_{(x,y) \in \sD} \cvec(x\tp{p})\tp{2}$,
we have 
$\frac{1}{2n} \|\opX_p(\Delta)\|_2^2
= \frac{1}{2} \trace(\cvec(\Delta)\tp{2} \hat\Sigma_p)
\ge \frac{1}{2} \|\Delta\|_F^2 \sigmamin(\hat\Sigma_p)$.
It remains to show that $\sigmamin(\hat\Sigma_p)$ and $\sigmamin(\Sigma_p)$ are close.
To do this, we first apply Hoeffding's inequality elementwise.
Since $\|x\|_2 \le R$, we have that for each element $(i,j)$,
$|(\hat\Sigma_p)_{ij} - (\Sigma_p)_{ij}| = O(R^{2p}\sqrt{\frac{\log (1/\delta)}{n}})$.
Applying the union bound over the $d^{2p}$ elements of $\hat\Sigma_p - \Sigma_p$,
we have that the max norm is bounded:
$\|\hat\Sigma_p - \Sigma_p\|_\text{\rm max} = O(R^{2p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}})$.
The max norm times $d^p$ upper bounds the Frobenius norm, which upper bounds the operator norm, so we have that
$\|\hat\Sigma_p - \Sigma_p\|_\text{\rm op} = O(d^p R^{2p} \sqrt{\frac{p \log(d) \log (1/\delta)}{n}})$.
Using the fact that $\sigmamin(\hat\Sigma_p) \ge \sigmamin(\Sigma_p) - \|\hat\Sigma_p - \Sigma_p\|_\text{\rm op}$
yields the result.
\end{proof}

Next, let us upper bound the operator norm of the observation adjoint:

\begin{lemma}[upper bound on adjoint noise]
\label{lem:lowRankUpper}
Let $\opX_p$ be the linear operator previously defined. Then with
probability at least $1-\delta$,
$$\frac1{n} \|\opX_p^*(\eta_p)\|_\op \le O\left(\sigma^3 L^3 R^6 \sqrt{\frac{\log^3(1/\delta)}{n}}\right).$$
for each $p \in \{1,2,3\}$.
\end{lemma}
\begin{proof}
Let $\hat\E_p[f(x,\epsilon,h)]$ denote the empirical expectation over
the examples in dataset $\sD_p$ (recall the $\sD_p$'s are independent to
simplify the analysis).  By definition,
$\frac1n \|\opX_p^*(\eta_p)\|_\op = \hat\E_p[\eta_p(x) x\tp{p}]$
for $p \in \{1,2,3\}$ defined in \refeqn{y1}, \refeqn{y2}, and
\refeqn{y3}. Each $\eta_p(x)$ is composed of several zero-mean random
variables, and since $\|A + B\|_\op \le \|A\|_\op + \|B\|_\op$, it
suffices to consider each term in turn. For each term, we will
condition on $x$ and use a standard concentration inequality to bound
the noise.

%\begin{align}
%  \| \hat\E_1[ \eta_1(x) x ] \|_{op} &\le 
%  \underbrace{ \|\hat\E_1[\innerp{M_1 - \beta_h}{x} x]\|_\op }_{ O(R^2 L \sqrt{\frac{\log(1/\delta_1)}{n}}) } +
%  \underbrace{ \|\hat\E_1[\epsilon x]\|_\op }_{ O(R^2 L \sqrt{\frac{\log(1/\delta_1)}{n}}) } +
%\end{align}

First, consider $\eta_1(x)$, which contains two terms.
The first term is $\innerp{M_1 - \beta_h}{x}$.
We have that $\|M_1 - \beta_h\|_2 \le 2L$,
so by Hoeffding's inequality, and using the fact that $\|x\|_2 \le R$,
with probability $1-\delta_1$,
$\|\hat\E_1[\innerp{M_1 - \beta_h}{x} x]\|_\op = O(R^2 L \sqrt{\frac{\log(1/\delta_1)}{n}})$.
The second term is $\epsilon \sim \normal{0}{\sigma^2}$,
with probability $1-\delta_1$,
$\|\hat\E_1[\epsilon x]\|_\op = O(R \sigma \sqrt{\frac{\log(1/\delta_1)}{n}})$.

Now, consider $\eta_2(x)$, which contains three terms.
Using similar techniques, we have that
$\|\hat\E_2[\eta_2(x) x\tp{2}]\|_\op = O(R^2 (L^2 R^2 + \sigma L R + \sigma^2) \sqrt{\frac{\log(1/\delta_1)}{n}})$.

Finally, $\eta_3(x)$ contains five terms plus an additional bias since the estimator uses $\hat M_1$ rather than $M_1$.
There are two additional considerations.
First, to bound $\hat\E_3[\epsilon^3]$, we use Lemma 19 of \cite{hsu13spherical}.
Second, the additional bias is small by the previous bound (and it is constructed on $\sD_1$, which is independent of $\sD_3$
used to construct $\hat M_3$.
Therefore, we incur another $O(\sigma^2 \cdot R \sigma \sqrt{\frac{\log(1/\delta_1)}{n}})$.
We can handle the other terms using conventional means.
Combining everything, we obtain
$\|\hat\E_3[\eta_3(x) x\tp{3}]\|_\op = O(R^3 (L^3 R^3 + \sigma L^2 R^2 + \sigma^2 L R + \sigma^3 + \sigma^3 R) \sqrt{\frac{\log^3(1/\delta_1)}{n}})$.

Finally, taking $\delta_1 = \delta/3$, and taking the union bound over the bounds for $p \in \{1,2,3\}$,
we get our result.
Note that we are bounding quantities fairly crudely for the sake of simplicity.
\end{proof}

\paragraph{Method of Moments} 

Now that we have bounds on the errors of recovering $M_2$ and $M_3$ from
the data, we will extend the analysis of the robust tensor method in
\citet{AnandkumarGeHsu2012} to bound the error on the parameters
$\theta$. 
\begin{lemma}
  \label{lem:tensorPower} Let $\|\hat M_2 - M_2\|_{op} \le \epsilon$ and
  $\|\hat M_3 - M_3\|_{op} \le \epsilon$, for some $\epsilon$ such that 
  $$\epsilon < \min\{\sigmamin(M_2)/2,O(k \sigmamin(M_2)^{1.5} \|M_3\|_{op}^{-1})\}.$$ 
  Then, for some permutation of indices, the parameter estimates found
  in part 2 of \algorithmref{algo:spectral-experts} satisfy the
  following with probability atleast $1 - \delta$,
  \begin{align*}
  \|\hat \pi - \pi \|_{\infty}
    &= O( k \pi_{max}^{2.5} \pi_{min}^{-0.5} \sigmamin(M_2)^{-1.5} \|M_3\|_{op} \epsilon ) \\
  \|\hat \beta_h - \beta_h\|_2
    &= O( k \pi_{max} \sigmamin(M_2)^{-1.5} \|M_2\|_{op}^{0.5} \|M_3\|_{op} \epsilon ).
  \end{align*}
  for all $h \in [k]$.
\end{lemma}
The proof of the above lemma has been deferred to
\appendixref{sec:proofs}. 

\paragraph{Synthesis}
Together, these lemmas allow us to control the compound parameter error
and the recovery error. We now apply them in the proof of
\refthm{convergence}:

\begin{proof}
By \reflem{lowRank} along with \reflem{lowRankLower} and
\reflem{lowRankUpper}, we can control the operator norm of the parameter
estimates which are directly upper bounded by the Frobenius norm: If $n
\ge n_1$, then
\begin{align}
  \|\hat M_p - M_p\|_{op} = O\left( \sigma^3 L^3 R^6 k^{\frac12} \sigmamin(\Sigma_p)^{-1} \sqrt{\frac{\log^3 (1/\delta)}{n}} \right).
  %\|\hat M_p - M_p\|_F = O\left( \frac{\sigma^3 L^3 R^6 \sqrt{k} \log^3 (1/\delta)}{\sqrt{n} (\sigmamin(\Sigma_p) - d^p R^p \sqrt{\frac{p \log(d) \log(1/\delta)}{n}}} \right).
\end{align}

We complete the proof by applying \reflem{tensorPower} with the above
bound on $\|\hat M_p - M_p\|_{op}$.

\end{proof}

