\documentclass[xcolor={svgnames}]{beamer}

\setbeameroption{hide notes} 

%\usetheme{NLP}
\usetheme{boxes}
\useoutertheme{infolines}

\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{calc}

\usepackage{soul}

\usepackage{amsmath,amsthm,amssymb}   

\usepackage{listings}
\usepackage[style=authoryear,babel=hyphen]{biblatex}
\addbibresource{ref.bib}
\addbibresource{pliang.bib}

%\usepackage{algorithm,algorithmic}

\usepackage{tikz}
%\usepackage[debug,debugmarks]{scabby}
\usepackage{scabby}

\usepackage[customcolors]{hf-tikz}

\usepackage{mathtools}

\input{macros}
\input{spectral-macros}
\input{diagrams}

% these will be used later in the title page
\title[Spectral Experts]{Spectral Experts for Estimating Mixtures of Linear Regressions}
\author[Chaganty, Liang]{%
    Arun Tejasvi Chaganty\\
    Percy Liang
}
\institute{Stanford University}

\begin{document}

% "Beamer, do the following at the start of every section"
\AtBeginSection[] 
{%
\begin{frame}<beamer> 
\frametitle{Outline} % make a frame titled "Outline"
\tableofcontents[currentsection]  % show TOC and highlight current section
\end{frame}
}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Latent Variable Models}

  \splitcolumn{%
    \begin{itemize} 
      \item  \tikzmark{gen} {\bf Generative Models}
        \uncover<2->{%
        \begin{itemize}
          \item Gaussian Mixture Models
          \item Hidden Markov Models
          \item Latent Dirichlet Allocation
          \item PCFGs
          \item \dots
        \end{itemize}
        }
      \item<3-> \tikzmark{disc} {\bf Discriminative Models} 
        \uncover<4->{%
        \begin{itemize}
          \item Mixture of Experts
          \item Latent CRFs
          \item Discriminative LDA
          \item \dots
        \end{itemize}
        }
      \item<5-> {\em Easy to include features and tend to be more accurate.}
      \end{itemize}
  }{%
  \begin{canvas}
    \point{mark}{(3cm,0)};
    \point{gen}{({pic cs:gen} -| mark)}
    \point{disc}{({pic cs:disc} -| mark)}
    \drawgen<1->{($(gen) + (0,0.0cm)$)};
    \drawdisc<3->{($(disc) + (0,-0.5cm)$)};
  \end{canvas}
  }

\end{frame}

\begin{frame}
  \frametitle{Parameter Estimation is Hard}

  \begin{tikzpicture}
    % x, y
    \llhood{0}{0};
    \node<2->[scale=0.3,circle,fill=black] at (mle) {};
    \node<2-> at ($(mle) + (0.6cm,0)$) {$\mathmb{\theta_{\textrm{MLE}}}$};
    \node<3->[scale=0.3,circle,fill=black] at (em1) {};
    \node<3-> at ($(em1) + (0.5cm,0)$) {$\mathmr{\theta_{\textrm{EM}}}$};
    \node<3->[scale=0.3,circle,fill=black] at (em2) {};
    \node<3-> at ($(em2) + (0.5cm,0)$) {$\mathmr{\theta_{\textrm{EM}}}$};

    %\draw<4->[latex-latex,DarkGreen,line width=1pt] ($(mle) + (-0.8cm,0.8cm)$) -- node[above]{$\mathmg{\epsilon}$} ($(mle) + (+0.8cm,0.8cm)$);
  \end{tikzpicture}

  % Simple message: MLE is consistent but intractable, EM is efficient not but consistent. Can we get something in between.

  \begin{itemize}
    \item<1-> Log-likelihood function is non-convex.
    \item<2-> MLE is consistent but intractable.
    \item<3-> Local methods (EM, gradient descent, etc.) are tractable but inconsistent.
    \item<4-> Can we build an {\bf efficient and consistent estimator}?
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Related Work}
  \begin{itemize}
    \item<1-> Method of Moments [Pearson, 1894]
    \item<2-> Observable operators
    \begin{itemize}
      \item Control Theory [Ljung, 1987]
      \item Observable operator models [Jaeger, 2000; {\small{Littman/Sutton/Singh, 2004}}]
      \item Hidden Markov models [Hsu/Kakade/Zhang, 2009]
      \item Low-treewidth graphs [Parikh et al., 2012]
      \item Weighted finite state automata [Balle \& Mohri, 2012]
    \end{itemize}
     \item<3-> Parameter Estimation
  \begin{itemize}
    \item Mixture of Gaussians [Kalai/Moitra/Valiant, 2010]
    \item \alert{Mixture models, HMMs [Anandkumar/Hsu/Kakade, 2012]}
    \item Latent Dirichlet Allocation [Anandkumar/Hsu/Kakade, 2012]
    \item Stochastic block models [Anandkumar/Ge/Hsu/Kakade, 2012]
    \item Linear Bayesian networks [Anandkumar/Hsu/Javanmard/Kakade, 2012]
  \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Aside: Tensor Operations}
  \splitcolumn{%
    \begin{itemize}
      \item \tikzmark{tensoring} Tensor Product
        \begin{align*}
          x\tp{3}       &= x \otimes x \otimes x \\
          x\tp{3}_{ijk} &= x_i x_j x_k
        \end{align*}
      \item<2-> \tikzmark{innerp}Inner product
        \begin{align*}
          \innerp{A}{B} &= \sum_{ijk} A_{ijk} B_{ijk} \\
          \uncover<3->{%
            &= \innerp{\vvec{A}}{\vvec{B}} 
          }
        \end{align*}
    \end{itemize}
  }{%
  }
  \begin{canvas}
    \point{mark}{(1.0cm,0)};
    \point{tensoring-pos}{({pic cs:tensoring} -| mark)}
    % Tensoring
    \tikzcube{tensoring}{black,fill=white}{($(tensoring-pos) + (0,-3cm) + (0,0,0)$)}{1}{1}{1};
    \node at ($(tensoring) + (1.0cm,-0.3cm)$) {$=$};
    \tikzcube{v1}{black,fill=white}{($(tensoring) + (2.5cm,-0.3cm)$)}{1}{0.3}{0.3};
    \node (lbl1) at ($(v1) + (0.40cm,-0.1cm)$) {$\times$};
    \tikzcube{v2}{black,fill=white}{($(v1) + (1cm,0.3cm)$)}{0.3}{1}{0.3};
    \node at ($(lbl1) + (1cm,0.0cm)$) {$\times$};
    \tikzcube{v3}{black,fill=white}{($(v2) + (1cm,-0.3cm)$)}{0.3}{0.3}{1};

    % Inner product
    \uncover<2->{%
    \point{ptA}{($({pic cs:innerp} -| mark) + (0,-3.0cm)$)}
    \point{ptB}{($(ptA) + (2cm,0)$)}

    \tikzcube{innerpA}{black,fill=yellow}{($(ptA) + (0,0,0)$)}{1}{1}{1};
    \tikzcube{innerpB}{black,fill=blue!70}{($(ptB) + (0,0,0)$)}{1}{1}{1};

    \innerpdiagt{(ptA)}{(ptB)};
    \node[scale=1.0] at ($(ptB) + (1.5cm,-0.3cm)$) {$= \mathmg{0.5}$};
    }

    % Inner product 2
    \uncover<3->{%
    \point{ptvA}{($(ptA) + (0,-2cm)$)}
    \point{ptvB}{($(ptB) + (0,-2cm)$)}

    \tikzrect{innerpAv}{black,fill=yellow}{($(ptvA) + (0,0.5cm)$)}{0.3}{2.0};
    \tikzrect{innerpBv}{black,fill=blue!70}{($(ptvB) + (0,0.5cm)$)}{0.3}{2.0};

    \innerpdiagt{(ptvA)}{(ptvB)};
    \node[scale=1.0] at ($(ptvB) + (1.5cm,-0.3cm)$) {$= \mathmg{0.5}$};
    }
  \end{canvas}

\end{frame}

\begin{frame}
  \frametitle{Example: Gaussian Mixture Model}
  \splitcolumn{%
    \begin{itemize}
      \item Generative process\tikzmark{gen}:
      \begin{align*}
        h &\sim \Mult([\pi_1, \pi_2, \cdots, \pi_k]) \\
        x &\sim \normal{\beta_h}{\sigma^2}.
      \end{align*}
      \item<2-> Moments:
      \begin{align*}
          \uncover<2->{%
          \E[x|h] &= \beta_h \\
          }
          \uncover<3->{%
          \E[x] &= \sum_h \pi_h \beta_h \\
          }
          \uncover<4->{%
          \tikzmark{m2}
          \E[x\tp{2}] &= \sum_h \pi_h (\beta_h \beta_h^T) + \sigma^2 \\
                      &= \sum_h \pi_h \beta_h{\tp{2}} + \sigma^2 \\
                      }
          \uncover<5->{%
          \tikzmark{m3}
          \E[x\tp{3}] &= \sum_h \pi_h \beta_h\tp{3} + \textrm{bias}.
          }
        \end{align*}
    \end{itemize}
  }{%
  }
  \begin{canvas}
    % The model
    \point{mark}{(1cm,0)};
    \point{start}{(1cm,1cm)}; %{pic cs:gen} -| mark)};
    \drawgen{($(start) + (0,1cm)$)};
    %\node[anchor=west] (diag1) at ($(start)$) {%
    %  \includegraphics[width=0.45\textwidth,height=2cm,keepaspectratio]{figures/gen.png}
    %};
    \node[anchor=west] (diag) at ($(start) + (1cm,0.0cm)$) {%
      \includegraphics[width=0.45\textwidth,height=3cm,keepaspectratio]{figures/mog.pdf}
    };

    \uncover<4->{%
      \point{m2-pos}{({pic cs:m2} -| mark)};
      \point{m2}{($(m2-pos) - (0,4.5cm)$)};
      \tikzrect{M2}{black,fill=white}{($(m2) + (1cm,1cm)$)}{1}{1};
      \node[anchor=west] at ($(m2) + (1.1cm, 0.5cm)$) {$\E[x\tp{2}]$};
      \node[anchor=east] at ($(m2) + (-0.1cm, 0.5cm)$) {$d$};
      \node[anchor=south] at ($(m2) + (0.5cm, 1.0cm)$) {$d$};
    }
    \uncover<5->{%
      \point{m3-pos}{({pic cs:m3} -| mark)};
      \point{m3}{($(m3-pos) - (-1cm,3.5cm)$)};
      \tikzcube{M3}{black,fill=white}{($(m3) + (0,0,0)$)}{1}{1}{1};
      \node[anchor=west] at ($(m3) + (0.5cm, -0.5cm)$) {$\E[x\tp{3}]$};
      \node[anchor=east] at ($(m3) + (-1.1cm, -0.5cm)$) {$d$};
    }
  \end{canvas}

\end{frame}

\begin{frame}[t]
  \frametitle{Solution: Tensor Factorization}
  \cornertext<3->{\cite{AnandkumarGeHsu2012}}

  \splitcolumn{%
    \begin{itemize}
      \item \tikzmark{gen}$\E[x\tp{3}] = \sum_{h=1}^k \pi_h \beta_h\tp{3}$.
      \item<3-> If $\beta_h$ are orthogonal, they are eigenvectors!
        \begin{align*}
          \E[x\tp{3}](\beta_h,\beta_h) 
            %&= \sum_{h'=1}^k \pi_{h'} (\beta_{h}^T \beta_{h'})^2 \beta_{h'} \\
            %&= \sum_{h'=1}^k \pi_{h'} \delta_{hh'} \beta_{h'} \\
            &= \pi_{h} \beta_{h}.
        \end{align*}
      \item<4-> In general, whiten $\E[x\tp{3}]$ first.
    \end{itemize}
  }{%
  }
  \begin{canvas}
    \point{mark}{(1cm,0)};
    \point{start}{(1cm,1cm)}; %{pic cs:gen} -| mark)};

    \drawgen{($(start) + (0,1cm)$)};
    \node[anchor=west] (diag) at ($(start) + (1cm,0.0cm)$) {%
      \includegraphics[width=0.45\textwidth,height=3cm,keepaspectratio]{figures/mog.pdf}
    };
    \uncover<2->{%
      \tensorfactorization{(-3cm,-2cm)};
    }
  \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{}

  \begin{canvas}
    % Tasks.
    \drawgen{(-3cm,1cm)}
    \node[below=0.1cm of x.south] {Generative Models};

    % Highlight
    \draw<2>[scale=0.8,fill=green,opacity=0.4,dashed] (1cm,2.5cm) rectangle (6.5cm,-2.5cm);
      \drawdisc{(3cm,1cm)}
      \node[below=0.1cm of y.south] {Discriminative Models};

  \end{canvas}

\end{frame}


\begin{frame}
  \frametitle{Mixture of Linear Regressions}

  \splitcolumn{%
    \tikzmark{model}
    \begin{tikzpicture}
      \drawdisc{(0,0)}
    \end{tikzpicture}
    \begin{itemize}
      \item<2-> Given x
      \begin{itemize}
        \item<2-> $h \sim \Mult([\pi_1, \pi_2, \cdots, \pi_k])$.
        \item<3-> $y = \beta_h^T x + \epsilon$.
      \end{itemize}
    \end{itemize}
  }{%
  }
    \begin{canvas}
      % x, y
      \point{mark}{(0cm,0cm)}
      \point{model}{({pic cs:model} -| mark)}
      \node[anchor=west] at ($(model) + (0,-1cm)$) {%
      \includegraphics<1>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-0.pdf}
      \includegraphics<2>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-1.pdf}
      \includegraphics<3>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-2.pdf}
      \includegraphics<4>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-3.pdf}
      \includegraphics<5>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-4.pdf}
      \includegraphics<6>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-5.pdf}
      \includegraphics<7>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-6.pdf}
        };
    \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{Mixture of Linear Regressions}

  \begin{canvas}
    \node[anchor=east] (data) at (-2cm,0) {%
    \includegraphics[width=4cm,height=6cm,keepaspectratio]{figures/mlr-6.pdf}
    };

    % x, y
    \node[anchor=west,scale=1.0] (params) at (1.5cm,0) {%
      $\begin{bmatrix} \pi_1 \\ \pi_2 \\ \vdots \\ \pi_k \end{bmatrix}  
        \ub{
       \begin{bmatrix} 
                 &         &       &         \\
                 &         &       &         \\
         \beta_1 & \beta_2 & \dots & \beta_k \\
                 &         &       &         \\
                 &         &       &         
               \end{bmatrix}}_{B} $
      };
    %\node[anchor=west,right=0.1cm of params] {%
    %\includegraphics[width=4cm,height=2cm,keepaspectratio]{figures/disc.png}
    %};

    \draw[-latex] (data) -- node[above]{?} (params);


  \end{canvas}
\end{frame}

% \begin{frame}
%   \frametitle{Method of Moments for Generative LVMs.}
% 
%   \begin{tikzpicture}
%     % x, y
%     \node<1->[style=box]  (moments) at (0,0) {\objw{12cm}{%
%       \begin{align*}
%         \underbrace{\E[x]}_{M_1} &= \sum_{h=1}^k \pi_h \beta_h & 
%         \underbrace{\E[x\tp{2}]}_{M_2} &= \sum_{h=1}^k \pi_h \beta_h\tp{2} & 
%         \underbrace{\E[x\tp{3}]}_{M_3} &= \sum_{h=1}^k \pi_h \beta_h\tp{3}. 
%       \end{align*}
%     }};
%     \node<1>[style=box,below=0.1cm of moments] {\objw{12cm}{%
%     \includegraphics[width=\textwidth,height=6cm,keepaspectratio]{figures/moments.png}
%       }};
%     \node<2->[style=box,below=0.1cm of moments] {\objw{12cm}{%
%       \begin{itemize}
%         \item<2-> {\bf Tensor Power Method} for orthonormal $\beta_h$,
%           \begin{align*}
%             M_3(\beta_h,\beta_h) 
%               &= \sum_{h'=1}^k \pi_{h'} (\beta_{h}^T \beta_{h'})^2 \beta_{h'} \\
%               &= \sum_{h'=1}^k \pi_{h'} \delta_{hh'} \beta_{h'} \\
%               &= \pi_{h'} \beta_{h}.
%           \end{align*}
%         \item<3-> Use $M_2$ to whiten $M_3$.
%       \end{itemize}
%       }};
%   \end{tikzpicture}
% 
% \end{frame}

\begin{frame}[c]
  \frametitle{Finding Tensor Structure}
  \withrmargin{%
  \begin{align*}
    y &= \innerpp{\robustaltm<1>{\beta_h}{\underbrace{\beta_h}_{\textmg{random}}}}
                {x} + \epsilon \\
    \uncover<3-5>{%
    &= \robustaltm<3>{\innerp{\E[\beta_h]}{x}}
      {\mathmb{\ub{\innerp{\E[\beta_h]}{x}}_{\textrm{linear measurement}}}} 
    + \robustaltm<3-4>{\innerp{(\beta_h - \E[\beta_h])}{x} + \epsilon}
    {\mathmr{\ub{\innerp{(\beta_h - \E[\beta_h])}{x} + \epsilon}_{\textrm{noise}}}} \\
    }
  \end{align*}
  }{%
    \uncover<3->{%
        $\mboxg{mom}{\E[\beta_h] = \sum_h \pi_h \beta_h}$.
    }
  }
\end{frame}

\begin{frame}
  \frametitle{Finding Tensor Structure}

  \splitcolumn{%
  \begin{align*}
    \action<1->{%
    y\tikzmark{regA} &= \mathmb{\ob{\innerp{\E[\beta_h]}{x}}^{\textrm{linear measurement}}} &&+ \mathmr{\ob{(\beta_h - \E[\beta_h])^T x + \epsilon}^{\textrm{noise}}} \\[4ex]
    }
    \action<2->{%
    y^2 \tikzmark{regB}
      &= \left(\innerp{\beta_h}{x} + \epsilon\right)^2 && \\
    }
    \action<3->{%
    &= 
    \color{blue}
      \robustaltm<3>{%
            \innerpp{\E[\beta_h\tp{2}]}{x\tp{2}}
        }{%
          \innerpp{\ub{\E[\beta_h\tp{2}]}_{M_2}}{x\tp{2}}
        }
        &&+ \color{DarkGreen} \textrm{bias} + \color{red} \textrm{noise} \\[4ex]
    }
    \action<5->{%
    y^3 \tikzmark{regC}
    &= \color{blue} \innerpp{\ub{\E[\beta_h\tp{3}]}_{M_3}}{x\tp{3}} &&+ \color{DarkGreen} \textrm{bias} + \color{red} \textrm{noise} 
    }
  \end{align*}
  }{%
  }
  \begin{canvas}
    \point{mark}{(3cm,0)};
    \uncover<1-> {%
      \regressionA{($({pic cs:regA} -| mark) + (0,-2.5cm)$)};
    }
    \uncover<3-> {%
      \regressionB{($({pic cs:regB} -| mark) + (0,-3cm)$)};
    }
    \uncover<5-> {%
      \regressionC{($({pic cs:regC} -| mark) + (0,-3cm)$)};
    }
  \end{canvas}

  %\begin{tikzpicture}
  %  % x, y
  %  \node<5->[style=box] (tensor) {\objw{12cm}{%
  %    \includegraphics[width=\textwidth,height=3cm,keepaspectratio]{figures/tensor.png}
  %    }};
  %\end{tikzpicture}
\end{frame}

\begin{frame}
  \frametitle{Recovering Parameters}
  \centering
  
  \begin{itemize}
  {\Large
    \item 
    $M_3 \eqdef \E[\beta_h\tp{3}] = \sum_{h=1}^k \pi_h \beta_h\tp{3}$
  }
    \item<3-> Apply tensor factorization!
  \end{itemize}

  \begin{canvas}
    \uncover<2->{%
      \tensorfactorization{(-3cm,-2cm)};
    }
  \end{canvas}

\end{frame}

\begin{frame}
  \frametitle{Overview: Spectral Experts}

  \begin{canvas}
    %\input{spectral-experts.tikz}
    \spectralexpertsdiag
    \node[style=txt] at (reg-label) {regression};
    \node[style=txt] at (tf-label) {tensor factorization};

    \point{reg-assumptions}{($(reg-label) - (0,1.0cm)$)};
    \node<2->[style=txt] at (data3 |- reg-assumptions) {
      {\bf Assumptions:}
    };
    \node<3->[style=txt] at (reg-assumptions) {
        $\begin{aligned}
          \hat\E[\vvec(x\tp{2})\tp{2}] &\succ 0\\
          \hat\E[\vvec(x\tp{3})\tp{2}] &\succ 0.
        \end{aligned}$
        };
    \node<4->[style=txt] at (reg-assumptions -| tf-label) {
        $\begin{aligned}
          \pi &\succ 0\\
          rank(B) &= k \le d
        \end{aligned}$
        };
  \end{canvas}
\end{frame}


\begin{frame}
  \frametitle{Exploiting Low-rank Structure.}

  \cornertext{%
  \uncover<2->{\cite{fazel2002matrix}}\\
  \uncover<3->{\cite{tomioka2010estimation}} 
  }

    \begin{align*}
      \robustaltm<-2>{\hat M_2}{\mathmb{\hat M_3}} &= \arg\min_{M} \sum_{(x,y)\in\mathcal{D}} \left( 
      \robustaltm<-2>{y^2 - \innerp{M}{x\tp{2}}}{\mathmb{y^3 - \innerp{M}{x\tp{3}}}}
      - \textrm{bias}\right)^2 
      \uncover<2->{+
      \robustaltm<2>{\mathmb{\ub{\|M\|_{*}}_{\sum_i \sigma_i(M)}}}
        {\|M\|_{*}}
      } \\
    \end{align*}

  \begin{canvas}
    % x, y
    \uncover<1-2>{
      \matrixfactorization{(-3cm,-2cm)}
    }
    \only<3>{
      \tensorfactorization{(-3cm,-2cm)}
    }
  \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{Sample Complexity}
  \cornertext{%
  \uncover<2->{\cite{NegahbanWainwright2009,Tomioka2011}} \\
  \uncover<3->{\cite{AnandkumarGeHsu2012}} 
  }

  \begin{canvas}
    \spectralexpertsdiag
    \node[style=txt] at (reg-label) {\robustalt<1>{\color{blue}\textbf{low-rank}}{low-rank}regression};
    \node[style=txt] at (tf-label) {tensor factorization};

    % Error bounds
    \node<2->[style=txt,below=0.1em of reg-label]{%
      $O\left( k\ 
      \hlmath{blue}{\|x\|^{12}}\
      \hlmath{DarkGreen}{\|\beta\|^{6}}\
      \hlmath{red}{\|\E[\epsilon^2]\|^{6}}
      \right)$};
    \node<3->[style=txt,below=0.1em of tf-label] {%
    $O\left( \frac{k \pi_{\max}^2}{\sigma_k(M_2)^5} \right)$};
  \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{Experimental Insights}

  \begin{canvas}
    % x, y
    \node[anchor=west] (em) at (-6.5cm,-0.0cm) {%
    \includegraphics<1>[width=\textwidth,height=5cm,keepaspectratio]{figures/1833.pdf}
      \includegraphics<2-3>[width=\textwidth,height=5cm,keepaspectratio]{figures/EM-1833.pdf}
      \includegraphics<4>[width=\textwidth,height=5cm,keepaspectratio]{figures/Spectral-1833.pdf}
      \includegraphics<5->[width=\textwidth,height=5cm,keepaspectratio]{figures/Spectral+EM-1833.pdf}
      };

    \node<1-2>[anchor=west, right=1 cm of em.east] (eq) {%
        $y = \beta^T 
            \ub{\begin{bmatrix} 
              1 \\
              t \\
              t^4 \\
              t^7
            \end{bmatrix}}_{x} + \epsilon$
      };
    \node<1-2>[anchor=west, below=0.1 cm of eq] {%
        $k = 3, d = 4, n = 10^5$
      };
    % x, y
    \node[anchor=west, right=6.2cm of em] at (em.west) {%
      \includegraphics<3>[width=\textwidth,height=5cm,keepaspectratio]{figures/EM-hist.pdf}
      \includegraphics<4>[width=\textwidth,height=5cm,keepaspectratio]{figures/EM-Spectral-hist.pdf}
      \includegraphics<5>[width=\textwidth,height=5cm,keepaspectratio]{figures/EM-Spectral-Spectral+EM-hist.pdf}
    };
  \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{Experimental Insights}

  \begin{canvas}
    \node (exp1) at (-3cm,2cm) {\includegraphics[width=4.5cm]{figures/err-hist-0.pdf}};
    \node[below=-0.2cm of exp1] {$d=4, k=2$};
    \node (exp2) at (3cm,2cm) {\includegraphics[width=4.5cm]{figures/err-hist-1.pdf}};
    \node[below=-0.2cm of exp2] {$d=5, k=2$};
    \node (exp3) at (-3cm,-2cm) {\includegraphics[width=4.5cm]{figures/err-hist-2.pdf}};
    \node[below=-0.2cm of exp3] {$d=5, k=3$};
    \node (exp4) at (3cm,-2cm) {\includegraphics[width=4.5cm]{figures/err-hist-3.pdf}};
    \node[below=-0.2cm of exp4] {$d=6, k=2$};
  \end{canvas}

%\begin{small}
%  \begin{tabular}{r r r c c c}
%\hline
%%$b$ & 
%$d$ & $k$ & Spectral & EM & Spectral + EM \\
%\hline
%  %1 & 
%  4 & 2 & 2.45 $\pm$ 3.68 & 0.28 $\pm$ 0.82 & {\bf 0.17 $\pm$ 0.57} \\
%  %2 & 
%  5 & 2 & 1.38 $\pm$ 0.84 & {\bf 0.00 $\pm$ 0.00} & {\bf 0.00 $\pm$ 0.00} \\
%  %2 & 
%  5 & 3 & 2.92 $\pm$ 1.71 & 0.43 $\pm$ 1.07 & {\bf 0.31 $\pm$ 1.02} \\
%  %2 & 
%  6 & 2 & 2.33 $\pm$ 0.67 & 0.63 $\pm$ 1.29 & {\bf 0.01 $\pm$ 0.01} \\
%\hline
%\end{tabular}
%      \end{small}

\end{frame}

\begin{frame}
  \frametitle{On Initialization (Cartoon)}

  \begin{tikzpicture}
    % x, y
    \llhood{0}{0};

    \node<2->[scale=0.7] at (em2) {x};
    \node<2-> at ($(em2) + (0.6cm,0)$) {$\mathmr{\hat\theta_{\textrm{EM}}}$};
    \draw<2->[-latex,smooth,line width=1pt,red] ($(em2-start) + (+0.1cm,+0.05cm)$) -- ($(em2) + (+0.15cm,0.00cm)$);
    \draw<2->[dashed,red,line width=0.7pt] ($(em2)-(3.5cm,0)$) -- ($(em2)+(0.5cm,0)$);

%    \draw<3>[latex-latex,DarkGreen,line width=1pt,dashed] ($(mle) + (-1.2cm,0.8cm)$) -- node[above]{$\mathmg{\epsilon}$} ($(mle) + (+1.2cm,0.8cm)$);
    \node<3->[scale=0.7] at (spec) {x};
    \node<3-> at ($(spec) + (0.5cm,0.3cm)$) {$\mathmg{\hat\theta_{\textrm{spec}}}$};
    \draw<3->[dashed,DarkGreen,line width=0.7pt] ($(spec)-(0.5cm,0)$) -- ($(spec)+(3.5cm,0)$);

    \draw<4->[-latex,smooth,line width=1pt,DarkGreen] ($(spec) + (-0.1cm,+0.05cm)$) -- ($(mle) + (-0.30cm,+0.10cm)$);
    \node<4->[scale=0.7] at (mle) {x};
    \node<4->[anchor=west] at ($(mle) + (0.3cm,0)$) {$\mathmb{\hat\theta}_{\textrm{spec + EM}}$};
    \draw<4->[dashed,blue,line width=0.7pt] ($(mle)-(0.4cm,0)$) -- ($(mle)+(0.4cm,0)$);
  \end{tikzpicture}

\end{frame}


\begin{frame}
  \frametitle{Conclusions}
  \begin{itemize}
    \item<+-> Consistent estimator for the mixture of linear regressions
    \item<+-> Expose tensor factorization structure through regression.
    \item<+-> {\bf Theory:} Polynomial sample and computational complexity.
    \item<+-> {\bf Experiments:} Method of moment estimates can be a good initialization for EM.
    \item<+-> {\bf Future Work:} How can we handle other discriminative models?
      \begin{itemize}
          \item<+-> Dependencies between $h$ and $x$ (mixture of experts).
          \item<+-> Non-linear link functions (hidden variable logistic regression).
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}
    Thank you.
\end{frame}

\end{document}

