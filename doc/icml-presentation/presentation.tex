\documentclass[xcolor={svgnames}]{beamer}

\setbeameroption{hide notes} 

%\usetheme{NLP}
\usetheme{boxes}
\useoutertheme{infolines}

\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{calc}

\usepackage{soul}

\usepackage{amsmath,amsthm,amssymb}   

\usepackage{listings}
\usepackage[style=authoryear,babel=hyphen]{biblatex}
\addbibresource{ref.bib}
\addbibresource{pliang.bib}

%\usepackage{algorithm,algorithmic}

\usepackage{tikz}
\usepackage[debug]{scabby}
%\usepackage{scabby}

\usepackage{mathtools}

\input{macros}
\input{spectral-macros}
\input{diagrams}

% these will be used later in the title page
\title[Spectral Experts]{Spectral Experts for Estimating Mixtures of Linear Regressions}
\author[Chaganty, Liang]{%
    Arun Tejasvi Chaganty\\
    Percy Liang
}
\institute{Stanford University}

\begin{document}

% "Beamer, do the following at the start of every section"
\AtBeginSection[] 
{%
\begin{frame}<beamer> 
\frametitle{Outline} % make a frame titled "Outline"
\tableofcontents[currentsection]  % show TOC and highlight current section
\end{frame}
}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Latent Variable Models}

  \splitcolumn{%
    \begin{itemize} 
      \item {\bf Generative Models} \tikzmark{gen}
        \uncover<2->{%
        \begin{itemize}
          \item Gaussian Mixture Models
          \item Hidden Markov Models
          \item Latent Dirichlet Allocation
          \item PCFGs
          \item \dots
        \end{itemize}
        }
      \item<3-> {\bf Discriminative Models} \tikzmark{disc}
        \uncover<4->{%
        \begin{itemize}
          \item Mixture of Experts
          \item Latent CRFs
          \item Discriminative LDA
          \item \dots
        \end{itemize}
        }
      \item<4-> {\em Easy to include features and tend to be more accurate.}
      \end{itemize}
  }{%
  \begin{canvas}
    \point{mark}{(3cm,0)};
    \point{gen}{({pic cs:gen} -| mark)}
    \point{disc}{({pic cs:disc} -| mark)}
    \node<1->[anchor=north east] at ($(gen) +(0,1.0cm)$) {\includegraphics[width=\textwidth,height=2cm,keepaspectratio]{figures/gen.png}};
    \node<3->[anchor=north east] at ($(disc) +(0,-0.5cm)$) {\includegraphics[width=\textwidth,height=2cm,keepaspectratio]{figures/disc.png}};
  \end{canvas}
  }

\end{frame}

\begin{frame}
  \frametitle{Parameter Estimation is Hard}

  \begin{tikzpicture}
    % x, y
    \node at (0,0) {%
    \includegraphics<1>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood.png}
    \includegraphics<2>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood-mle.png}
    \includegraphics<3>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood-em.png}
    \includegraphics<4>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood-mom.png}
      };
    %\point{opt}{(0.4cm,-1.1cm)};
    %\node[style=lbl,below=0.0cm of opt,scale=1.2] {$\hlmath{blue}{\theta^*}$};
    % Axes
    \draw[-latex] (-6cm,-2.25cm) -- node[right,at end]{$-\log(p_{\theta}(x))$}  (-6cm,2cm);
    \draw[-latex] (-6cm,-2.25cm) -- node[above,at end]{$\theta$}  (5cm,-2.25cm);
  \end{tikzpicture}

  % Simple message: MLE is consistent but intractable, EM is efficient not but consistent. Can we get something in between.

  \begin{itemize}
    \item<1-> Log-likelihood function is non-convex.
    \item<2-> MLE is consistent but intractable.
    \item<3-> Local methods (EM, gradient descent, etc.) are tractable but inconsistent.
    \item<4-> Can we build {\bf efficient yet consistent estimators} that get better with more data?
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Related Work}
  \begin{itemize}
    \item<1-> Method of Moments [Pearson, 1894]
    \item<2-> Observable operator models
    \begin{itemize}
      \item Control Theory [Ljung, 1987]
      \item Observable operator models [Jaeger, 2000; Singh et al., 2004]
      \item \alert<2>{Hidden Markov models [Hsu et al, 2009]}
      \item Low-treewidth graphs [Parikh et al., 2012]
      \item Weighted finite state automata [Balle \& Mohri, 2012]
    \end{itemize}
     \item<3-> Parameter Estimation
  \begin{itemize}
    \item Mixture of Gaussians [Kalai/Moitra/Valiant, 2010]
    \item \alert{Mixture models, HMMs [Anandkumar/Hsu/Kakade, 2012]}
    \item Latent Dirichlet Allocation [Anandkumar/Hsu/Kakade, 2012]
    \item Stochastic block models [Anandkumar/Ge/Hsu/Kakade, 2012]
    \item Linear Bayesian networks [Anandkumar/Hsu/Javanmard/Kakade, 2012]
  \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Aside: Tensor Operations}
  \splitcolumn{%
    \begin{itemize}
      \item \tikzmark{tensoring} Tensoring
        $$x\tp{3}_{ijk} = x_i x_j x_k$$
      \item<2-> \tikzmark{innerp}Inner product
        \begin{align*}
          \innerp{A}{B} &= \sum_{ijk} A_{ijk} B_{ijk} \\
          \pause
            &= \innerp{\vvec{A}}{\vvec{B}} 
        \end{align*}
    \end{itemize}
  }{%
  }
  \begin{canvas}
    \point{mark}{(6.8cm,0)};
    % Tensoring
    \tikzcube{tensoring}{black,fill=white}{($({pic cs:tensoring} -| mark) + (0,0,0)$)}{1}{1}{1};
    \node at ($(tensoring) + (1.0cm,-0.3cm)$) {$=$};
    \tikzcube{v1}{black,fill=white}{($(tensoring) + (2.5cm,-0.3cm)$)}{1}{0.3}{0.3};
    \node (lbl1) at ($(v1) + (0.40cm,-0.1cm)$) {$\times$};
    \tikzcube{v2}{black,fill=white}{($(v1) + (1cm,0.3cm)$)}{0.3}{1}{0.3};
    \node at ($(lbl1) + (1cm,0.0cm)$) {$\times$};
    \tikzcube{v3}{black,fill=white}{($(v2) + (1cm,-0.3cm)$)}{0.3}{0.3}{1};

    % Inner product
    \uncover<2->{%
    \point{ptA}{($({pic cs:innerp} -| mark) + (0,0.2cm)$)}
    \point{ptB}{($(ptA) + (2cm,0)$)}
    \tikzcube{innerpA}{black,fill=yellow}{($(ptA) + (0,0,0)$)}{1}{1}{1};
    \tikzcube{innerpB}{black,fill=blue,opacity=0.7}{($(ptB) + (0,0,0)$)}{1}{1}{1};
    \node[scale=2.0] (op1a) at ($(ptA) + (-1.3cm,-0.3cm)$) {$\langle$};
    \node (op1b) at ($(ptA) + (0.7cm,-0.3cm)$) {$,$};
    \node[scale=2.0] (op1c) at ($(ptB) + (0.8cm,-0.3cm)$) {$\rangle$};
    \node[scale=2.0] (op2) at ($(op1c) + (1.5cm,0.0cm)$) {$= \hlmath{green}{0.5}$};
    }

    % Inner product 2
    \uncover<3->{%
    \tikzrect{innerpAv}{black,fill=yellow}{($(ptA) + (0,-1.5cm)$)}{0.3}{2.0};
    \tikzrect{innerpBv}{black,fill=blue,opacity=0.7}{($(ptB) + (0,-1.5cm)$)}{0.3}{2.0};
    \node[scale=2.0] (op1a) at ($(innerpAv) + (-1.3cm,-1.1cm)$) {$\langle$};
    \node (op1b) at ($(innerpAv) + (0.7cm,-1.1cm)$) {$,$};
    \node[scale=2.0] (op1c) at ($(innerpBv) + (0.8cm,-1.1cm)$) {$\rangle$};
    \node[scale=1.0] (op2) at ($(op1c) + (1.5cm,0.0cm)$) {$= \hlmath{green}{0.5}$};
    }
  \end{canvas}

\end{frame}

\begin{frame}
  \frametitle{Example: Gaussian Mixture Model}

  \splitcolumn{%
    \begin{itemize}
      \item \tikzmark{gen} Generative process:
      \begin{align*}
        h &\sim \Mult(\pi) \\
        x &\sim \normal{\beta_h}{\sigma^2}.
      \end{align*}
      \item Moments:
      \begin{align*}
          \uncover<2->{%
          \E[x|h] &= \beta_h \\
          }
          \uncover<3->{%
          \E[x] &= \sum_h \pi_h \beta_h \\
          }
          \uncover<4->{%
          \tikzmark{m2}
          \E[x\tp{2}] &= \sum_h \pi_h (\beta_h \beta_h^T) \\
                      &= \sum_h \pi_h \beta_h{\tp{2}} \\
                      }
          \uncover<5->{%
          \tikzmark{m3}
          \E[x\tp{3}] &= \sum_h \pi_h \beta_h\tp{3}.
          }
        \end{align*}
    \end{itemize}
  }{%
  }
  \begin{canvas}
    % The model
    \point{mark}{(6.5cm,0)};
    \point{start}{({pic cs:gen} -| mark)};
    \node[anchor=west] (diag1) at ($(start)$) {%
      \includegraphics[width=0.45\textwidth,height=2cm,keepaspectratio]{figures/gen.png}
    };
    \node[anchor=west] (diag) at ($(start) + (1cm,0.0cm)$) {%
      \includegraphics[width=0.45\textwidth,height=3cm,keepaspectratio]{figures/mog.png}
    };

    \uncover<4->{%
      \point{m2}{({pic cs:m2} -| mark)};
      \tikzrect{M2}{black,fill=white}{($(m2) + (1cm,1cm)$)}{1}{1};
      \node[anchor=west] at ($(m2) + (1.1cm, 0.5cm)$) {$\E[x\tp{2}]$};
      \node[anchor=east] at ($(m2) + (-0.1cm, 0.5cm)$) {$d$};
      \node[anchor=south] at ($(m2) + (0.5cm, 1.0cm)$) {$d$};
    }
    \uncover<5->{%
      \point{m3}{($({pic cs:m3} -| mark) + (1cm,1cm)$)};
      \tikzcube{M3}{black,fill=white}{($(m3) + (0,0,0)$)}{1}{1}{1};
      \node[anchor=west] at ($(m3) + (0.5cm, -0.5cm)$) {$\E[x\tp{3}]$};
      \node[anchor=east] at ($(m3) + (-1.1cm, -0.5cm)$) {$d$};
    }
  \end{canvas}

\end{frame}

\begin{frame}
  \frametitle{Solution: Tensor Factorization}
  \splitcolumn{%
    \begin{itemize}
      \item \tikzmark{gen}$\E[x\tp{3}] = \sum_{h=1}^k \pi_h \beta_h\tp{3}$.
      \item<2-> If $\beta_h$ are orthogonal, the are eigenvectors! \footcite{AnandkumarGeHsu2012}
        \begin{align*}
          \E[x\tp{3}](\beta_h,\beta_h) 
            %&= \sum_{h'=1}^k \pi_{h'} (\beta_{h}^T \beta_{h'})^2 \beta_{h'} \\
            %&= \sum_{h'=1}^k \pi_{h'} \delta_{hh'} \beta_{h'} \\
            &= \pi_{h} \beta_{h}.
        \end{align*}
      \item<3-> In general, whiten $\E[x\tp{3}]$ first.
    \end{itemize}
  }{%
  }
  \begin{canvas}
    \point{mark}{(6.5cm,0)};
    \point{start}{({pic cs:gen} -| mark)};

    \node[anchor=west] (diag1) at ($(start)$) {%
      \includegraphics[width=0.45\textwidth,height=2cm,keepaspectratio]{figures/gen.png}
    };
    \node[anchor=west] (diag) at ($(start) + (1cm,0.0cm)$) {%
      \includegraphics[width=0.45\textwidth,height=3cm,keepaspectratio]{figures/mog.png}
    };
    %\drawgen{(start)};
    %\node[anchor=west] (diag) at ($(start) + (1cm,-0.5cm)$) {%
    %  \includegraphics[width=0.45\textwidth,height=3cm,keepaspectratio]{figures/mog.png}
    %};
    % x, y
    %\node[anchor=north west] at ($(start) - (1cm,2cm)$) {%
    %    \includegraphics[width=6cm,height=2cm,keepaspectratio]{figures/tensor.png}
    %};
  \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{}

  \begin{canvas}
    % Tasks.
    \node[anchor=west] (gen) at (1cm,0) {%
      \includegraphics[width=\textwidth,height=2.5cm,keepaspectratio]{figures/gen.png}
     };
     \node[below=0.1cm of gen.south] {Generative Models};

    \draw[scale=0.8,fill=green,opacity=0.4,dashed] (6.5cm,2cm) rectangle (12.5cm,-3cm);
    \node[style=box,anchor=west] (disc) at (6cm,0) {%
      \includegraphics[width=\textwidth,height=3cm,keepaspectratio]{figures/disc.png}
     };
     \node[below=0.1cm of disc.south] {Discriminative Models};
     % Highlight
  \end{canvas}

\end{frame}


\begin{frame}
  \frametitle{Mixture of Linear Regressions}

  \splitcolumn{%
    \tikzmark{model}
    \includegraphics[width=3cm,height=4cm,keepaspectratio]{figures/disc.png}
    \begin{itemize}
      \item<2-> $h \sim \Mult(\pi)$.
      \item<4-> $y = \beta_h^T x + \epsilon$.
    \end{itemize}
  }{%
  }
    \begin{canvas}
      % x, y
      \point{mark}{(5.5cm,0)}
      \point{model}{({pic cs:model} -| mark)}
      \node[anchor=west] at ($(model) + (0,0.5cm)$) {%
      \includegraphics<1>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-2.png}
      \includegraphics<2>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-3a.png}
      \includegraphics<3>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-3b.png}
      \includegraphics<4>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-4a.png}
      \includegraphics<5>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-4b.png}
      %\includegraphics<6>[width54cm,height=6cm,keepaspectratio]{figures/mlr-data-4c.png}
      \includegraphics<6>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-5.png}
        };
    \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{Mixture of Linear Regressions}

  \begin{canvas}
    \node[anchor=west] (data) at (0,0) {%
    \includegraphics[width=4cm,height=6cm,keepaspectratio]{figures/mlr-data.png}
    };

    % x, y
    \node[anchor=west,scale=2.0] (params) at (7cm,0) {%
      $\begin{bmatrix} \pi \\ B \end{bmatrix}$
      };
    \node[anchor=west,right=0.1cm of params] {%
    \includegraphics[width=4cm,height=2cm,keepaspectratio]{figures/disc.png}
    };

    \draw[-latex] (data) -- node[above]{?} (params);


  \end{canvas}
\end{frame}

% \begin{frame}
%   \frametitle{Method of Moments for Generative LVMs.}
% 
%   \begin{tikzpicture}
%     % x, y
%     \node<1->[style=box]  (moments) at (0,0) {\objw{12cm}{%
%       \begin{align*}
%         \underbrace{\E[x]}_{M_1} &= \sum_{h=1}^k \pi_h \beta_h & 
%         \underbrace{\E[x\tp{2}]}_{M_2} &= \sum_{h=1}^k \pi_h \beta_h\tp{2} & 
%         \underbrace{\E[x\tp{3}]}_{M_3} &= \sum_{h=1}^k \pi_h \beta_h\tp{3}. 
%       \end{align*}
%     }};
%     \node<1>[style=box,below=0.1cm of moments] {\objw{12cm}{%
%     \includegraphics[width=\textwidth,height=6cm,keepaspectratio]{figures/moments.png}
%       }};
%     \node<2->[style=box,below=0.1cm of moments] {\objw{12cm}{%
%       \begin{itemize}
%         \item<2-> {\bf Tensor Power Method} for orthonormal $\beta_h$,
%           \begin{align*}
%             M_3(\beta_h,\beta_h) 
%               &= \sum_{h'=1}^k \pi_{h'} (\beta_{h}^T \beta_{h'})^2 \beta_{h'} \\
%               &= \sum_{h'=1}^k \pi_{h'} \delta_{hh'} \beta_{h'} \\
%               &= \pi_{h'} \beta_{h}.
%           \end{align*}
%         \item<3-> Use $M_2$ to whiten $M_3$.
%       \end{itemize}
%       }};
%   \end{tikzpicture}
% 
% \end{frame}

\begin{frame}[c]
  \frametitle{Finding Moments}
  \withrmargin{
  \begin{align*}
    y &= \innerpp{\robustaltm<1>{\beta_h}{\underbrace{\beta_h}_{\textmg{random}}}}
                {x} + \epsilon \\
    \uncover<4-6>{%
    &= \robustaltm<4>{\innerp{\E[\beta_h]}{x}}
      {\mathmb{\ub{\innerp{\E[\beta_h]}{x}}_{\textrm{linear measurement}}}} 
    + \robustaltm<4-5>{\innerp{(\beta_h - \E[\beta_h])}{x} + \epsilon}
    {\mathmr{\ub{\innerp{(\beta_h - \E[\beta_h])}{x} + \epsilon}_{\textrm{noise}}}} \\
    }
  \end{align*}
  }{
    $\E[\beta_h] = \sum_h \pi_h \beta_h.$
  }
\end{frame}

\begin{frame}
  \frametitle{Method of Moments for the Mixture of Linear Regressions.}

  \begin{align*}
    \action<1->{%
    y &= \overbrace{\innerp{\E[\beta_h]}{x}}^{\textrm{\color{blue} linear measurement}} & & + \overbrace{(\beta_h - \E[\beta_h])^T x + \epsilon}^{\textrm{\color{red} noise}} \\
    }
    \action<2->{%
      y^2 
      &= \innerp{\beta_h}{x}^2 &+ \epsilon^2 &+ 2 \epsilon \innerp{\beta_h}{x}  \\
    }
    \action<3->{%
      &= \color{blue}\innerp{\E[\beta_h\tp{2}]}{x\tp{2}} &+ \color{DarkGreen} \textrm{bias} &+ \color{red} \textrm{noise} \\
    }
    \action<4->{%
    y^3 &= \color{blue} \innerp{\E[\beta_h\tp{3}]}{x\tp{3}} &+ \color{DarkGreen} \textrm{bias} &+ \color{red} \textrm{noise} \\
    }
  \end{align*}

  %\begin{tikzpicture}
  %  % x, y
  %  \node<5->[style=box] (tensor) {\objw{12cm}{%
  %    \includegraphics[width=\textwidth,height=3cm,keepaspectratio]{figures/tensor.png}
  %    }};
  %\end{tikzpicture}
\end{frame}

\begin{frame}
  \frametitle{Method of Moments for the Mixture of Linear Regressions.}
  \centering
  
  \begin{itemize}
  {\Large
    \item 
    $\E[\beta_h\tp{3}] = \sum_{h=1}^k \pi_h \beta_h\tp{3}$
  }
    \item<2-> Apply tensor factorization!
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Overview: Spectral Experts}

  \begin{canvas}
    \input{spectral-experts.tikz}
    \node[style=txt] at (reg-label) {\alert<1>{regression}};
    \node[style=txt] at (tf-label) {\alert<2>{tensor factorization}};
  \end{canvas}
\end{frame}


\begin{frame}
  \frametitle{Exploiting Low-rank Structure.}

    \begin{align*}
      \hat\E[\beta_h\tp{2}] &= \arg\min_{M} \left(y^2 - \innerp{M}{x\tp{2}}\right)^2_{(x,y)\in\mathcal{D}} 
        \alt<1>{+ \|M\|_{2}}
          {+ \hlmath{blue}{\|M\|_{*}}} \\
      \action<2->{%
      \|M\|_{*} &= \sum_i \sigma_i(M).
      }
    \end{align*}

  \begin{tikzpicture}
    % x, y
    \node (measurements) {%
      \includegraphics[width=\textwidth,height=6cm,keepaspectratio]{figures/tensor.png}
      };
  \end{tikzpicture}
\end{frame}

\begin{frame}
  \frametitle{Spectral Experts}

  \begin{canvas}
    \input{spectral-experts.tikz}
    \node[style=txt] at (reg-label) {{\color{blue}\bf low-rank} regression};
    \node[style=txt] at (tf-label) {tensor factorization};

    % Error bounds
    \node<2->[style=txt,below=0.1em of reg-label]{%
      $O\left( k\ 
      \hlmath{blue}{\|x\|^{12}}\
      \hlmath{DarkGreen}{\|\beta\|^{6}}\
      \hlmath{red}{\|\E[\epsilon^2]\|^{6}}
      \right)$};
    \node<3->[style=txt,below=0.1em of tf-label] {%
    $O\left( \frac{k \pi_{\max}^2}{\sigma_k(M_2)^5} \right)$};
  \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{Experimental Insights}

  \begin{canvas}
    % x, y
    \node[anchor=west] (em) at (-0.5cm,-1.5cm) {%
      \includegraphics<1>[width=\textwidth,height=5cm,keepaspectratio]{figures/1-8-3.png}
      \includegraphics<2>[width=\textwidth,height=5cm,keepaspectratio]{figures/1-8-3-em.png}
      \includegraphics<3>[width=\textwidth,height=5cm,keepaspectratio]{figures/1-8-3-spec.png}
      \includegraphics<4->[width=\textwidth,height=5cm,keepaspectratio]{figures/1-8-3-specm.png}
      };
    % x, y
    \node[anchor=west, right=6cm of em] at (em.west) {%
      \includegraphics[width=\textwidth,height=5cm,keepaspectratio]{figures/hist.png}
    };

    \node[above=0.1 cm of em] {%
        $y = \beta^T 
            \begin{bmatrix} 
              1 \\
              t \\
              t^4 \\
              t^7
            \end{bmatrix} + \epsilon$
      };
  \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{Experimental Insights}

\begin{small}
  \begin{tabular}{r r r c c c}
\hline
%$b$ & 
$d$ & $k$ & Spectral & EM & Spectral + EM \\
\hline
  %1 & 
  4 & 2 & 2.45 $\pm$ 3.68 & 0.28 $\pm$ 0.82 & {\bf 0.17 $\pm$ 0.57} \\
  %2 & 
  5 & 2 & 1.38 $\pm$ 0.84 & {\bf 0.00 $\pm$ 0.00} & {\bf 0.00 $\pm$ 0.00} \\
  %2 & 
  5 & 3 & 2.92 $\pm$ 1.71 & 0.43 $\pm$ 1.07 & {\bf 0.31 $\pm$ 1.02} \\
  %2 & 
  6 & 2 & 2.33 $\pm$ 0.67 & 0.63 $\pm$ 1.29 & {\bf 0.01 $\pm$ 0.01} \\
\hline
\end{tabular}
      \end{small}

\end{frame}

\begin{frame}
  \frametitle{Experimental Insights}

  \begin{tikzpicture}
    \node at (0,0) {%
      \includegraphics<1>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood-mom.png}
      \includegraphics<2>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood-mom-em1.png}
      \includegraphics<3>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood-mom-em2.png}
    };
    \draw[-latex] (-6cm,-2.25cm) -- node[right,at end]{$-\log(p_{\theta}(x))$}  (-6cm,2cm);
    \draw[-latex] (-6cm,-2.25cm) -- node[above,at end]{$\theta$}  (5cm,-2.25cm);

    \node<2-> at (1.8cm,0.6cm) {$\hlmath{DarkGreen}{\theta_{spec}}$};
    \node<3-> at (0.3cm,-0.8cm) {$\hlmath{blue}{\theta^{*}}$};
  \end{tikzpicture}

\end{frame}


\begin{frame}
  \frametitle{Conclusions}
  \begin{itemize}
    \item {\bf Consistent estimator} for the mixture of linear regressions with {\bf polynomial sample and computational complexity}.
    \item<2-> Derive conditional moments using regression.
    \item<3-> Method of moment estimates can be a good initialization for EM.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}
    Thank you.
\end{frame}

\end{document}

% TikZ
%\begin{canvas}
%  \node (img1) at (0,0) {\includegraphics[height=0.6\textheight,width=0.4\linewidth,keepaspectratio]{}};
%\end{canvas}

% Notes 
% \note[item]{}

% 2-column
% \begin{columns}
%   \begin{column}{0.48\textwidth}
%     \begin{itemize}
%         \item
%     \end{itemize}
%   \end{column}
%   \hfill
%   \begin{column}{0.48\textwidth}
%     \begin{itemize}
%         \item
%     \end{itemize}
%   \end{column}
% \end{columns}

% TikZ
%\begin{canvas}
%  \node (img1) at (0,0) {\includegraphics[height=0.6\textheight,width=0.4\linewidth,keepaspectratio]{}};
%\end{canvas}

% Notes 
% \note[item]{}

% 2-column
% \begin{columns}
%   \begin{column}{0.48\textwidth}
%     \begin{itemize}
%         \item
%     \end{itemize}
%   \end{column}
%   \hfill
%   \begin{column}{0.48\textwidth}
%     \begin{itemize}
%         \item
%     \end{itemize}
%   \end{column}
% \end{columns}

