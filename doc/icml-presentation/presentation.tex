\documentclass[xcolor={svgnames}]{beamer}

\setbeameroption{hide notes} 

%\usetheme{NLP}
\usetheme{boxes}
\useoutertheme{infolines}

\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{calc}

\usepackage{soul}

\usepackage{amsmath,amsthm,amssymb}   

\usepackage{listings}
\usepackage[style=authoryear,babel=hyphen]{biblatex}
\addbibresource{ref.bib}
\addbibresource{pliang.bib}

%\usepackage{algorithm,algorithmic}

\usepackage{tikz}
\usepackage[debug]{scabby}
%\usepackage{scabby}

\usepackage[customcolors]{hf-tikz}

\usepackage{mathtools}

\input{macros}
\input{spectral-macros}
\input{diagrams}

% these will be used later in the title page
\title[Spectral Experts]{Spectral Experts for Estimating Mixtures of Linear Regressions}
\author[Chaganty, Liang]{%
    Arun Tejasvi Chaganty\\
    Percy Liang
}
\institute{Stanford University}

\begin{document}

% "Beamer, do the following at the start of every section"
\AtBeginSection[] 
{%
\begin{frame}<beamer> 
\frametitle{Outline} % make a frame titled "Outline"
\tableofcontents[currentsection]  % show TOC and highlight current section
\end{frame}
}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Latent Variable Models}

  \splitcolumn{%
    \begin{itemize} 
      \item  \tikzmark{gen} {\bf Generative Models}
        \uncover<2->{%
        \begin{itemize}
          \item Gaussian Mixture Models
          \item Hidden Markov Models
          \item Latent Dirichlet Allocation
          \item PCFGs
          \item \dots
        \end{itemize}
        }
      \item<3-> \tikzmark{disc} {\bf Discriminative Models} 
        \uncover<4->{%
        \begin{itemize}
          \item Mixture of Experts
          \item Latent CRFs
          \item Discriminative LDA
          \item \dots
        \end{itemize}
        }
      \item<4-> {\em Easy to include features and tend to be more accurate.}
      \end{itemize}
  }{%
  \begin{canvas}
    \point{mark}{(3cm,0)};
    \point{gen}{({pic cs:gen} -| mark)}
    \point{disc}{({pic cs:disc} -| mark)}
    \drawgen<1->{($(gen) + (0,1.5cm)$)};
    \drawdisc<3->{($(disc) + (0,0.0cm)$)};
  \end{canvas}
  }

\end{frame}

\begin{frame}
  \frametitle{Parameter Estimation is Hard}

  \begin{tikzpicture}
    % x, y
    \node at (0,0) {%
    \includegraphics<1>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood.png}
    \includegraphics<2>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood-mle.png}
    \includegraphics<3>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood-em.png}
    \includegraphics<4>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood-mom.png}
      };
    %\point{opt}{(0.4cm,-1.1cm)};
    %\node[style=lbl,below=0.0cm of opt,scale=1.2] {$\hlmath{blue}{\theta^*}$};
    % Axes
    \draw[-latex] (-6cm,-2.25cm) -- node[right,at end]{$-\log(p_{\theta}(x))$}  (-6cm,2cm);
    \draw[-latex] (-6cm,-2.25cm) -- node[above,at end]{$\theta$}  (5cm,-2.25cm);
  \end{tikzpicture}

  % Simple message: MLE is consistent but intractable, EM is efficient not but consistent. Can we get something in between.

  \begin{itemize}
    \item<1-> Log-likelihood function is non-convex.
    \item<2-> MLE is consistent but intractable.
    \item<3-> Local methods (EM, gradient descent, etc.) are tractable but inconsistent.
    \item<4-> Can we build an {\bf efficient and consistent estimator}?
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Related Work}
  \begin{itemize}
    \item<1-> Method of Moments [Pearson, 1894]
    \item<2-> Observable operator models
    \begin{itemize}
      \item Control Theory [Ljung, 1987]
      \item Observable operator models [Jaeger, 2000; Singh et al., 2004]
      \item \alert<2>{Hidden Markov models [Hsu et al, 2009]}
      \item Low-treewidth graphs [Parikh et al., 2012]
      \item Weighted finite state automata [Balle \& Mohri, 2012]
    \end{itemize}
     \item<3-> Parameter Estimation
  \begin{itemize}
    \item Mixture of Gaussians [Kalai/Moitra/Valiant, 2010]
    \item \alert{Mixture models, HMMs [Anandkumar/Hsu/Kakade, 2012]}
    \item Latent Dirichlet Allocation [Anandkumar/Hsu/Kakade, 2012]
    \item Stochastic block models [Anandkumar/Ge/Hsu/Kakade, 2012]
    \item Linear Bayesian networks [Anandkumar/Hsu/Javanmard/Kakade, 2012]
  \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Aside: Tensor Operations}
  \splitcolumn{%
    \begin{itemize}
      \item \tikzmark{tensoring} Tensoring
        $$x\tp{3}_{ijk} = x_i x_j x_k$$
      \item<2-> \tikzmark{innerp}Inner product
        \begin{align*}
          \innerp{A}{B} &= \sum_{ijk} A_{ijk} B_{ijk} \\
          \uncover<3->{%
            &= \innerp{\vvec{A}}{\vvec{B}} 
          }
        \end{align*}
    \end{itemize}
  }{%
  }
  \begin{canvas}
    \point{mark}{(6.8cm,0)};
    % Tensoring
    \tikzcube{tensoring}{black,fill=white}{($({pic cs:tensoring} -| mark) + (0,0,0)$)}{1}{1}{1};
    \node at ($(tensoring) + (1.0cm,-0.3cm)$) {$=$};
    \tikzcube{v1}{black,fill=white}{($(tensoring) + (2.5cm,-0.3cm)$)}{1}{0.3}{0.3};
    \node (lbl1) at ($(v1) + (0.40cm,-0.1cm)$) {$\times$};
    \tikzcube{v2}{black,fill=white}{($(v1) + (1cm,0.3cm)$)}{0.3}{1}{0.3};
    \node at ($(lbl1) + (1cm,0.0cm)$) {$\times$};
    \tikzcube{v3}{black,fill=white}{($(v2) + (1cm,-0.3cm)$)}{0.3}{0.3}{1};

    % Inner product
    \uncover<2->{%
    \point{ptA}{($({pic cs:innerp} -| mark) + (0,0.2cm)$)}
    \point{ptB}{($(ptA) + (2cm,0)$)}
    \tikzcube{innerpA}{black,fill=yellow}{($(ptA) + (0,0,0)$)}{1}{1}{1};
    \tikzcube{innerpB}{black,fill=blue,opacity=0.7}{($(ptB) + (0,0,0)$)}{1}{1}{1};
    \node[scale=2.0] (op1a) at ($(ptA) + (-1.3cm,-0.3cm)$) {$\langle$};
    \node (op1b) at ($(ptA) + (0.7cm,-0.3cm)$) {$,$};
    \node[scale=2.0] (op1c) at ($(ptB) + (0.8cm,-0.3cm)$) {$\rangle$};
    \node[scale=1.0] (op2) at ($(op1c) + (1.5cm,0.0cm)$) {$= \mathmg{0.5}$};
    }

    % Inner product 2
    \uncover<3->{%
    \tikzrect{innerpAv}{black,fill=yellow}{($(ptA) + (0,-1.5cm)$)}{0.3}{2.0};
    \tikzrect{innerpBv}{black,fill=blue,opacity=0.7}{($(ptB) + (0,-1.5cm)$)}{0.3}{2.0};
    \node[scale=2.0] (op1a) at ($(innerpAv) + (-1.3cm,-1.1cm)$) {$\langle$};
    \node (op1b) at ($(innerpAv) + (0.7cm,-1.1cm)$) {$,$};
    \node[scale=2.0] (op1c) at ($(innerpBv) + (0.8cm,-1.1cm)$) {$\rangle$};
    \node[scale=1.0] (op2) at ($(op1c) + (1.5cm,0.0cm)$) {$= \mathmg{0.5}$};
    }
  \end{canvas}

\end{frame}

\begin{frame}
  \frametitle{Example: Gaussian Mixture Model}

  \splitcolumn{%
    \begin{itemize}
      \item Generative process\tikzmark{gen}:
      \begin{align*}
        h &\sim \Mult([\pi_1, \pi_2, \cdots, \pi_k]) \\
        x &\sim \normal{\beta_h}{\sigma^2}.
      \end{align*}
      \item Moments:
      \begin{align*}
          \uncover<2->{%
          \E[x|h] &= \beta_h \\
          }
          \uncover<3->{%
          \E[x] &= \sum_h \pi_h \beta_h \\
          }
          \uncover<4->{%
          \tikzmark{m2}
          \E[x\tp{2}] &= \sum_h \pi_h (\beta_h \beta_h^T) \\
                      &= \sum_h \pi_h \beta_h{\tp{2}} \\
                      }
          \uncover<5->{%
          \tikzmark{m3}
          \E[x\tp{3}] &= \sum_h \pi_h \beta_h\tp{3}.
          }
        \end{align*}
    \end{itemize}
  }{%
  }
  \begin{canvas}
    % The model
    \point{mark}{(6.5cm,0)};
    \point{start}{({pic cs:gen} -| mark)};
    \drawgen{($(start) + (0,2cm)$)};
    %\node[anchor=west] (diag1) at ($(start)$) {%
    %  \includegraphics[width=0.45\textwidth,height=2cm,keepaspectratio]{figures/gen.png}
    %};
    \node[anchor=west] (diag) at ($(start) + (1cm,1.0cm)$) {%
      \includegraphics[width=0.45\textwidth,height=3cm,keepaspectratio]{figures/mog.png}
    };

    \uncover<4->{%
      \point{m2}{({pic cs:m2} -| mark)};
      \tikzrect{M2}{black,fill=white}{($(m2) + (1cm,1cm)$)}{1}{1};
      \node[anchor=west] at ($(m2) + (1.1cm, 0.5cm)$) {$\E[x\tp{2}]$};
      \node[anchor=east] at ($(m2) + (-0.1cm, 0.5cm)$) {$d$};
      \node[anchor=south] at ($(m2) + (0.5cm, 1.0cm)$) {$d$};
    }
    \uncover<5->{%
      \point{m3}{($({pic cs:m3} -| mark) + (1cm,1cm)$)};
      \tikzcube{M3}{black,fill=white}{($(m3) + (0,0,0)$)}{1}{1}{1};
      \node[anchor=west] at ($(m3) + (0.5cm, -0.5cm)$) {$\E[x\tp{3}]$};
      \node[anchor=east] at ($(m3) + (-1.1cm, -0.5cm)$) {$d$};
    }
  \end{canvas}

\end{frame}

\begin{frame}
  \frametitle{Solution: Tensor Factorization}
  \cornertext<3->{\cite{AnandkumarGeHsu2012}}

  \splitcolumn{%
    \begin{itemize}
      \item \tikzmark{gen}$\E[x\tp{3}] = \sum_{h=1}^k \pi_h \beta_h\tp{3}$.
      \item<3-> If $\beta_h$ are orthogonal, the are eigenvectors!
        \begin{align*}
          \E[x\tp{3}](\beta_h,\beta_h) 
            %&= \sum_{h'=1}^k \pi_{h'} (\beta_{h}^T \beta_{h'})^2 \beta_{h'} \\
            %&= \sum_{h'=1}^k \pi_{h'} \delta_{hh'} \beta_{h'} \\
            &= \pi_{h} \beta_{h}.
        \end{align*}
      \item<4-> In general, whiten $\E[x\tp{3}]$ first.
    \end{itemize}
  }{%
  }
  \begin{canvas}
    \point{mark}{(6.5cm,0)};
    \point{start}{({pic cs:gen} -| mark)};

    \drawgen{($(start) + (0,2cm)$)};
    \node[anchor=west] (diag) at ($(start) + (1cm,0.0cm)$) {%
      \includegraphics[width=0.45\textwidth,height=3cm,keepaspectratio]{figures/mog.png}
    };
    \uncover<2->{%
      \tensorfactorization{(2cm,-1cm)};
    }
  \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{}

  \begin{canvas}
    % Tasks.
    \drawgen{(3cm,0)}
    \node[below=0.1cm of x.south] {Generative Models};

    % Highlight
    \draw[scale=0.8,fill=green,opacity=0.4,dashed] (8cm,1cm) rectangle (14cm,-4cm);
      \drawdisc{(9cm,0)}
      \node[below=0.1cm of y.south] {Discriminative Models};

  \end{canvas}

\end{frame}


\begin{frame}
  \frametitle{Mixture of Linear Regressions}

  \splitcolumn{%
    \tikzmark{model}
    \begin{tikzpicture}
      \drawdisc{(0,0)}
    \end{tikzpicture}
    \begin{itemize}
      \item<2-> $h \sim \Mult([\pi_1, \pi_2, \cdots, \pi_k])$.
      \item<4-> $y = \beta_h^T x + \epsilon$.
    \end{itemize}
  }{%
  }
    \begin{canvas}
      % x, y
      \point{mark}{(5.5cm,0)}
      \point{model}{({pic cs:model} -| mark)}
      \node[anchor=west] at ($(model) + (0,0.5cm)$) {%
      \includegraphics<1>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-2.png}
      \includegraphics<2>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-3a.png}
      \includegraphics<3>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-3b.png}
      \includegraphics<4>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-4a.png}
      \includegraphics<5>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-4b.png}
      %\includegraphics<6>[width54cm,height=6cm,keepaspectratio]{figures/mlr-data-4c.png}
      \includegraphics<6>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-5.png}
      \includegraphics<7>[width=5cm,height=6cm,keepaspectratio]{figures/mlr-data-6.png}
        };
    \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{Mixture of Linear Regressions}

  \begin{canvas}
    \node[anchor=west] (data) at (0,0) {%
    \includegraphics[width=4cm,height=6cm,keepaspectratio]{figures/mlr-data.png}
    };

    % x, y
    \node[anchor=west,scale=1.0] (params) at (7cm,0) {%
      $\begin{bmatrix} \pi_1 \\ \pi_2 \\ \vdots \\ \pi_k \end{bmatrix}  
       \begin{bmatrix} 
                 &         &       &         \\
                 &         &       &         \\
         \beta_1 & \beta_2 & \dots & \beta_k \\
                 &         &       &         \\
                 &         &       &         
       \end{bmatrix} $
      };
    %\node[anchor=west,right=0.1cm of params] {%
    %\includegraphics[width=4cm,height=2cm,keepaspectratio]{figures/disc.png}
    %};

    \draw[-latex] (data) -- node[above]{?} (params);


  \end{canvas}
\end{frame}

% \begin{frame}
%   \frametitle{Method of Moments for Generative LVMs.}
% 
%   \begin{tikzpicture}
%     % x, y
%     \node<1->[style=box]  (moments) at (0,0) {\objw{12cm}{%
%       \begin{align*}
%         \underbrace{\E[x]}_{M_1} &= \sum_{h=1}^k \pi_h \beta_h & 
%         \underbrace{\E[x\tp{2}]}_{M_2} &= \sum_{h=1}^k \pi_h \beta_h\tp{2} & 
%         \underbrace{\E[x\tp{3}]}_{M_3} &= \sum_{h=1}^k \pi_h \beta_h\tp{3}. 
%       \end{align*}
%     }};
%     \node<1>[style=box,below=0.1cm of moments] {\objw{12cm}{%
%     \includegraphics[width=\textwidth,height=6cm,keepaspectratio]{figures/moments.png}
%       }};
%     \node<2->[style=box,below=0.1cm of moments] {\objw{12cm}{%
%       \begin{itemize}
%         \item<2-> {\bf Tensor Power Method} for orthonormal $\beta_h$,
%           \begin{align*}
%             M_3(\beta_h,\beta_h) 
%               &= \sum_{h'=1}^k \pi_{h'} (\beta_{h}^T \beta_{h'})^2 \beta_{h'} \\
%               &= \sum_{h'=1}^k \pi_{h'} \delta_{hh'} \beta_{h'} \\
%               &= \pi_{h'} \beta_{h}.
%           \end{align*}
%         \item<3-> Use $M_2$ to whiten $M_3$.
%       \end{itemize}
%       }};
%   \end{tikzpicture}
% 
% \end{frame}

\begin{frame}[c]
  \frametitle{Finding Tensor Structure}
  \withrmargin{%
  \begin{align*}
    y &= \innerpp{\robustaltm<1>{\beta_h}{\underbrace{\beta_h}_{\textmg{random}}}}
                {x} + \epsilon \\
    \uncover<4-6>{%
    &= \robustaltm<4>{\innerp{\E[\beta_h]}{x}}
      {\mathmb{\ub{\innerp{\E[\beta_h]}{x}}_{\textrm{linear measurement}}}} 
    + \robustaltm<4-5>{\innerp{(\beta_h - \E[\beta_h])}{x} + \epsilon}
    {\mathmr{\ub{\innerp{(\beta_h - \E[\beta_h])}{x} + \epsilon}_{\textrm{noise}}}} \\
    }
  \end{align*}
  }{%
    \uncover<3->{%
        $\mboxg{mom}{\E[\beta_h] = \sum_h \pi_h \beta_h}$.
    }
  }
\end{frame}

\begin{frame}
  \frametitle{Finding Tensor Structure}

  \splitcolumn{%
  \begin{align*}
    \action<1->{%
    y &= \mathmb{\ob{\innerp{\E[\beta_h]}{x}}^{\textrm{linear measurement}}} &&+ \mathmr{\ob{(\beta_h - \E[\beta_h])^T x + \epsilon}^{\textrm{noise}}} \\[4ex]
    }
    \action<2->{%
      y^2 
      &= \left(\innerp{\beta_h}{x} + \epsilon\right)^2 && \\
    }
    \action<3->{%
    &= 
    \color{blue}
      \robustaltm<3>{%
            \innerp{\E[\beta_h\tp{2}]}{x\tp{2}}
        }{%
          \ub{\innerp{\E[\beta_h\tp{2}]}{x\tp{2}}}_{M_2} 
        }
        &&+ \color{DarkGreen} \textrm{bias} + \color{red} \textrm{noise} \\[4ex]
    }
    \action<5->{%
    y^3 &= \color{blue} \ub{\innerp{\E[\beta_h\tp{3}]}{x\tp{3}}}_{M_3} &&+ \color{DarkGreen} \textrm{bias} + \color{red} \textrm{noise} 
    }
  \end{align*}
  }{%
  }

  %\begin{tikzpicture}
  %  % x, y
  %  \node<5->[style=box] (tensor) {\objw{12cm}{%
  %    \includegraphics[width=\textwidth,height=3cm,keepaspectratio]{figures/tensor.png}
  %    }};
  %\end{tikzpicture}
\end{frame}

\begin{frame}
  \frametitle{Method of Moments for the Mixture of Linear Regressions.}
  \centering
  
  \begin{itemize}
  {\Large
    \item 
    $M_3 \eqdef \E[\beta_h\tp{3}] = \sum_{h=1}^k \pi_h \beta_h\tp{3}$
  }
    \item<3-> Apply tensor factorization!
  \end{itemize}

  \begin{canvas}
    \uncover<2->{%
      \tensorfactorization{(-3cm,-1cm)};
    }
  \end{canvas}

\end{frame}

\begin{frame}
  \frametitle{Overview: Spectral Experts}

  \begin{canvas}
    \input{spectral-experts.tikz}
    \node[style=txt] at (reg-label) {\alert<2>{regression}};
    \node[style=txt] at (tf-label) {\alert<3>{tensor factorization}};
  \end{canvas}
\end{frame}


\begin{frame}
  \frametitle{Exploiting Low-rank Structure.}

  \cornertext{%
  \uncover<2->{\cite{fazel2002matrix}}\\
  \uncover<3->{\cite{tomioka2010estimation}} 
  }

    \begin{align*}
      \hat M_{\robustaltm<-2>{2}{\mathmb{3}}} &= \arg\min_{M} \sum_{(x,y)\in\mathcal{D}} \left(y^2 - 
      \robustaltm<-2>{\innerp{M_2}{x\tp{2}}}{\mathmb{\innerp{M_3}{x\tp{3}}}}
      - \textrm{bias}\right)^2 
      \uncover<2->{+
      \robustaltm<2>{\mathmb{\ub{\|M\|_{*}}_{\sum_i \sigma_i(M)}}}
        {\|M\|_{*}}
      } \\
    \end{align*}

  \begin{canvas}
    % x, y
    \alt<-2>{%
      \matrixfactorization{(2cm,0)}
    }{%
      \tensorfactorization{(2cm,0)}
    }
  \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{Spectral Experts}
  \cornertext{%
  \uncover<2->{\cite{NegahbanWainwright2009,Tomioka2011}} \\
  \uncover<3->{\cite{AnandkumarGeHsu2012}} 
  }

  \begin{canvas}
    \input{spectral-experts.tikz}
    \node[style=txt] at (reg-label) {{\color{blue}\bf low-rank} regression};
    \node[style=txt] at (tf-label) {tensor factorization};

    % Error bounds
    \node<2->[style=txt,below=0.1em of reg-label]{%
      $O\left( k\ 
      \hlmath{blue}{\|x\|^{12}}\
      \hlmath{DarkGreen}{\|\beta\|^{6}}\
      \hlmath{red}{\|\E[\epsilon^2]\|^{6}}
      \right)$};
    \node<3->[style=txt,below=0.1em of tf-label] {%
    $O\left( \frac{k \pi_{\max}^2}{\sigma_k(M_2)^5} \right)$};
  \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{Experimental Insights}

  \begin{canvas}
    % x, y
    \node[anchor=west] (em) at (-0.5cm,-1.5cm) {%
      \includegraphics<1>[width=\textwidth,height=5cm,keepaspectratio]{figures/1-8-3.png}
      \includegraphics<2>[width=\textwidth,height=5cm,keepaspectratio]{figures/1-8-3-em.png}
      \includegraphics<3>[width=\textwidth,height=5cm,keepaspectratio]{figures/1-8-3-spec.png}
      \includegraphics<4->[width=\textwidth,height=5cm,keepaspectratio]{figures/1-8-3-specm.png}
      };
    % x, y
    \node[anchor=west, right=6cm of em] at (em.west) {%
      \includegraphics[width=\textwidth,height=5cm,keepaspectratio]{figures/hist.png}
    };

    \node[above=0.1 cm of em] {%
        $y = \beta^T 
            \begin{bmatrix} 
              1 \\
              t \\
              t^4 \\
              t^7
            \end{bmatrix} + \epsilon$
      };
  \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{Experimental Insights}

\begin{small}
  \begin{tabular}{r r r c c c}
\hline
%$b$ & 
$d$ & $k$ & Spectral & EM & Spectral + EM \\
\hline
  %1 & 
  4 & 2 & 2.45 $\pm$ 3.68 & 0.28 $\pm$ 0.82 & {\bf 0.17 $\pm$ 0.57} \\
  %2 & 
  5 & 2 & 1.38 $\pm$ 0.84 & {\bf 0.00 $\pm$ 0.00} & {\bf 0.00 $\pm$ 0.00} \\
  %2 & 
  5 & 3 & 2.92 $\pm$ 1.71 & 0.43 $\pm$ 1.07 & {\bf 0.31 $\pm$ 1.02} \\
  %2 & 
  6 & 2 & 2.33 $\pm$ 0.67 & 0.63 $\pm$ 1.29 & {\bf 0.01 $\pm$ 0.01} \\
\hline
\end{tabular}
      \end{small}

\end{frame}

\begin{frame}
  \frametitle{Experimental Insights}

  \begin{tikzpicture}
    \node at (0,0) {%
      \includegraphics<1>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood-mom.png}
      \includegraphics<2>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood-mom-em1.png}
      \includegraphics<3>[width=\textwidth,height=6cm,keepaspectratio]{figures/likelihood-mom-em2.png}
    };
    \draw[-latex] (-6cm,-2.25cm) -- node[right,at end]{$-\log(p_{\theta}(x))$}  (-6cm,2cm);
    \draw[-latex] (-6cm,-2.25cm) -- node[above,at end]{$\theta$}  (5cm,-2.25cm);

    \node<2-> at (1.8cm,0.6cm) {$\hlmath{DarkGreen}{\theta_{spec}}$};
    \node<3-> at (0.3cm,-0.8cm) {$\hlmath{blue}{\theta^{*}}$};
  \end{tikzpicture}

\end{frame}


\begin{frame}
  \frametitle{Conclusions}
  \begin{itemize}
    \item<1-> Expose tensor factorization structure through regression.
    \item<2-> {\bf Consistent estimator} for the mixture of linear regressions with {\bf polynomial sample and computational complexity}.
    \item<3-> Empirically, method of moment estimates can be a good initialization for EM.
    \item<4-> How can we handle other discriminative models?
      \begin{itemize}
          \item<5-> Dependencies between $h$ and $x$ (mixture of experts).
          \item<6-> Non-linear link functions (logistic regression).
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}
    Thank you.
\end{frame}

\end{document}

% TikZ
%\begin{canvas}
%  \node (img1) at (0,0) {\includegraphics[height=0.6\textheight,width=0.4\linewidth,keepaspectratio]{}};
%\end{canvas}

% Notes 
% \note[item]{}

% 2-column
% \begin{columns}
%   \begin{column}{0.48\textwidth}
%     \begin{itemize}
%         \item
%     \end{itemize}
%   \end{column}
%   \hfill
%   \begin{column}{0.48\textwidth}
%     \begin{itemize}
%         \item
%     \end{itemize}
%   \end{column}
% \end{columns}

% TikZ
%\begin{canvas}
%  \node (img1) at (0,0) {\includegraphics[height=0.6\textheight,width=0.4\linewidth,keepaspectratio]{}};
%\end{canvas}

% Notes 
% \note[item]{}

% 2-column
% \begin{columns}
%   \begin{column}{0.48\textwidth}
%     \begin{itemize}
%         \item
%     \end{itemize}
%   \end{column}
%   \hfill
%   \begin{column}{0.48\textwidth}
%     \begin{itemize}
%         \item
%     \end{itemize}
%   \end{column}
% \end{columns}

