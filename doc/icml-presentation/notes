# June 7th 2013

* Be very clear that we're looking at generative models.
* cite mixture of experts in the discriminative model slide
* Use "if" instead of "assume"; it's less permanent.

* I should some how get to the meat faster.

* Correct citations
* Talk about the axes when presentinga a graph
* Include full details of the experimental setup.
* Using "intutive" instead of "not surprising"
* Including future work might be good.
* Pull MLR earlier?
* s/Negabhan/Negahban


# Slide 1

* took too long "features about your data", wandering
* Logistic regression => mixture of experts

# Slide 2

* negative log-likelihood

# Slide 3

* "EM" => "generally, local optimization"
* Goal of everything:
* "not only get better" => "but converge to the correct parameters"
* Method of moments is more general than spectral methods;
  + Be aware that spectral methods is more general
  + Spectral methods for learning latent-variable models

# Slide 4:

* [k] notation not defined
* Slip up: Say "define generative model"

* Need to say given $\beta_h$ alone, can recover mean, but not sufficient
* Your dot operator changed
* M_p is undefined

* Title: method of moments for mixture of linear regressions (emphasize low-rank)
* Say method: regularize by nuclear norm first before saying analysis
* Refer to AHK work (GMM): before, moments of the data were powers of $\beta$;
here have to work harder to recover them
* Going through derivation: say explicitly we prove sample complexity results,
don't slog through it

* Flip back between visualization and histogram of errors

* "Not too surprising" => "intuitive"
* Figure: emphasize "spectral gets ballpark" (this should be a conclusion)
* Experimental setup: say what the number of examples is, k = 3, d = 3

* Say assumptions
  h is independent of x
  B is full rank

* Slip: "anneal regression coefficient" => "regularization parameter"
  "featurization" => "not deterministic polynomial dependencies"
* Say "this is good when d is small relative to n"
----

- Histogram axes unlabeled
- Reordering: put a bit more "setup" upfront and then go through histories.
-----

- Label that histogram, I found it hard to interpret in the beginning.
It also looked very sparse.
- Maybe address problems with the experiments proactively: like one
thing that stood out for me is that it seems that MoM did not work on
the graph. Given it is standard practice to show the best results in a
talk, perhaps you should explain why it didnt converge as a part of
the talk. But the insight is appreciated.
- i think it took 5 min for it to get anywhere near your
contributions, i think that can be shortened to 2 or 3 min.
- for the error bound, unless it is compared to some previous bound,
it seems rather un-interpretable besides that it is polynomial.
- a few words about complexity
- if the slides are going to contain boxes, i think its better if they
align together, but not a big deal.
----

Time: 15 minutes; 10:16

Big points
 - Introduce notation, make it clear before saying
 - "moments" is way too overloaded
 - Focus on important point; don't say things that hide details
 - reference the figures more

When in doubt, just move on

-- Slide 2
Use examples of latent variable models
Latent CRF
Discriminative LDA
reference h, z, x, y, h

Discriminative models: logistic regression

-- Slide 3
marginal likelihood $p(x)$
don't say NP hard, say intractable
EM can work in practice provided good initialization

-- Slide 4
Method of moments is old
studied in control theory / automata
Observable operator models (not representation learning)
Parameter estimation

-- Slide 5
introduce simple mixture model
Need to introduce notation (x^\otimes 3), \beta, means
Need actual mixture model
tensor power method
words: \beta's...means... forget whitening

-- Slide 6
mixture of linear regressions: simplest model
Generative model is as follows:
Purpose is defining the model:
don't stress dimensionality - we can't estimate in this
To generate a point
Data => Example
Stress: don't know h

E[\beta_h]: define
Say, that $M_1$ doesn't give you $\beta$
Hide hideous equations: focus on first term.
Don't use trace, just do dot product
just say bias, noise
generalized product

don't use word "moments"
low-rank: $d^3$ too subtle

Slide just to summarize algorithm
Then slide for analysis;
don't say too much about analysis; make faster

Slide 9
featurizations: confusing: (t, t^2)
curves are just
label axes
graphs are
explain the figure (build), explain axes

Draw an example where spectral is right basin of attraction but too high,
whereas EM has better objective, but

# June 6th 2013

* Gen vs Disc. graphical model. (*)
* HMMs, GMMs, etc. (*)

* MLE is consistent. EM is not.  (*)
* Get rid of applications nonsense (*)

* MoM the big picture, i.e. /talks/2013/microsoft/ (~)
* Cite the new result in 2009,  (~)
  * Outline the high level idea of tensor factorization (-symmetric).
  * move the tensor factorization to mom. (*)
  * Let's see how we can apply this building block to our scenario. (*)
* -unmixing (*)
* -mom (*)
* -unweidly stuff . (*)
* one slide for y = ... (*)
* diagrmas for y = x . M_p (*)
* Warm up with M1 and M2 (*)
* regression just for y3. (*)
  + Error analysis for just M3
* EM for just 1
* Compared to observable operator 
  - animations.
  - histogram
  - revisit the optimaization picture.

* Get clear the difference between discriminative and generative models.

