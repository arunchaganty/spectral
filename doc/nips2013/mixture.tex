\section{Three-view mixture models}
\label{sec:threeViewMixtureModel}

\Fig{figures/simpleModels}{0.3}{simpleModels}{Three-view mixture models: conditioned on a latent variable $h \in [\nh]$,
we have three conditionally independent observed variables.  We consider
estimation for both directed (left) and undirected (right) versions.
}

In this section, we consider the simplest model to illustrate the form of our method.
Consider the undirected mixture model model in \reffig{simpleModels}(a).
We have one discrete latent variable $h \in [\nh]$ and
three observed variables
$x = (x_1, x_2, x_3) \in \sX_1 \times \sX_2 \times \sX_3 = \sX$
which are conditionally independent given $h$.
Formally, the model is as follows:\footnote{We assume for simplicity that the $x_j$'s are identically distributed conditioned on $h$,
but it is possible to use different distributions (see \citet{anandkumar12moments}).}
\begin{align}
  \label{eqn:mixtureModel}
  p_\theta(x, h) \eqdef \exp \{ \phi(x, h)^\top\theta - A(\theta) \}, \quad\text{where} \, \phi(x, h) \eqdef \phiprior(h) + \sum_{j=1}^3 \philocal(x_j, h) \in \R^d.
\end{align}

In the following, we will detail the algorithm for producing a parameter
estimate $D$.  For simplicity of exposition, we will assume we have an infinite
amount of data and work with the population quantities.  Actual parameter
estimates are constructed by simply plugging in empirical quantities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Step 1: Compute observed moments (aggregation)}

Rather than directly trying to connect the parameters $\theta$ to the data, we start
bottom-up by considering what properties of the data we can measure.
Let $\phix : \sX \mapsto \R^\ell$ denote a \emph{transformation function} (the requirement will be specified later),
which will extract the properties of the observed variables $x$ relevant to $h$.
Define the \emph{observed moments} (analogous to those used in \cite{anandkumar12moments}):
\begin{align}
  M_2 \eqdef \E[\phix(x_1) \otimes \phix(x_2)] \in \R^{\nphix \times \nphix}, \quad\quad
  M_3 \eqdef \E[\phix(x_1) \otimes \phix(x_2) \otimes \phix(x_3)] \in \R^{\nphix \times \nphix \times \nphix}.
\end{align}
Importantly, these quantities are easily estimated from data by taking empirical averages.

\subsection{Step 2: Recover hidden moments (factorization)}
\label{sec:factorization}

By exploiting the conditional independence structure of the model, the observed
moments $M_2,M_3$ (which only reference the observed variables) admit a
factorization in terms of the \emph{hidden moments}
(which provides a window into the hidden variables).

Define
$\pi \eqdef (\BP[h=1], \dots, \BP[h=\nh])$ to be the marginal probabilities over the latent variable $h$,
and define the matrix $B \eqdef (\E[\phix(x_j) \mid h])_{h \in [\nh]} \in \R^{\ell \times k}$,
where each column is the expected transformated data conditioned on $h$.

Now, we can express the observed moments $(M_2,M_3)$ in terms of the hidden moments $(\pi,B)$:
\begin{align}
  M_2 = \sum_h \pi_h B_h \otimes B_h \in \R^{\nphix \times \nphix}, \quad\quad
  M_3 = \sum_h \pi_h B_h \otimes B_h \otimes B_h \in \R^{\nphix \times \nphix \times \nphix}.
\end{align}
\begin{assumption}
  The hidden moments matrix $B \in \R^{\nphix \times \nh}$ has full column rank $\nh$.
\end{assumption}

\citep{anandkumar13tensor} presented a robust tensor power method, which we call $\TensorFactorize$,
which converts consistent estimates of $(M_2,M_3)$ into consistent estimates of $(\pi, B)$ (up to permutation of the columns).
In brief, algorithm first computes a whitening matrix $W \in \R^{\nphix \times \nh}$ such that $W^\top M_2 W = I$,
and uses it to whiten $M_3$.  Then a robust tensor power method is applied to extract the eigenvectors of the whitened $M_3$,
which correspond to columns of $B$.
%\begin{lemma}[tensor factorization \citep{anandkumar13tensor}]
%Let $B \in \R^{\nphix \times \nh}$ be a rank $k$ matrix.
%Let $M_2 = \sum_h \pi_h B_h \otimes B_h \in \R^{\nphix \times \nphix}$ be a rank $k$ symmetric matrix.
%and $M_3 = \sum_h \pi_h B_h \otimes B_h \otimes B_h \in \R^{\nphix \times \nphix \times \nphix}$ be a rank $k$ symmetric tensor.
%There exists an polynomial time algorithm $\TensorFactorize$ that takes consistent estimates of $M_2$ and $M_3$
%and returns consistent estimates of $(\pi, B)$ (up to permutation of the columns).
%\end{lemma}

We need to specify some conditions on the transformation function $\phix$
to ensure that enough information is retained to pinpoint the parameters later.
The following condition characterizes $\phix$:
\begin{property}
  \label{property:transformation}
  The transformation $\phix$ (chosen by us) is such that $C \eqdef
  (\E[\philocal(x_j, h) \mid h])_{h \in [\nh]} \in \R^{d \times h}$ is a
  function of $B$.
\end{property}
Here, $C$ provide a view on the local features $\phi_L(x_j,h)$ which is more
directly related to the model parameters \refeqn{mixtureModel}, whereas $B$ is
what we have recovered so far.  Note that we couldn't recover $C$ directly because it depends on $h$.

However, it is not difficult to construct $\phix$ such that Property~\ref{property:transformation} holds.
Since $h$ is discrete, we can always take $\phix(x) = \flatten((\philocal(x, h))_{h \in [\nh]}) \in \R^{dk}$,
which contains $k$ blocks, one for each latent state.  We can recover a column $C_h = \E[\philocal(x_j, h) \mid h]$
by taking the block of column $B_h = \E[\flatten(\philocal(x_j, h'))_{h' \in [\nh]} \mid h]$
corresponding to $h = h'$.
Often, the local features are simply the tensor product of some function $g(x)$ and an indicator on $h$:
$\philocal(x,h) = \flatten(g(x) \otimes e_h)$.
In this case, it suffices to take $\phix(x) = g(x)$.

Finally, having retrieved $C_h$, it is easy to compute the \emph{hidden moments} $\mu$:
\begin{align}
  \mu \eqdef \E[\phi(x,h)] = \E[\phiprior(h)] + \sum_{j=1}^3 \E[\philocal(x_j,h)] = \sum_{h \in [\nh]} \pi_h (\phi_p(h) + 3 C_h).
\end{align}
In \cite{anandkumar12moments}, these hidden moments would be exactly the parameters (multinomial probabilities), but
for log-linear models, we still need to do more work to extract the parameters $\theta$.

\subsection{Step 3: Recover parameters (optimization)}

For the mixture model, the hidden moments $\E[\phi(x,h)]$ we recover are actually
are the sufficient statistics of the log-linear model.
To elaborate, let us suppose for the moment that we have infinite examples of $(x,h)$.
Let us consider the undirected mixture model.
Then we would seek to maximize the joint log-likelihood:
\begin{align}
L(\theta) \eqdef \E[\log p_\theta(x, h \mid \theta)] = {\underbrace{\E[\phi(x,h)]}_{\mu}}^\top\theta - A(\theta).
\end{align}
Note that the optimization problem here only depends on the data via $\mu$, which is usually estimated from examples of $(x,h)$.
Since $h$ is latent in our latent-variable setting, we must recover $\mu$ from latent observations.
By the moment-generating properties of the log-partition function,
the optimality condition is that $\mu = \E_\theta[\phi(x, h)]$.

We thus obtain a consistent estimator for $\theta$ assuming the exponential family is identifiable:
\begin{assumption}
The log-linear model is identifiable from fully-observed data.
Equivalently, the log-partition function $A(\theta)$ is strongly convex.
\end{assumption}
Identifiability means that the moments $\mu$ uniquely determine $\theta^*$.
In summary, factorization yields the mean parameters of the exponential family,
and we can use convex optimization to retrieve the (canonical) parameters $\theta$.

% Directed mixture model
\paragraph{Directed mixture model}

The directed mixture model, we have $p_\theta(x,h) = p_\theta(h) \prod_{j=1}^3 p_\theta(x_j \mid h)$,
where
$p_\theta(h) \eqdef \exp\{\phiprior(h)^\top\theta - \Aprior(\theta)\}$
and
$p_\theta(x_j \mid h) \eqdef \exp\{\philocal(x_j, h)^\top\theta - \Alocal(\theta; h)\}$.
Consequently, the joint log-likelihood is:
\begin{align}
  L(\theta) = {\underbrace{\E[\phi(x,h)]}_{\mu}}^\top\theta - \E[A(\theta; z)], \quad
  \E[A(\theta, z)] = \Aprior(\theta) + 3 \sum_{h \in [\nh]} \pi_h \Alocal(\theta;h).
\end{align}
Note that in directed models, the log-partition functions depends on the parent variables (in this case, $h$).
Having recovered the marginal distribution over $\pi$, we can compute the desired expectation.
Again, this problem is convex.

\paragraph{Perturbation analysis.}
We have so far worked with exact population values $M_2,M_3,\pi,B,\mu,\theta^*$.
With finite samples, we must work the corresponding estimated quantities $\hat M_2,\hat M_3,\hat\pi,\hat B,\hat\mu,\hat\theta$.
How the error in the moments $(\hat M_2,\hat M_3)$ affects error in $(\hat\pi,\hat B)$ is well studied
(see \citet{anandkumar12moments,anandkumar13tensor}).
The hidden moments $\hat\mu$ are simply a quadratic combination of $(\hat\pi,\hat B)$,
and $\hat\theta = \nabla^{-1} A(\hat\mu)$, which is Lipschitz.
Therefore, we have a consistent estimator with polynomial sample complexity.
