\begin{abstract}
  Latent variable models are at the core of machine learning today, yet
  the local estimation methods in use, like EM, are neither well
  understood nor are do they come with any guarantees. In this paper, we
  study asymptotic rates of consistent estimators, based on the method
  of moments, for a large class of latent variable models. We do so by
  deriving rates for several base cases like the moments of vectors and
  conditional moments and composing results in a modular way using
  combinators like the mixture combinator and the logistic operator.  We
  compare our asymptotic bounds with those of recently proposed method
  of moments algorithms for the mixture of Gaussians and linear
  regressions. Finally, we formalize the intuition that estimates from
  method of moment estimators are good initializations for local methods
  like EM by characterizing the ``local convexity'' region for the above
  two models.
\end{abstract}

