\section{Measurements: partial information from moments} \label{sec:generalModels}

Our learning algorithm for three-view mixture models relies on the fact that
the sufficient statistics of the exponential family
can be expressed as a function of the latent moments.
Of course, these conditions do not hold for general log-linear models.
Rather than falling back on plain local optimization of the likelihood in these
cases, we would still like
to somehow use the \emph{partial information} offered by the latent moments.
%We will show how to incorporate these moments into a likelihood-based
%objective.
Partial information is available when the following holds:
\begin{property}
  There exists some latent variable $h_{j_0} \in [\nh]$ (the bottleneck)
  and three conditionally independent observed variables $x_{j_1},x_{j_2},x_{j_3}$ (views) such that
  each conditional mean matrix $\E[\phix(x_{j}) \mid h_{j_0}] \in \R^{\nphix \times \nh}$ ($j \in \{ j_1, j_2, j_3 \}$) has full column rank $\nh$
  for some transformation $\phix$.
\end{property}

% Examples
\reffig{generalModels} shows examples of log-linear models.
For notational simplicity, assume each latent variable $h_j \in [\nh]$ is an integer
and each observed variable $x_j \in \R^\ell$ is an indicator vector encoding
one of $\ell$
possible values.

\Fig{figures/generalModels}{0.3}{generalModels}{
Three examples of undirected log-linear models.  In each of these cases,
we can recover latent moments, which are themselves not sufficient to identify the parameters,
but provide partial information which leads to a more well-behaved likelihood function.
}

\paragraph{(a) HMM}
Following the reduction in \citet{anandkumar12moments},
we could take $h_2$ as the bottleneck and $x_1, x_2, x_3$ as the three views.
Define $\pi = (\BP[h_1]) \in \R^{\nh}$,
transition matrix $T = (\BP[h_3 \mid h_2]) \in \R^{\nh \times \nh}$
and observation matrix $O = (\E[x_2 \mid h_2]) \in \R^{\nphix \times \nh}$.
$\TensorFactorize$ recovers the marginal probability of the bottleneck
$T \pi$ and conditional means of the three views
$O T^\top \diag(T \pi)^{-1}$,
$O$,
and $O T$.
Note that we can recover $T = O^\dagger(OT)$ and $\pi = T^{-1} (T\pi)$,
from which we can compute the latent moments/sufficient statistics $\mu = \E[\phi(x,h)]$.

However, this model only uses the first three observations,
which doesn't use all the information available.
Let us consider a new reduction:
Let $h_j$ be the bottleneck where $j$ is a \emph{random} variable
taking on values uniformly from $\{2,
\dots, L-1\}$, where $L$ is the number of time steps.
Let $x_{j-1},x_j,x_{j+1}$ be the three views.\footnote{
When we actually estimate the observed moments, we simply average over the choice of $j$.}
Now, note that we can recover $O = \inv{L-2} \sum_{j=2}^{L-1} \E[x_j \mid h_j]$ from the second view,
but using the data more fully.  The conditional means of the first and third
views are now a complex function of the parameters.
%due to marginalization over the rest of the variables,
However, the point is that we can get most of our utility out of the partial
information offered by $O$ and ignore information which is hard to ingest.

\paragraph{(b) Grid model}
Analogous to the second HMM reduction, let $h_{ij}$ be the bottleneck,
where $(i,j)$ is chosen randomly.
Let $x_{ij}^a,x_{ij}^b$ be two of the views, and any other observed variable as the third view.
$\TensorFactorize$ would reveal conditional means $\E[x_{ij}^a \mid h_{ij}]$
and $\E[x_{ij}^b \mid h_{ij}]$,
and a third quantity that involves the latent-to-latent features and the observation features.
This quantity involves marginalizing out the rest of the graph,
and is therefore difficult to use.
Note that if each latent variable had only one observation,
there would be no bottlenecks, and our method would not be applicable.
%to extract something useful like $\BP[h_{ij}, h_{i(j+1)}]$
%in closed form.
%Note that the analog of this was still possible in the first HMM reduction that
%used only the first three observations, but not in the second one.

\paragraph{(c) Tall mixture model}
Let $h_0$ be the bottleneck and $x_1,x_2,x_3$ be the three
views.
$\TensorFactorize$ reveals the conditional means $\E[x_j \mid h_0]$, which are
not even expectations over features in the original model, let alone sufficient
statistics.\footnote{Note that
the model might not be identifiable in general \cite{anandkumar11tree}.
However, the model is identifiable with adequate parameter sharing,
and therefore
parameter estimation in this model is still meaningful.}

In each of these three models, we could extract marginals $\pi = \BP[h_j] \in \R^{\nh}$
and some conditional mean matrix $B = \E[x_{j'} \mid h_j] \in \R^{\nphix \times \nh}$,
but these are insufficient to identify the parameters.
How can we still use this partial information $(\pi,B)$?
To this end, we adopt the measurements framework of \citet{liang09measurements}
(also, see \cite{graca08em}),
which allows us to incorporate moment constraints into a likelihood-based objective in a coherent way.
The basic idea is to define two families of distributions:
$\sQ$, which are consistent with all information; and
$\sP$, the exponential family.  The
framework tries to choose $q \in \sQ$ and $p \in \sP$ to minimize
$\KL{q}{p}$.

Specifically,
let $\bx = (x^{(1)}, \dots, x^{(n)})$ be the vector of observed variables
and let
$\bh = (h^{(1)}, \dots, h^{(n)})$ be the vector of latent variables.
Our model family $\sP$ is defined by product distributions
$p_\theta(\bx,\bh) = \prod_{i=1}^n p_\theta(x^{(i)}, h^{(i)})$ as $\theta \in \R^d$.
Suppose our observations are $(\tilde x^{(1)}, \dots, \tilde x^{(n)})$
and we have recovered $(\pi,B)$, where $\pi$ is the marginal of the bottleneck $h_j$ and $B$ is the conditional mean of $x_{j'}$.

Define $\sQ$ to be $q$ satisfying the following constraints:\footnote{We use a hard constraint
for expository clarity, but in practice, we use a quadratic penalty.  See
\citet{liang09measurements} for details.}
\begin{align}
  q(x^{(i)} = \tilde x^{(i)}) &= 1, \quad i \in [n] \label{eqn:exConstraint} \\
  \inv{n} \sum_{i \in [n]} q(x_{j'}^{(i)} = v, h_j^{(i)} = u) &= \pi_u B_{v u}, \quad u \in [\nh], v \in [\nphix]. \label{eqn:momentConstraint}
\end{align}
The first constraint $\sQ$ encodes our observed examples
and the second puts additional constraints on the latent variables.
Our final objective function is as follows:
\begin{align}
  \min_{q \in \sQ} \min_{\theta \in \R^d} \KL{q(\bx, \bh)}{p_\theta(\bx, \bh)}. \label{eqn:minKL}
\end{align}
\paragraph{Intuitions.}
With only the first constraint \refeqn{exConstraint}, the objective
\refeqn{minKL} is equivalent to our original maximum (empirical) marginal
likelihood problem, which is non-convex.  If the second constraint
\refeqn{momentConstraint} are exactly the features/sufficient statistics of the
model (as in the three-view mixture model from \refsec{threeViewMixtureModel}),
then \refeqn{minKL} is always convex and
converges to the (expected) maximum likelihood objective function as the number of examples $n \to\infty$.

% Optimization
We optimize \refeqn{minKL} using a variant of EM.
Briefly, the E-step is equivalent to a generalized maximum entropy problem,
which we solve by taking the dual, resulting in a parameter for each moment constraint \refeqn{momentConstraint}.
In the M-step, we maximize the expected log-likelihood with respect to $q$.

%$\beta = \argmax_{q} H(q) \tau^\top\beta - B(\beta, \theta) + r(\beta)$
%E-step: $\beta = \arg\max_{\beta} \tau^\top\beta - B(\beta, \theta) - r(\beta)$
%M-step: $\theta = \arg\max_\theta \E_q[\phi(\bx,\bh)^\top - A(\theta)]$.
