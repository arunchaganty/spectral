\section{Experiments} \label{sec:experiments}

%We have argued the case for using measurements of the latent moments to 
%aid parameter estimation.
The spectral decomposition (Steps 1 and 2) in \refsec{threeViewMixtureModel}
provides measurements,
which are used in the optimization problem \refeqn{minKL}.
In this section, we
seek to answer two questions empirically:
(i) how much do the measurements aid estimation, and
(ii) how effectively are the measurements estimated using the
method of moments.

In summary, we find that measurements can improve the fit of the
parameters estimated considerably.
While empirical measurements estimated using the
method of moments are less accurate, we still find that they improve
parameter estimates in general.

% Do measurements help in parameter estimation?
\subsection{Measurements for parameter estimation}

Our generative process for the data is as follows.
We generated 10 random hidden Markov models,
each with $k=3$ hidden states and $\ell=5$ possible observed values. 
For each model, we generated 10 sets of 10,000 sequences of length $L=3$.
We compared the prediction accuracy of parameters estimated using EM
initialized using \refeqn{minKL} when different percentages of the expected
latent measurements, $\E[\phi(x,h)]$, were observed.

%For example, we ran one experiment where we assumed 10\% of the latent
%measurements were observed.

\Fig{figures/measurements}{0.3}{measurements}{a}

\reffig{measurements} plots the training set accuracy of the parameters
micro-averaged over the different data and models.  For comparison, we have
included the accuracy achieved using the method of moments estimation procedure
described in \refsec{factorization} with empirical moment estimates.  We see
that using measurements for even a small percentage of features can improve
prediction accuracy by \todo{x\%}. Spectral estimates too improve accuracy,
though not as markedly; we observed a \todo{x\%} improvement.

% - we ran experiments with different proportions of the expected feature counts being observed and plot performance here. 
% - while such information is not available in practice, we note that it provides a significant improvement over learning with just EM.
% - next, we used spectral techniques to estimate the counts on the data and observed that it also showed a concrete improvement in performance, though not as much as the measurements.

% details:
% - For purposes of illustration, we looked at 10 different hmms with
% parameters $K=3$, $D=5$ and $N=10^5$ samples. We ran each experiment for
% 10 different data samples and report the micro-averages.

% Could it be effective in practice?
\subsection{Method of moments for measurement estimation}

\begin{table}
    \label{tab:errors}
    \begin{tabular}{l | l l | l l l }
        Algorithm & Hidden States & Dimension & Accuracy & Error in $\hat \E[\phi(x,h)]$ & Log Likelihood \\ \hline
        & \multicolumn{2}{|c|}{Mixture Models} & & & \\ \hline
        EM & 2 & 2 & 0.69 (+/- 0.1) & 0.69 (+/- 0.6) & -2.53 (+/- 0.1) \\
        & \multicolumn{2}{|c|}{HMMs} & & & \\ \hline
        % EM
% 3 2 0.648872 (+/- 0.095464) 1.227816 (+/- 0.719636) -3.722825 (+/- 0.245093) 2.350263 (+/- 1.288540)
% 3 3 0.514018 (+/- 0.062330) 1.816636 (+/- 0.739626) -4.216187 (+/- 0.102203) 4.186464 (+/- 1.747718)
% 5 2 0.724705 (+/- 0.070466) 1.131530 (+/- 0.559746) -5.213327 (+/- 0.172906) 3.993639 (+/- 1.360042)
% 
% % Spec
% 3 2 0.797079 (+/- 0.086223) 0.230606 (+/- 0.164765) -3.542495 (+/- 0.203750) 1.383477 (+/- 1.297197)
% 3 3 0.678500 (+/- 0.107540) 1.014050 (+/- 0.604031) -3.766423 (+/- 0.273059) 4.417891 (+/- 2.540264)
% 5 2 0.797205 (+/- 0.055123) 0.186235 (+/- 0.144865) -5.042223 (+/- 0.095900) 2.029760 (+/- 1.227884)
% 
% % Me
% 3 2 0.801385 (+/- 0.084407) 0.010773 (+/- 0.004410) -3.546779 (+/- 0.200814) 1.284341 (+/- 1.345515)
% 3 3 0.705435 (+/- 0.115996) 0.023051 (+/- 0.018703) -3.877286 (+/- 0.135231) 1.466214 (+/- 0.904723)
% 5 2 0.801550 (+/- 0.051041) 0.017168 (+/- 0.003883) -5.032909 (+/- 0.098391) 1.810640 (+/- 1.453650)
    \end{tabular}
    \caption{}
\end{table}

Having shown that measurements can help the task of parameter estimation, we
now present results for variety of models, summarized in \reftab{errors}. 
For each row, we report micro-averaged log-likelihoods and prediction accuracy
over 5 different models with the specified number of parameters and 5 random
samples of $10,000$ instances from the model. 
In the table, we compare randomly initialized EM with EM initialized using (a)
empirical measurements estimated with the method of moments and (b) fully
expected measurements.

First, we note that we found measurements to uniformly improve the prediction
accuracy of EM. The picture for method of moments is similar, though there is
greater variation in the improvement it provides.

% - Generated data from different 3-view mixture models and hmms. 
% - Each averages over 5 different parameters and 5 different data samples.
% - Find that there is quite some variation in spectral performance, but measurements uniformly helps estimation.

% \subsection{Basic mixture model}
% 
% Show simple mixture model works on artifical data.
% 
% Show EM/gradient gets stuck in local optima.
% 
% spectral methods are not statistically efficient (especially with parameter sharing).
% 
% \subsection{Measurements}
% 
% For models in \reffig{generalModels}, do experiments.
% 
% Show that the non-convexity decreases.
% 
% \subsection{Factorial models}
% 
% Show that the unshuffling basically works with near infinite data, while EM gets stuck in local optima.
% 
% \subsection{Part-of-speech induction}
% 
% \cite{kirkpatrick10painless} trains for 1,000 iterations, which takes a long time.
% 63.1\% Basic HMM
% 68.1\% EM, L-BFGS 75.5\%
% 
% Can we match the performance?
% 
% Also, since method of moments doesn't require inference, can we use a much larger corpus
% and get better results.
