Thank you for your comments; we will try to respond to your questions below.

# Major Comments

## Regarding Novelty and Section 4 (Examples)

Several reviewers remarked that our development of an algorithm for parameter estimation in latent variable models and the examples we used in Section 4 were derivative from [8,13]. While we certainly build on their work, the contribution of our work is to present an alternative view point to parameter estimation using the method of moments wherein the moments act as constraints for an objective as opposed viewing the problem as one of solving a polynomial. This perspective allows us to open up the space of possible estimation procedures in a very practical sense. We demonstrate this in section 4 by describing how to estimate parameters for various MRF models, models which cannot actually be handled using prior work.

in particular, our presentation of the HMM might be better appreciated if one realizes that it is actually an undirected MRF with general parameter tying. This allows arbitrary coarse features (e.g. all words ending in '-ing') to be included in the model, which has been shown to be extremely effective for several unsupervised language learning tasks in [25]. Further more,
the moment equations for the grid and tall mixture models are considerably complex and it is not clear how the techniques employed in [8,13] could be used to solve them. 

% In the HMM example, we were able to use all of the data to get fairly reliable estimates of the emission probabilities ($O$) and consequently avoid several nasty local optimal, as the experiments show.

[25] Painless Unsupervised Learning with Features; Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté, John DeNero, Dan Klein; NAACL 2010.

## Strong Convexity of Log-Partition Function (Reviewer 3)
If the log-partition function is not strongly convex, standard results show that the model is not identifiable and no procedure can work. Our algorithm will constrain the parameters to the equivalence class of correct parameters and allows regularization, etc. to break ties if desired. In effect, our method does not require the assumption that the log-partition function be strongly convex, but when it is, it finds an exact fit in the exponential family.

## Regarding how the HMMs and mixture models were generated (Reviewer 3)
We generated parameters for both the mixture model and for the
HMMs by generating the model parameter vector $\theta$ with uniform
random entries in [-1,1]. Note that normalization is taken care of by
the partition function.

## Regarding Assumptions 1 and 2 (Reviewer 3)
The assumption that $B$ have full column rank (Assumption 1) is a standard assumption [8,11,13] and is reasonable in practice. From a practical stand-point, if $B$ is rank deficient, then it is indicative that we are over-estimating the number of latent states, and should choose a smaller $k$.
Assumption 2 is certainly strong, but can still be useful when in a large network, we can find a small subset of sources from which we have 3 conditionally independent views (e.g. as in [26])

[26] Unsupervised Learning of Noisy-Or Bayesian Networks; Yoni Halpern, David Sontag; UAI 2013.

## Regarding Section 5 (Reviewer 6)

The size of the composite domain is indeed the product over the domains of the latent variables. With regard to the sensitivity of the unshuffling procedure, the estimates $A_2$ that appear in the regularization term have a sample complexity that is polynomial in the separation between factors; this is not too different from other separation terms that appear in the method of moments algorithms in [8,13]. 

