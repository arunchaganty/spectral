\section{Unshuffling factorization for factorial mixture models}
\label{sec:factorialModels}

So far, we have relied on the existence of a bottleneck hidden variable $h_j$ on which
we have three conditionally independent views $x_{j_1},x_{j_2},x_{j_3}$.  Such bottlenecks do not always
exist; for example, in \reffig{factorialMixtureModel}(a), neither $h_1$ nor $h_2$ is
a bottleneck.
In this section, we present a technique to handle such models.
We start by creating a composite latent variable $c$ which is in
one-to-one correspondence with the existing latent variables $(h_1,h_2)$.
We can invoke $\TensorFactorize$ to obtain composite marginals and
conditional means, but the identities of $h_1$ and $h_2$ becomes lost in the
composite.  We develop a new algorithm $\UnshuffleFactorize$
to restore the identities and place constraints on the original
parameters.

\Fig{figures/factorialMixtureModel}{0.3}{factorialMixtureModel}{The factorial mixture model (a)
does not have three-view bottlenecks, so we first identify composite bottlenecks,
and then perform an \emph{unshuffling factorization} (b):
Given known composite conditional means
$L_c = \log \BP[x_j \mid c] \in \R^\ell$,
which are derived from combinations of unknown parameters
$f_a = \theta_1(\cdot, a), g_b = \theta_2(\cdot, b)$ and log-normalization scalars $Z_{ab}$,
we group difference vectors into bins (dashed boxes), and maximal bins into sources (red rounded boxes),
producing a linear program with variables $\{f_a\},\{g_b\},\{Z_{ab}\}$ and
a constraint for each $B_c$.
}

We focus on the restricted Boltzmann machine
\cite{smolensky86rbm,salakhutdinov09softmax},
which we refer to as an undirected factorial mixture model
(the directed version is a two-layer sigmoid belief network \cite{saul96sigmoid}).
Let $h = (h_1, \dots, h_s) \in [\nh]^s$ be the latent variables (sources)
and $x = (x_1, x_2, x_3) \in [\nphix]^3$ be the observed variables.
The joint model probability is as follows:
\begin{align}
%p_\theta(x, h) = \prod_{i=1}^s p_\theta(h_i) \prod_{j=1}^3 p_\theta(x_j \mid h), \quad
%p_\theta(x_j \mid h) = \exp\left\{ \sum_{i=1}^s \theta_i(x_j, h_i) - A(\theta; h) \right\},
p_\theta(x, h) = \exp \left\{ \sum_{j=1}^3 \sum_{i=1}^s \theta_i(x_j, h_i) - A(\theta) \right\},
\end{align}
where each $\theta_i \in \R^{\nphix \times \nh}$ is the parameter matrix for the $i$-th source.

Let $B \eqdef (\BP[x_j \mid c])_{x_j \in [\nphix],c \in [\nh]^s} \in
\R^{\nphix \times \nh^s}$ be the conditional mean of $x_j$ given composite cluster $c$.

\begin{assumption}
  Assume that $B$ has full column rank ($k^s$).\footnote{Note that this implies
  $d \ge k^s$, which is a rather stringent assumption, but is necessary for the
  three-view method of moments technique to work.}
\end{assumption}

First, we run $\TensorFactorize$ on estimates of
$M_2 \eqdef \E[x_1 \otimes x_2]$ and $M_3 \eqdef \E[x_1 \otimes x_2 \otimes x_3]$,
which produces $B = (\E[x_j \mid c])_{c \in [\nh]^s}$.
%Next, we still have to extract the actual parameters $\theta$ from $B$. 
In the simple mixture model,
the clusters produced by $\TensorFactorize$ are in exact one-to-one correspondence
with the actual mixture components, but in the factorial case, we must solve
a \emph{credit assignment problem} to match combinations of the $s$ sources
with columns of $B$.

Let $\{ v_{ia} \}_{i \in [s], a \in [k]}$, where $v_{ia} = \theta_i(\cdot, a) \in \R^\ell$,
be the collection of parameter vectors that we seek to recover.
Let $L \in \R^{\ell \times k^s}$ be the elementwise logarithm of $B$; that is
$L(x_j, h) = \log B(x_j, h)$.
The key observation is that the columns of $L$ are exactly the unordered collection of vectors
$\{ \sum_{i=1}^s v_{ia_i} - Z_{\ba} : \ba = (a_1, \dots, a_s) \in [\nh]^s \}$,
where $Z_{\ba} = \log \sum_{e=1}^\ell (\sum_{i=1}^s \exp(v_{ia_i}(e)))$ is the
log-normalization constant (a scalar) that sums over possible values $e$ of $x_j$.

Note that there is a combinatorial dimension to this problem since the association between
columns of $L$ and combinations of $\{ v_{ia} \}$ is unknown.
For example, example (\reffig{factorialMixtureModel}(b)),
consider $s=2$ sources, each taking on $k=3$ values
(for convenience, let $f_a = v_{1a}$ and $g_b = v_{2b}$).
The following algorithm, $\UnshuffleFactorize$,
creates a linear program that places constraints on
variables $\{v_{ia}\}$ and $Z_{\ba}$:
\begin{enumerate}

\item Compute the difference vector $L_c - L_d$ for every distinct pair of
columns $c,d \in [\nh^s]$.

\item Put the pairs $(c,d)$ into bins such that the difference vectors $L_c-L_d$
in each bin are \emph{comparable}, by which we mean that they
differ by a constant scalar times $\bone$ (for the normalization constant).

\item Keep only the maximal bins.
Such bins should contain $\nh^{s-1}$ pairs, each corresponding to a fixed
setting of the $s-1$ sources
and changing the value of only one source.
Specifically, the difference vector is equal to $v_{ia} - v_{ia'}$ up to a
constant scalar for some source $i \in [k]$ and distinct values $a, a' \in
[\nh]$.

\item Group the bins into sources:
  For each source $i = 1, \dots, s$,
  choose any bin $\omega$, declare it as $v_{i1} - v_{i2}$,
  and add $\omega$ to the source.
  Let $\gamma$ be the set of $c$ such that $\omega$
  contains some difference vector $L_c - L_d$,
  and include any other bin $\omega'$ with the same $\gamma$;
  $\omega$' should correspond to $v_{i1} - v_{ia}$ for $a \ge 3$.

  Then throw away any bin whose difference vector is $-(L_c - L_d)$
  or is comparable to the difference of two existing difference vectors
  (we want to discard bins corresponding to $v_{i2} - v_{i1}$ and $v_{i2} - v_{i3}$).
  Finally, the resulting source should have $\nh-1$ bins, corresponding to $\{ v_{i1} - v_{ia} : 2 \le a \le \nh \}$.
  At this point, we have identified which vectors contribute to each $L_c$,
  taking care of the combinatorial aspect of the problem,
  but have yet to recover the actual vectors.

  \item To do this last step, construct a linear program with $\nh^s \cdot d$ constraints, one for each entry of $B$.
  It will have $s\nh \cdot d + \nh^s$ variables,
  one for each of $s\nh$ vectors $v_{ia}$ and $\nh^s$ for the normalization constants $Z_\ba$.
  Note that $Z_\ba$ is actually a deterministic function of the the vectors, but since it is a non-linear one,
  it is difficult to work with; therefore, we create a separate variable to keep the problem linear.

\end{enumerate}

It turns out that this LP is rank deficient for two reasons,
the first intrinsic to our model,
and the second due to loose linearization.
First, the vectors $v_{ia}$ are non-identifiable since we can always add any vector $u \in \R^\ell$ to
all the vectors of a source $i_1$ ($v_{i_1 a}$ for all $a \in [\nh]$)
and subtract $u$ from all the vectors of another source $v_{i_2 a}$ for all $a \in [\nh]$.
Second, if we treated log-normalization constants $\{Z_\ba\}$ as a deterministic
function of $v_{ia}$, then the resulting program would have no other sources
of rank deficiency, but would also be non-linear.
Treating $\{Z_\ba\}$ as independent variables creates additional rank
deficiency.

Despite this, in the spirit of providing partial information,
we can still use the linear constraints provided by $\UnshuffleFactorize$.
In particular, define a regularization term
$R(\theta, Z) = \frac{1}{2} (\bA_1 \theta - \bA_2 Z - B)^2$, where $\bA_1$ and $\bA_2$ are coefficients
recovered by unshuffling.
In particular, we propose optimizing $\min_{\theta, Z} \sum_{i=1}^n -\log
p_\theta(x^{(i)}) + \lambda R(\theta, Z)$,
where $\lambda$ will be relatively large.
Although the resulting objective is still non-convex,
the regularization imposes quite strong constraints,
resulting in an optimization problem
that effectively operates over a much lower-dimensional subspace.
