% December 31st 2012

The last few days have been slow on account of various break-related factors.

A general summary of status updates:

* I've spent some time empirically looking at the role of the Q's in the
  problem. 
* The moment computations for the data with the smallest number of dimensions
  for one-two of the datasets is finally complete, though it continues to run
  for other datasets. I will be running the moment algorithms shortly to see at
  last how it works in practice.
* I've debugged and tested more extensively the Java version of Algorithm B. It
  works quite well and in a similar manner as the python code on smaller
  datasets. However, the recovery keeps returning imaginary eigenvalues when the
  number of clusters is large enough. I need to tease out whether this is
  a numerical issue or simply due to too small spectral values in the tail?

# Convergence Behaviour with different Q's

## Experiment Setup

I plotted a scatter plot of the absolute error vs. condition number of each
scheme for $200$ different samples of $10^4$ data points produced with the same
parameters, i.e. $K = 2, D = 3$, weights of $(0.5, 0.5)$, and $\beta
= \{(1,0,0), (0,1,0)\}$. Aside from using different settings of the $Q$, I also
used three different initialization strategies,

* Zero: $B = \beta \diag(\pi) \beta^T$ was initialized at zero.
* Near-Optimal: $B$ initialized at the exact value plus Gaussian noise with std 0.1.
* Random: $B$ initialized with random values, generated by a Gaussian with std 1.0.

I ran one set of experiments without any regularization, and with
regularization, using $\lambda = 10^{-3}$.

To get an idea of the aggregate performance, I also plotted an ellipse centered
at the mean (condition number, error) and radii equal to the standard deviation. 

## Results

Here is a table summarizing the recovery performance with various initializations. 

+-----------------+---------------------+--------------------+---------------------+
|                 | Zero                | Near Optimal       | Random              |
+=================+=====================+====================+=====================+
| All             | 0.707 (+/- 0.000)   | 0.283 (+/- 0.070)  | 2.985 (+/- 0.661)   |
+-----------------+---------------------+--------------------+---------------------+
| Dirichlet (100) | 0.293 (+/- 0.084)   | 0.325 (+/- 0.084)  | 1.642 (+/- 0.612)   |
+-----------------+---------------------+--------------------+---------------------+
| Dirichlet (500) | 0.140 (+/- 0.044)   | 0.226 (+/- 0.063)  | 1.641 (+/- 0.752)   |
+-----------------+---------------------+--------------------+---------------------+
| Local (100)     | 0.498 (+/- 0.012)   | 0.525 (+/- 0.029)  | 1.756 (+/- 0.679)   |
+-----------------+---------------------+--------------------+---------------------+
| Local (500)     | 0.499 (+/- 0.013)   | 0.525 (+/- 0.023)  | 1.696 (+/- 0.646)   |
+-----------------+---------------------+--------------------+---------------------+
| None            | 0.033 (+/- 0.011)   | 0.168 (+/- 0.074)  | 1.632 (+/- 0.679)   |
+-----------------+---------------------+--------------------+---------------------+
| Random (100)    | 0.707 (+/- 0.000)   | 0.289 (+/- 0.071)  | 3.005 (+/- 0.673)   |
+-----------------+---------------------+--------------------+---------------------+
| Random (500)    | 0.707 (+/- 0.000)   | 0.282 (+/- 0.071)  | 3.007 (+/- 0.758)   |
+-----------------+---------------------+--------------------+---------------------+
| Subset (100)    | 0.707 (+/- 0.000)   | 0.294 (+/- 0.074)  | 3.034 (+/- 0.655)   |
+-----------------+---------------------+--------------------+---------------------+
| Subset (500)    | 0.707 (+/- 0.000)   | 0.293 (+/- 0.070)  | 3.027 (+/- 0.713)   |
+-----------------+---------------------+--------------------+---------------------+
| White           | 0.354 (+/- 0.007)   | 0.429 (+/- 0.041)  | 2.317 (+/- 0.712)   |
+-----------------+---------------------+--------------------+---------------------+

Table: Performance without regularization.

+-------------------------+---------------------+--------------------+---------------------+
| Method \ Initialization | Zero                | Near Optimal       | Random              |
+=========================+=====================+====================+=====================+
| All                     | 0.707 (+/- 0.000)   | 0.421 (+/- 0.177)  | 1.994 (+/- 0.939)   |
+-------------------------+---------------------+--------------------+---------------------+
| Dirichlet (100)         | 0.301 (+/- 0.097)   | 0.317 (+/- 0.084)  | 1.493 (+/- 0.634)   |
+-------------------------+---------------------+--------------------+---------------------+
| Dirichlet (500)         | 0.145 (+/- 0.049)   | 0.209 (+/- 0.053)  | 1.547 (+/- 0.631)   |
+-------------------------+---------------------+--------------------+---------------------+
| Local (100)             | 0.502 (+/- 0.004)   | 0.505 (+/- 0.011)  | 1.101 (+/- 0.545)   |
+-------------------------+---------------------+--------------------+---------------------+
| Local (500)             | 0.499 (+/- 0.014)   | 0.506 (+/- 0.014)  | 1.181 (+/- 0.511)   |
+-------------------------+---------------------+--------------------+---------------------+
| None                    | 0.033 (+/- 0.011)   | 0.158 (+/- 0.061)  | 1.633 (+/- 0.717)   |
+-------------------------+---------------------+--------------------+---------------------+
| Random (100)            | 0.707 (+/- 0.000)   | 0.424 (+/- 0.174)  | 1.600 (+/- 0.838)   |
+-------------------------+---------------------+--------------------+---------------------+
| Random (500)            | 0.707 (+/- 0.000)   | 0.410 (+/- 0.163)  | 1.564 (+/- 0.812)   |
+-------------------------+---------------------+--------------------+---------------------+
| Subset (100)            | 0.707 (+/- 0.000)   | 0.327 (+/- 0.125)  | 1.080 (+/- 0.500)   |
+-------------------------+---------------------+--------------------+---------------------+
| Subset (500)            | 0.707 (+/- 0.000)   | 0.302 (+/- 0.088)  | 1.060 (+/- 0.469)   |
+-------------------------+---------------------+--------------------+---------------------+
| White                   | 0.438 (+/- 0.007)   | 0.449 (+/- 0.020)  | 1.785 (+/- 0.713)   |
+-------------------------+---------------------+--------------------+---------------------+

Table: Performance with regularization.

And the aggregates over different methods, 

+-------------------------+-------------------+-----------------------+
| Method \ Initialization | Recovery Error    | Condition Number      |
+=========================+===================+=======================+
| All                     | 1.183 (+/- 1.088) |   1.000 (+/-   0.000) |
+-------------------------+-------------------+-----------------------+
| Dirichlet (100)         | 0.728 (+/- 0.699) |   6.020 (+/-   1.517) |
+-------------------------+-------------------+-----------------------+
| Dirichlet (500)         | 0.651 (+/- 0.780) |   3.485 (+/-   0.322) |
+-------------------------+-------------------+-----------------------+
| Local (100)             | 0.815 (+/- 0.592) |   6.002 (+/-   1.847) |
+-------------------------+-------------------+-----------------------+
| Local (500)             | 0.818 (+/- 0.573) |   3.575 (+/-   0.402) |
+-------------------------+-------------------+-----------------------+
| None                    | 0.610 (+/- 0.831) |   2.652 (+/-   0.037) |
+-------------------------+-------------------+-----------------------+
| Random (100)            | 1.122 (+/- 1.040) | 163.485 (+/- 254.244) |
+-------------------------+-------------------+-----------------------+
| Random (500)            | 1.113 (+/- 1.047) | 117.053 (+/- 196.286) |
+-------------------------+-------------------+-----------------------+
| Subset (100)            | 1.025 (+/- 0.997) |   5.819 (+/-   1.332) |
+-------------------------+-------------------+-----------------------+
| Subset (500)            | 1.016 (+/- 1.001) |   3.638 (+/-   0.474) |
+-------------------------+-------------------+-----------------------+
| White                   | 0.962 (+/- 0.887) |   1.000 (+/-   0.000) |
+-------------------------+-------------------+-----------------------+

Table: Aggregate Errors and Condition Numbers 

\begin{figure}
\centering
\subfigure[Without Regularization]{
  \includegraphics[width=5in]{../../figures/dec-31-2012/conditioning-zero-ur.png}}
\subfigure[With Regularization]{
  \includegraphics[width=5in]{../../figures/dec-31-2012/conditioning-zero.png}}
\caption{Zero Initialization}
\end{figure}
\begin{figure}
\centering
\subfigure[Without Regularization]{
\includegraphics[width=5in]{../../figures/dec-31-2012/conditioning-near-optimal-ur.png}}
\subfigure[With Regularization]{
\includegraphics[width=5in]{../../figures/dec-31-2012/conditioning-near-optimal.png}}
\caption{Near-Optimal Initialization}
\end{figure}
\begin{figure}
\centering
\subfigure[Without Regularization]{
\includegraphics[width=5in]{../../figures/dec-31-2012/conditioning-random-ur.png}}
\subfigure[With Regularization]{
\includegraphics[width=5in]{../../figures/dec-31-2012/conditioning-random.png}}
\caption{Random Initialization}
\end{figure}

## Empirical Findings

### Regularization

Regularization worsened the performance when initializing at zero; this is
because the singular values can initially be small enough to be set to zero by
soft thresholding. Some methods, like random and subset, do not diverge from
zero, resulting in the $0.707$ score. When initializing near the optimal value,
regularization improved the performance of the Dirichlet, Local and None
methods, both in terms of the mean and the variance. However, the performance of
the random Qs and the subsets degraded significantly. When using random
initializations, regularization improved the general performance across the
board.

### Condition Numbers

The condition numbers of the random methods were particularly bad, though their
performance was not proportionally worse than methods which were better
conditioned. The methods which were engineered to have a unit condition number,
`all` and `white` also did not perform as well as not using any $Q$ at all. In
general, using more points decreased the condition number and improved
performance. Also, looking at the actual condition numbers, I could verify that
the large values for the random technique were not just due to the presence of
outliers.

Within each cluster, it seems like the performance is noisily related to the
condition number. This leads me to believe that the condition numbers aren't
really so important to the problem.

### General notes on performance

Of all the techniques, not using any special $Q$ scheme (i.e. $Q$ is the
identity matrix) worked the best. Interestingly, the method worked better when
initialized at zero than when initialized near the optimal value.

Using $Q$ drawn from a sufficiently many Dirichlets, i.e. 500, was the next best
method. The $\alpha$ for the Dirichlet was the uniform prior $0.1 * 1/N$. Using
the Dirichlet with too few points leads to a terribly conditioned problem
($\kappa \approx 700$) with poor results $\epsilon \approx 1.0$.

The local methods do not work as well as I had seen earlier (why?).

