% January 17, 2013  

I spent the last few days massaging the code to generate data from non-linear
experts and to get the EM implementation to work on it. 

The results are interesting in that EM doesn't demolish it like it usually does.
It seems to fail pretty badly with increasing non-linearity (nD = the order of
the polynomial used). The reported figures are the absolute error on the best
aligned $\beta$s. The experiments were run on $10^5$ data points $20$ times
each.

# D nD  K   Error    (+/- Variance)

1   1   3   0.795869 (+/- 1.123832)
1   2   3   3.623332 (+/- 4.503195)
1   3   3   2.568450 (+/- 10.207033)

1   1   5   1.130433 (+/- 0.620073)
1   2   5   1.411400 (+/- 0.519733)
1   3   5   25.860900 (+/- 21.023116)

1   1   10  2.869671 (+/- 1.615262)
1   2   10  4.503179 (+/- 1.171522)
1   3   10  60.017875 (+/- 42.419096)

2   1   3   0.442375 (+/- 0.601629)
2   2   3   0.359920 (+/- 1.077390)
2   3   3   53.995490 (+/- 66.237213)

2   1   5   1.582020 (+/- 1.034257)
2   2   5   4.693100 (+/- 5.549562)
2   3   5   57.936005 (+/- 39.002148)

2   1   10  3.395200 (+/- 1.627100)
2   2   10  8.638135 (+/- 3.908530)
2   3   10  43.161535 (+/- 7.732736)

3   1   3   0.133380 (+/- 0.577903)
3   2   3   5.318855 (+/- 6.854261)
3   3   3   36.547545 (+/- 29.859390)

3   1   5   0.181060 (+/- 0.539280)
3   2   5   9.051095 (+/- 5.216001)
3   3   5   61.703861 (+/- 32.211783)

3   1   10  error
3   2   10  10.691392 (+/- 6.618079)
3   3   10  81.493200 (+/- 15.905653)

The averages for several of these runs is increased by a couple of outliers. For
example, the row "2 1 3" has an error of 8e-4 on 18/20 of the runs, with two at
3.5. This seems indicative of an initialization problem. The parameters I'm
initializing with are uniform mixture weights with random $\beta$.

Also, to note, the $\beta$s were initialized using random Gaussians, which is
the same distribution as the model. 

An implementation note: it's always a win to field out these computations with
as many jobs on the NLP cluster.


