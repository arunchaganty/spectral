November 17th 2012
==================

Today's objective was to push all the experiments on to the NLP clusters
before leaving for the thanksgiving break.

Progress
--------

In order to meet the goals, I needed to fix some aspects of the
implementation, including using features generated by the random seed
(to circumvent the problem of storing the entire map in memory) and
compressing the corpus by grouping infrequently occurring words into
categories like @DIGIT@, @MISC@, etc.

I decided on the datasets I would run the algorithm on:
  * 12 million New York Times articles with about 100 and 1000 hidden states
  * 5 datasets with 1 million articles generated by the HMM with a full 42 states each
  * 5 datasets with 1 million articles generated by the HMM with merged 15 states each
In order to generate data from a 15 state HMM, I used a simple heuristic
algorithm of taking last (n - k) states and merge with one of the first
k states. 

Finally, I farmed out the brown clustering (wcluster[^1]) and spectral
learning tasks on to the NLP machines. I queued up three instances for
each configuration of the spectral algorithms. 

I fell short of the goal because I did not manage to implement
Baum-Welch to learn the HMMs using EM, which is the only remaining
experiment to be performed.

[^1]: To get wcluster running on the NLP machines, I needed to prepend
/usr/local/lib64 to the LD_LIBRARY_PATH
