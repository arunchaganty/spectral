\section{Introduction}
\label{sec:introduction}

% 1. Latent variable models are good.
Latent variable models offer a succinct representation of a rich model
family. 
% 2. Learning them is hard.
Despite their success across many fields
\cite{quattoni04crf,haghighi06prototype,liang06discrimative,kirkpatrick10painless},
learning these models remains a difficult problem due to the
non-convexity of the likelihood. Local optimization (e.g.
expectation-maximization) is the standard approach, but is susceptible
to local optima.

% 3. People have approached unsupervised learning with the MoM magic sauce, but the sauce is limited.
Recently, unsupervised learning techniques based on the method of moments and
spectral decomposition have offered a refreshing and promising perspective on
this learning problem \citep{hsu09spectral,anandkumar11tree,anandkumar12moments,anandkumar12lda,hsu12identifiability,balle11transducer,balle12automata}.
These methods exploit the linear algebraic properties of the model and
factorize the moments into parameters, providing strong theoretical guarantees.
However, these methods are not as universally applicable as EM, applying to a limited set of models.

\begin{figure*}
  \centering
  \input{figures/approach.tikz}
  \caption{Overview of approach}
\end{figure*}

% 4. State what we do: exploit moment constraints to make the problem easier.
In this work, we exploit the spectral method to learn moment constraints
on the observed variables and show how these constraints lead to consistent parameter estimates for a broad class of graphical models.

% 5. We get moments from third-order tensors from bottlenecks and factorize them into marginals.
Our approach is illustrated in \figureref{approach}. 
First, we identify conditionally independent observed variables $x_1,
  x_2, x_3$ for each hidden variable $h$ (bottlenecks). 
We use the tensor factorization algorithm of
  \citet{anandkumar12moments,anandkumar13tensor} to identify the
  conditional moments.
Given these factors, we show that for every clique of hidden variables
  $\sC$, the piecewise likelihood of the observed variables is convex in
  the marginal distribution. 
This guarantees that we can recover the true marginal distributions
  using EM; finally, with some post-processing, we are able to
  consistently recover the true parameters of the model.
In \sectionref{piecewise}, we detail our algorithm and describe the
  settings and technical assumptions in which this approach is guaranteed
  to work.
As a particular example, we are able to recover parameters for the
  grid model depicted in \figureref{approach} under the condition that
  $O$ is full-rank.

For undirected log-linear latent variable models, this approach allows us to 
  recover the marginals for each clique, given which the learning
  problem is convex (\sectionref{undirected}).
Finally, our approach gracefully extends to more general model families
  in which we are unable to recover the marginal distributions for all
  cliques; in such scenarios, we use posterior regularization to constrain
  EM. 
Empirically, we find that this approach leads to better solutions
  (\sectionref{experiments}).

