\section{Introduction}
\label{sec:introduction}

% 1. Latent variable models are good.
Latent-variable graphical models provide compact representations of data
and have been employed across many fields % \todo{find a much better/broader set of citations}
\cite{ghahramani1999variational,jaakkola1999variational,blei03lda,quattoni04crf,beaumont2004bayesian,haghighi06prototype}.
% 2. Learning them is hard.
However, learning these models remains a difficult problem due to the
non-convexity of the negative log-likelihood.  Local methods such as expectation maximization (EM)
are the norm, but are susceptible to local optima.

% 3. People have approached unsupervised learning with the MoM magic sauce, but the sauce is limited.
Recently, unsupervised learning techniques based on the spectral method of moments
have offered a refreshing perspective on
this learning problem
\citep{mossel2005learning,
hsu09spectral,
bailly2010spectral,
song2011spectral,
anandkumar11tree,
anandkumar12lda,
anandkumar12moments,
hsu12identifiability,
balle12automata}.
%\todo{make sure we are citing the right people}
These methods exploit the linear algebraic properties of the model to
factorize the moments into parameters, providing strong theoretical guarantees.
However, they apply to a limited set of models, and are thus
not as broadly applicable as EM.

\begin{figure}[t]
  \label{fig:approach}
  \centering
  \subimport{figures/}{approach.tikz}
  \caption{
 % \todo{need three steps: tensor factorization, optimize composite likelihood, recover model parameters;}
  Overview of our approach: we first use tensor factorization to learn the conditional moments for each hidden variable.
  Then we optimize a composite likelihood to recover the hidden clique marginals,
  given which we can extract the model parameters.
  }
\end{figure}

% DONE: need to say from the beginning that we have a hybrid approach, not just using method of moments;
% there are really two contributions: one is the consistent estimate result for class of models,
% and the other is the general idea of using partial constraints]

% 4. State what we do: exploit moment constraints to make the problem easier.
In this paper,
we show that a much broader class of discrete directed and undirected graphical models can be consistently estimated,
specifically those in which each hidden variable has three conditionally
independent observed variables (``views'')---see \propertyref{bottleneck}.
The key idea is to leverage the spectral method of moments,
not to directly provide a consistent parameter estimate as in previous work,
but as partial constraints on a likelihood-based objective.
%For a broad class of directed and undirected graphical models

%In this work, we exploit the spectral method to learn moment constraints
  %on the observed variables and show how these constraints lead to consistent parameter estimates 
  %for any graphical model with the property that there exist at least
  %three ``views'' or conditionally independent observed variables for
  %every hidden variable (\propertyref{bottleneck}).

% 5. We get moments from third-order tensors from bottlenecks and factorize them into marginals,
% then get parameters.
% DONE: try to talk about the approach in terms of three steps
Our approach is illustrated in \figureref{approach}. 
First, we identify the views for each hidden variable $h_i$ (for example,
$x_1^a$, $x_1^b$ and $x_3^a$ are conditionally independent given $h_1$) and use
the tensor factorization algorithm of
\citet{anandkumar13tensor} to identify the \emph{conditional
moments} $\Pr[x_i^a \mid h_i]$ and $\Pr[x_i^b \mid h_i]$ for each $i$.
Second, we optimize a \emph{composite marginal likelihood} to recover the marginals over
each hidden clique (e.g., $\mathbb P(h_2, h_3, h_4)$).
Normally, such a marginal likelihood objective would be non-convex,
but given the conditional moments, we obtain a convex objective,
which can be globally optimized using EM.
Finally, the hidden marginals yield the model parameters
via simple post-processing in the directed case and convex optimization in the undirected case.
%Given these factors, we show that there exists a set of observed
  %variables for every clique such that the likelihood is convex in the
  %parameters of that clique. 
%This guarantees that we can recover the true marginal distributions
  %using EM.
  %finally, with some post-processing, we are able to
  %consistently recover the true parameters of the model.
In \sectionref{directed}, we detail our algorithm and
show that our algorithm yields consistent parameter estimates
under non-degenerate rank assumptions on the conditional moments.
%describe the
  %settings and technical assumptions in which this approach is guaranteed
  %to work.
% PL: hard to talk about $O$ and $T$ at this stage.
%As a particular example, we are able to recover parameters for the
  %model depicted in \figureref{approach} if $O$ and $T$ are full-rank.

%Our approach also applies to undirected log-linear latent variable
  %models, where we use the recovered marginals to make the learning
  %problem convex (\sectionref{undirected}).
%Finally, our approach gracefully extends to more general model families
%  in which we are unable to recover the marginal distributions for all
%  hidden cliques; in such scenarios, we use posterior regularization to constrain
%  EM.  
%\todo{Is this possible?:  We show that empirically, using such constraints alleviates local
%optima issues.}
