\section{Convex Piecewise Likelihoods}
\label{sec:piecewise}

The linear algebraic solution to recover parameters presented in the
  previous section is easy to understand and analyze, however it is
  sensitive to noise. 
In this section we propose an alternate solution; optimize the piecewise
  likelihoods.
We show that under the same conditions for \algorithmref{directed}, the
  piecewise likelihoods are strictly convex, guaranteeing that
  gradient-based optimization will converge to the unique global
  optimum.

\begin{algorithm}
  \caption{\LearnPiecewise}
  \label{algo:piecewise}
  \begin{algorithmic}
    \REQUIRE A graphical model $\sG$ satisfying \textbf{(P1)}, \textbf{(P2)}, data $\sD$
    \ENSURE Parameters $\theta$ for $\sG$

      \FOR{$h \in H$} 
        \STATE Apply \TensorFactorize for each bottleneck $h$ and learn parameters $\Pr(x \mid h) ~ \forall x \in \sB(h)$.

%    \COMMENT{Recover observation potentials $O$ using bottlenecks}
      \ENDFOR
%      \COMMENT{\textbf{Step 2:} Recover clique potentials from the piecewise likelihood.}
      \FOR{every clique $\sC = \{h_1, \cdots, h_m\} \in \sG$} 
      \STATE Let $x_\sC = \{x_1, \cdots, x_m\}$.
      \STATE $\hat P_\sC = \arg\max_{P_\sC \in \Delta_{k^m-1}} \E_{\vec x_\sC}[ \log P_\sC( \pinv O_{x_1}, \cdots, \pinv O_{x_m} ) ]$.
%      Run expectation-maximization to convergence on the piecewise likelihood \eqref{eqn:piecewise}, over data $\{\vec x_\sC : x \in \sD\}$
      \ENDFOR
  \end{algorithmic}
\end{algorithm}


\paragraph{Intuition}

Consider a hidden Markov model \figureref{fig:examples-hmm} with
  2 states ($k=2$), but an arbitrary number of emissions, $d$. 
After learning the observation potentials $O$ in step 1 of
  \algorithmref{piecewise}, the log-likelihood objective for $\pi$ is,
\begin{align}
  \sL_p(x_1) &= \sum_{x_1} \log( \pi_1 O_{1,x_1} + (1-\pi_1) O_{2,x_1} )

\end{align}

Empirically, we observe that optimizing the piecewise objective gives
  better solutions with finite samples.

\begin{figure}
  \centering
  \label{fig:piecewise-objective}
  \includegraphics[width=\columnwidth]{figures/piecewise-objective.pdf}
  \caption{Comparing the piecewise objective with the moment-matching objective for just one parameter}
\end{figure}

The negative log-likelihood is more convex than the moment-reconstruction loss (\figureref{piecewise-objective})

\begin{corollary}
  The asymptotic variance of optimizing the log-likelihood, $\Sigma_p$
  is strictly less than that of the moment-matching objective
  $\Sigma_m$; $\Sigma_p \succ \Sigma_m \succ 0$.
\end{corollary}

For part two of our algorithm, we must identify the clique potentials between the
  hidden variables. 
Consider a clique $\sC = \{h_1, \cdots, h_m\}$. 
Define $\vec x_\sC = \Union_{h \in \sC} \sB(h)$ to be the collection of
  participating observed variables for this clique.
The piecewise log-likelihood of $\vec x_{\sC} =\sB(\sC)$ can then be
  written down as,
\begin{align}
  \sL_p(\vec x_\sC) 
    &= \E_{\vec x_\sC}[ \sum_{h_1, h_2, \ldots, h_m} P(h_1, h_2, \cdots, h_m) 
    \prod_{i=1}^{m} O_{h_i}(\vec x_{\sB(h_i)}) ]. \label{eqn:piecewise}
\end{align}
As we saw earlier, this objective is convex in the marginal distribution
  $P(h_1, h_2, \cdots, h_m)$, which implies that
  expectation-maximization in step 2 converges to a global optimum.
The following lemma describes conditions under which the piecewise
  likelihood objective is strongly convex, guaranteeing that the global
  optimum is unique.

  \begin{lemma} (Strict convexity of \equationref{piecewise})
  \label{lem:hessian}

  Let $\vec x_{\sC} \eqdef\{x_1, x_2, \cdots, x_L\}$. The Fisher information
  matrix $J_\sC$ for the piecewise objective in \equationref{piecewise} at the
  true parameters is 
  \begin{align}
    \grad^2 \sL_p(\vec x_\sC) 
    &= \sum_{x_1, x_2, \ldots, x_L} \tilde O_{\vec x_\sC} \tilde O_{\vec x_\sC}^T,
  \end{align}
  where $\tilde O \in \Re^{K^m \times D^L}$ is the matrix constructed as follows,
  \begin{align}
    \tilde O_{\sC, \vec x_\sC} &= O_{h_1}(\vec x_{\sB(h_1)} O_{h_2}(\vec x_{\sB(h_2)} \cdots O_{h_m}(\vec x_{\sB(h_m)}).
  \end{align}

  Consequently, the Fisher matrix is non-singular iff the span of $D^L$ vectors $\{ O_{\vec x_\sC} \}$ is rank-deficient.
  Furthermore, the smallest singular value of $J_\sC$ is bounded below
    by \todo{condition}.
\end{lemma}
\begin{proof}
  Rewrite $\sL_p$ as inner product.

  Differentiate twice.

  Observed denominators cancel at the true parameters.

  Finally, the Fisher information matrix $J_\sC$ for the piecewise
  objective in \equationref{piecewise} at the true parameters is 
  \begin{align}
    \grad^2 \sL_p(\vec x_\sC) 
    &= \sum_{x_1, x_2, \ldots, x_L} \tilde O_{\vec x_\sC} \tilde O_{\vec x_\sC}^T,
  \end{align}
  where $\tilde O \in \Re^{K^m \times D^L}$ is the matrix constructed as follows,
  \begin{align}
    \tilde O_{\sC, \vec x_\sC} &= O_{h_1}(\vec x_{\sB(h_1)} O_{h_2}(\vec x_{\sB(h_2)} \cdots O_{h_m}(\vec x_{\sB(h_m)}).
  \end{align}

  Consequently, the Fisher matrix is non-singular iff the span of $D^L$
    vectors $\{ O_{\vec x_\sC} \}$ is rank-deficient.
\end{proof}

Intuitively, the condition \todo{(C1)} says that the observations give
  us sufficient information about the latent variables to identify the
  model. 
For example, when each observed symbol corresponds to exactly one value
  of the hidden variable, i.e. $P(x \mid h )$ is binary, the above
  condition trivially holds\verify.
\todo{Is this condition the weakest possible? Can we hope to identify a model for which this is not true?}.

As a special case, suppose each hidden variable $h \in \sC$ had
  a distinct observed variable $x \in \sB(h)$. 
Then $\tilde O = O_{h_1} \otimes O_{h_2} \cdots \otimes O_{h_m}$, which
  has singular values $\sigma_1(O_{h_1}) \sigma_1(O_{h_2}) \cdots
  \sigma_1(O_{h_m}), \cdots, \sigma_k(O_{h_1}) \sigma_k(O_{h_2}) \cdots
  \sigma_k(O_{h_m})$. 
Thus, the smallest singular value is of the order $\sigma_k(O)^m$; as
  expected, the more variables in a clique, the more sensitive the
  estimation problem is to noise.
If two hidden variables $h_1, h_2$ share the same bottleneck
  variables, i.e. $\sB(h_1) = \sB(h_2)$, then we must have more
  observations in order to tease apart the effect of the two
  variables\verify\reword.


Next, we appeal to the delta method\findcite{van de vaart} to give us
  consistency and asymptotic rates on sample complexity.
\begin{corollary}(Consistency and Asymptotic Complexity)
  \label{cor:asymptotics}
  Step 2 of \algorithmref{directed} is a consistent estimator for
  the clique potential for every clique $\sC$.

  Furthermore, the asymptotic variance of $\theta_{\sC}$ is
  \todo{condition}.
\end{corollary}

Finally, coupled with \theoremref{three-view}, we can guarantee that
  \algorithmref{directed} is a consistent estimator for $\theta$ and 
  that has polynomial computational and sample complexity.
\begin{theorem}
  \algorithmref{directed} is a consistent estimator for $\theta$.
  Furthermore, it has a computational complexity of \todo{$O(?)$} and an
    asymptotic sample complexity of \todo{$O(?)$}.
\end{theorem}
\begin{proof}
  \todo{Staple together \theoremref{three-view} and \corollaryref{cor:asymptotics}}.
\end{proof}

\subsection{More examples}

We will now instantiate our algorithm for several examples, illustrated in \figureref{examples}.

\begin{figure}
  \label{fig:examples}
  \subfigure[Hidden Markov Model] {
    \label{fig:examples-hmm}
    \includegraphics{figures/hmm.pdf}
  }
%  \subfigure[Directed grid model] {
%    \label{fig:examples-grid}
%    \includegraphics{figures/grid.pdf}
%  }
  \subfigure[Tree model] {
    \label{fig:examples-tree}
    \includegraphics{figures/tree.pdf}
  }
\end{figure}


\paragraph{Hidden Markov Model}

\paragraph{Latent Tree Structure}

\paragraph{Directed Grid Model}

