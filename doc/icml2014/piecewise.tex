\section{Parameter Estimation with Convex Piecewise Likelihoods}
\label{sec:piecewise}

Key idea: don't ride the MoM train all the way, rather let it do some pre-processing to make your job easier.

\paragraph{Example 1: Improving statistical efficiency by averaging over trigrams}

The algorithm described in \citet{anandkumar12moments} looks only at the
  first three symbols (words) in a sequence (sentence). 
The algorithm can trivially be made to use the data more efficiently
  by using every trigram in a sentence, i.e. $\hat M_{123}
  = \sum_{x\in\sD} \sum_{i = 1}^{|x| - 2} x_{i} \otimes x_{i+1} \otimes
  x_{i+2}$. However, in this case, $\hat M_{123} \to M_{123}$, where 
\begin{align}
  M_{123} &= \sum_{h} \E[(\pi + T \pi + T^2 \pi \cdots T^{|x|-2} \pi)]_h M_1 \otimes O \otimes OT.
\end{align}

Using the tensor factorization algorithm, we recover $O$ and $OT$,
  ignoring $M_1$. From $O$, it is easy to estimate $\pi$ using maximum
  likelihood:
\begin{align}
  \hat \pi &= \frac{1}{|\sD|} \sum_{x\in\sD} O^T_{x_1}.
\end{align}

\paragraph{Example 2: Recovering $T$ from piecewise likelihood}

Inverting $O$ to get $T$ in the above procedure can be dangerous when $O$ is poorly conditioned. 
Alternative: solving the piecewise likelihood,
\begin{align}
  \sL_{p}(\pi, T, O) &= \sum_{x\in\sD} \log( \sum_{h_1, h_2} \underbrace{\pi(h_1) T(h_2 | h_1)}_{P(h_1,h_2)} O(x_1 | h_1) O(x_2 | h_2) ),
\end{align}
Because we know $O$, the likelihood is convex in $P$. Once we have recovered $P^*$ from the above optimization problem, we can recover $T$ as follows:
$T(h_1, h_2) = \frac{P(h_1, h_2}{\sum_{h_2} P(h_1, h_2)}$

It is necessary that $\sL_p$ be {\em strictly convex} to for $P^*$ to be unique \findcite{IBM-1 problem}, as we will prove below,

\begin{theorem}
  $\sL_p$ is strictly convex. The hessian $\grad^2 \sL_p$ is full rank. 
\end{theorem}
\begin{proof}

  We can write out the Hessian as,
\begin{align}
  \grad^2 \sL_p &= \E[ \sum_{x_1, x_2} \frac{ (O_{x_1} \otimes O_{x_2}) (O_{x_1} \otimes O_{x_2})^T }{(O_{x_1}^T P O_{x_2})^2} ].
\end{align}

  Incorporating expectations (complicated expression),
\begin{align}
  \grad^2 \sL_p &= \E[ \sum_{h_1, h_2} \frac{ (O_{x_1} \otimes O_{x_2}) (O_{x_1} \otimes O_{x_2})^T }{(O_{x_1}^T P O_{x_2})^2} ].
\end{align}

Firstly, this is full-rank. Secondly, we can show a lower bound on the singular value as follows.
\end{proof}

\begin{corollary}(Asymptotic efficiency)
  The estimator has so and so efficiency.
\end{corollary}
\begin{proof}
  From the delta method: depends on $\Sigma^-1$, which is basically $\det \Sigma$.
\end{proof}

\paragraph{Example 3: Grid model}

Next, consider the grid model shown in \figureref{grid}. Let us
assume every emission has same parameters $O$ and every transition has
$T$.

Each hidden
node has two observations; conditioning on the hidden node, the two
observations and any third observation are independent, and thus we can
get a consistent estimate for $O$.

Once again, we can write down a piecewise likelihood,
\begin{align}
  \sL_{p}(\pi, T, O) &= \sum_{x\in\sD} \log( \sum_{h_1, h_2, h_3, h_4} P(h_1, h_2, h_3, h_4).
\end{align}

The same approach as above holds; all that remains to be shown is that the model is indeed identifiable,

\begin{theorem}(Identifiability of Grid Model)
  $\sL_p$ is strictly convex. The hessian $\grad^2 \sL_p$ is full rank. 
\end{theorem}
\begin{proof}
  We can write out the Hessian as,
\begin{align}
  \grad^2 \sL_p &= \E[ \sum_{x_1, x_2} \frac{ (O_{x_1} \otimes O_{x_2}) (O_{x_1} \otimes O_{x_2})^T }{(O_{x_1}^T P O_{x_2})^2} ].
\end{align}

  Incorporating expectations (complicated expression),
\begin{align}
  \grad^2 \sL_p &= \E[ \sum_{h_1, h_2} \frac{ (O_{x_1} \otimes O_{x_2}) (O_{x_1} \otimes O_{x_2})^T }{(O_{x_1}^T P O_{x_2})^2} ].
\end{align}

  Firstly, this is full-rank. Secondly, we can show a lower bound on the singular value as follows.
\end{proof}

\paragraph{Example 5: Tree model}

Finally, we consider a tree model as shown in \figureref{tree}. Let us
assume every emission has same parameters $O$ and every transition has
$T$.

\todo{Showing identifiability is a bit trickier in this model. Also, the
algorithm is unlikely to scale.}

