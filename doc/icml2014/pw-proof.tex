\subsection{\lemmaref{mom-pw-variance}}
\label{app:pw-proof}

In \sectionref{piecewise}, we compared the asymptotic variance
  $\Sigma^\ml_\sC$ of the composite likelihood estimator for a clique
  $\sC$, with that of the pseudoinverse estimator, $\Sigma^\mom_\sC$. 
  Now we will derive these asymptotic variances in detail.
%In this part, we will derive the asymptotic variances of these two
  %estimators and compare their relative efficiencies.

Recall, that in \sectionref{piecewise} we simplified notation by taking
  $m=1$ and flattening the moments $M_\sV$ and hidden marginals $Z_\sC$
  into vectors $\mu \in \Re^d$ and $z \in \Re^k$ and similarly
  flattening the tensor $O$ into a matrix $O \in \Re^{d\times
  k}$.
The hidden marginals $z$ and observed marginals $\mu$ are related via
  $\mu = O z$.

Note that $\mu$ and $z$ are multinomial distributions and are hence
constrained to lie on simplexes. This makes asymptotic analysis tricky
because $\mu, z$ lie on subspaces with Lebesgue measure zero in $\Re^d$
and $\Re^k$ respectively. 

To handle this constraint, we re-parameterize the problem in terms of
  $\tm \in \Re^{d-1}$ and $\tz \in \Re^{k-1}$, 
which are related to the original representation as follows:
\begin{align*}
  \mu &= 
    \begin{bmatrix}
      \tm \\
      1 - \ones^\top\tm
    \end{bmatrix} 
  &
  z &= 
    \begin{bmatrix}
      \tz \\
      1 - \ones^\top\tz
    \end{bmatrix}.
\end{align*}

Now, $\tm$ and $\tz$ are related as,
\begin{align*}
  \mu &= O z \\
  \begin{bmatrix}
    \tm \\
    1 - \ones^\top\tm
  \end{bmatrix} 
  &=
    \begin{bmatrix}
      O_{\neg d,\neg k} & O_{\neg d, k} \\ 
      O_{d,\neg k} & O_{d, k} \\ 
    \end{bmatrix}
    \begin{bmatrix}
      \tz \\
      1 - \ones^\top\tz
    \end{bmatrix} \\
 %  \mu
 %  &=
 %    \begin{bmatrix}
 %      O_{\neg d,\neg k}\tz + O_{\neg d, k}(1 - \ones^\top \tz) \\ 
 %      O_{d,\neg k}\tz + O_{d, k}(1 - \ones^\top \tz) \\ 
 %    \end{bmatrix} \\
 %  &=
 %    \begin{bmatrix}
 %      O_{\neg d,\neg k} - O_{\neg d, k} \ones^\top \\
 %      O_{d,\neg k} - O_{d, k} \ones^\top
 %    \end{bmatrix} \tz 
 %    + 
 %    \begin{bmatrix}
 %      O_{\neg d, k} \\
 %      O_{d, k}
 %    \end{bmatrix} \\
 %  &=
 %    \begin{bmatrix}
 %      \tO \\
 %      \ones - \ones^\top\tO
 %    \end{bmatrix} \tz 
 %    + 
 %    \begin{bmatrix}
 %      O_{\neg d, k} \\
 %      1 - \ones^\top O_{\neg d, k}
 %    \end{bmatrix} \\
  \tm &= O_{\neg d,\neg k} \tz + O_{\neg d, k} - O_{\neg d, k} \ones^\top \tz \\
      &= \underbrace{(O_{\neg d,\neg k} - O_{\neg d, k} \ones^\top )}_{\eqdef \tO} \tz +  O_{\neg d,k}.
\end{align*}

We also describe $\mu$ in terms of $\tz$,
\begin{align*}
  \mu 
  &= 
    \begin{bmatrix}
      \tm \\
      1 - \ones^\top\tm
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      \tO \\
      - \ones^\top\tO
    \end{bmatrix} \tz 
    + 
    \begin{bmatrix}
      O_{\neg d, k} \\
      1 - \ones^\top O_{\neg d, k}
    \end{bmatrix}.
\end{align*}

With this reparameterization, we are ready to derive the asymptotic
variances of the two estimators.

\begin{proof}[Proof for \lemmaref{mom-pw-variance}]
  First, let us look at the
  asymptotic variance of the pseudoinverse estimator. With the
  reparameterization $\tz$, we get,
  \begin{align*}
    \tm &= \tO \tz +  O_{\neg d,k} \\
    \tz &= \tOi (\tm - O_{\neg d,k}).
  \end{align*}
  Note that $\mu$ is a multinomial distribution, hence the variance of
    $\hat\tm$ is $(\tD - \tm \tm^\top)$ where $\tD \eqdef \diag(\tm)$.
  Since $\tz$ is just a linear transformation of $\tm$,
  the asymptotic variance of $\hat{z}^\mom = \tOi (\hat\tm - O_{\neg d,k})$ is:
  \begin{align*}
      \Sigmamom &= \tOi \Var(\hat\tm) \tOit \\
      &= \tOi (\tD - \tm \tm^\top) \tOit.
      %&= \tOi \tD \tOit - \tOi \tD \ones \ones^\top \tD \tOit,
  \end{align*}
  
  Now, let us look at the variance of the piecewise estimator.  Using
  the delta-method \cite{vaart98asymptotic} we have that the asymptotic
  variance of 
  %$\widehat{z}^\ml = \argmin_{\tz \in [0,1]^{k-1}} \hat\E[-\ell(x)]$ is,
  % Definitely not optimizing over the whole set
  $\hat{z}^\ml = \argmax_{\tz} \hat\E[\ell(x ; \tz)]$ is,
  \begin{align*}
    \Sigmaml &= \E[\grad^2 \ell(x ; \tz^*)]^{-1} \Var[\grad \ell(x ; \tz^*)] \E[\grad^2 \ell(x ; \tz^*)]^{-1},
  \end{align*}
  where $\ell(x;\tz)$ is the log-likelihood of the observations $x$
  given parameters $\tz$. We can write $\ell(x;\tz)$ in terms of $\tz$
  and $\tO$ as,
  \begin{align*}
    \ell(x;\tz) 
              &= \log( \mu[x] ) \\
              &= \log \left( 
    e_x^\top \begin{bmatrix}
      \tO \\
      - \ones^\top\tO
    \end{bmatrix} \tz 
    + 
    e_x^\top \begin{bmatrix}
      O_{\neg d, k} \\
      1 - \ones^\top O_{\neg d, k}
    \end{bmatrix}
    \right),
 %             
 %             \BI[x \neq d] \tm[x] + \BI[x = d] (1 - \ones^\top \tm ) )\\
 %             &= \log( \BI[x \neq d] (\tO[x] \tz + O_{\neg d, k}[x]) \\
 %             &\quad + \BI[x = d] (1 - \ones^\top (\tO \tz + O_{\neg d, k})) ).
  \end{align*}
where $e_x$ is an indicator vector on $x$.

Taking the first derivative,
\begin{align}
  \grad \ell(x; \tz)
  &= 
  \frac{
  1}{\mu[x]}
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}^\top
 e_x \nonumber \\
  &= 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}^\top
  D^{-1} e_x, \label{eqn:lhood-grad}
\end{align}
where $D \eqdef \diag(\mu)$.

It is easily verified that the expectation of the first derivative is indeed $\zeros$.
%Note that the expectation of the first derivative is indeed $\zeros$,
\begin{align*}
  \E[\grad \ell(x; \tz)]
  &= 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}^\top
  D^{-1} \E[e_x] \\
  &= 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}^\top
  D^{-1} \mu \\
  &= 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}^\top
  \ones \\
  &= \tOt\ones - \tOt\ones \\
  &= \zeros.
\end{align*}

Taking the second derivative,
\begin{align}
  \grad^2 \ell(x; \tz)
  &= 
  \frac{
  1}{\mu[x]^2}
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}^\top
 e_x e_x^\top 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}
  \nonumber
 \\ 
  &= 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}^\top
    D^{-1} e_x e_x^\top D^{-1} 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}. \label{eqn:lhood-hess}%
\end{align}

From \equationref{lhood-grad} and \equationref{lhood-hess}, we get
  \begin{align*}
    \E[\grad^2 \ell(x; \tz^*)] 
    &= -
        \begin{bmatrix}
          \tO \\
          -\ones^\top \tO
        \end{bmatrix}^\top
          D^{-1} \E[e_x e_x^\top] D^{-1} 
        \begin{bmatrix}
          \tO \\
          -\ones^\top \tO
        \end{bmatrix} \\
    \Var [\grad \ell(x; \tz^*)] 
    &= \hphantom{-}
        \begin{bmatrix}
          \tO \\
          -\ones^\top \tO
        \end{bmatrix}^\top
          D^{-1} \E[e_x e_x^\top] D^{-1} 
        \begin{bmatrix}
          \tO \\
          -\ones^\top \tO
        \end{bmatrix} \\
    &= \hphantom{-}
        \begin{bmatrix}
          \tO \\
          -\ones^\top \tO
        \end{bmatrix}^\top
          D^{-1} D D^{-1} 
        \begin{bmatrix}
          \tO \\
          -\ones^\top \tO
        \end{bmatrix} \\
    &= \hphantom{-}
        \begin{bmatrix}
          \tO \\
          - \ones^\top \tO
        \end{bmatrix}^\top
        \begin{bmatrix}
          \tD\inv & \zeros \\
          \zeros^\top & \td\inv
        \end{bmatrix}
        \begin{bmatrix}
          \tO \\
          - \ones^\top \tO
        \end{bmatrix} \\
        &= \hphantom{-} 
          \tOt \tD\inv \tO + \td\inv \tOt \ones \ones^\top \tO,
  \end{align*}
where $\td = 1 - \ones^\top \tm$.
As expected, $\E[\grad^2 \ell(x)] = -\Var [\grad \ell(x)]$ because
$\hat{z}$ is a maximum likelihood estimator. 

Finally, the asymptotic variance of $\Sigmaml$ is,
\begin{align*}
    \Sigmaml &= \E[\grad^2 \ell(x ; \tz^*)]^{-1} \Var[\grad \ell(x ; \tz^*)] \E[\grad^2 \ell(x ; \tz^*)]^{-1} \\
      &= \Var[\grad \ell(x;\tz^*)]^{-1} \\
      &= \left( \tOt \tD\inv \tO + \td\inv \tOt \ones \ones^\top \tO \right)\inv.
      %&= \left( \tOt \tD\inv \tO  \right)\inv - 
      %    \frac{\left( \tOt \tD\inv \tO \right)\inv  \tOt \ones \ones^\top \tO \left( \tOt \tD\inv \tO \right)\inv}
      %    {\td + \ones^\top \tO \left( \tOt \tD\inv \tO \right)\inv \tOt \ones}.
\end{align*}

Given our assumptions, $\ones \succ \mu \succ \zeros$. Consequently,
$\tD$ is invertible and the asymptotic variance is finite and our
estimator is consistent as well. 
\end{proof}

\subsection{Comparing the pseudoinverse and composite likelihood estimators}

In \lemmaref{mom-pw-variance}, we derived concrete expressions for the
asymptotic variances of the pseudoinverse and composite likelihood
estimators, $\Sigmamom$ and $\Sigmaml$ respectively. In this section, we
will study their relative efficiency, $e^\pi = \Tr(\Sigmaml
\Sigmamomi)$.

\paragraph{Case I: $d = k$}
First, let us consider the case where $d=k$. Under our assumptions $\tO$
is invertible, and we can simplify the expression of the asymptotic
variance of the composite likelihood estimator, $\Sigmaml$, 
\begin{align*}
    \Sigmaml 
      &= \left( \tO^\top(\tD\inv + \td\inv \ones \ones^\top) \tO \right)\inv \\
      &= \tO\inv (\tD\inv - \td\inv \ones \ones^\top)\inv \tO\tinv \\
      &= \tO\inv (\tD - \frac{\tD \ones \ones^\top \tD}{\td + \ones^\top \tD \ones}) \tO\tinv \\
      &= \tO\inv (\tD - \frac{\tm \tm^\top}{1 - \ones^\top \tm + \ones^\top \tm}) \tO\tinv \\
      &= \tO\inv (\tD - \tm \tm^\top) \tO\tinv \\
      &= \Sigmamom,
\end{align*}
where we have used the fact that $\tD \ones = \tm$ and $\td
= 1 - \ones^\top \tm$.
Thus, we observe that the asymptotic variance of the pseudoinverse estimator
matches that of the composite likelihood estimator when $d=k$, and the
pseudoinverse estimator is efficient in this case.

\paragraph{Case II: Uniform moments}
Next, let us consider the case where the moments are the uniform
distribution, where $\mu = \frac{\ones}{d}$ and $\tD = \frac{1}{d} I$.
The expressions for $\Sigmaml$ can be simplified as
follows,
\begin{align*}
    \Sigmaml 
      &= \left( \tOt(d I + d \ones \ones^\top) \tO \right)\inv \\
      &= \frac{1}{d} \left( \tOt\tO + \tOt\ones \ones^\top \tO \right)\inv \\
      &= \frac{1}{d} \left(
      (\tOt\tO)\inv - \frac{(\tOt\tO)\inv \tOt\ones \ones^\top \tO (\tOt\tO)\inv}
        {1 + \ones^\top \tO (\tOt\tO)\inv \tOt\ones} 
        \right)\\
      &= \frac{1}{d} \left(
      \tOi\tOit - \frac{\tOi\tOit\tOt \ones \ones^\top \tO\tOi\tOit}
        {1+ \ones^\top \tO \tOi \tOit \tOt\ones} 
        \right)
        \\
      &= \frac{1}{d} \left(
      \tOi\tOit - \frac{\tOi \ones \ones^\top \tOit}
        {1+ \|\tO \tOi \ones\|_2^2} 
        \right) \\
      &= \frac{1}{d} \left(
      \tOi\tOit - \frac{\tOi \ones \ones^\top \tOit}
        {1+ \|\ones_U\|_2^2} 
        \right),
\end{align*}
where $\ones_U \eqdef \tO \tOi \ones = \tOit \tOt \ones$ is the projection of $\ones$ onto
the column space of $\tO$. 

Next, we can simplify the expression for $\Sigmamomi$,
\begin{align*}
    \Sigmamom &= 
      \tOi \left( \frac{I}{d} - \frac{\ones \ones^\top}{d^2} \right) \tOit \\
    \Sigmamomi &=
        \left(
        \frac{1}{d} 
          \tOi\tOit - \frac{1}{d^2} \tOi \ones \ones^\top \tOit
        \right)\inv \\
        &=
        d
          (\tOi\tOit)\inv \\
        &\quad
          + d^2
          \frac{
              (\tOi\tOit)\inv \tOi \ones \ones^\top \tOit (\tOi\tOit)\inv 
          }{
            d - \ones^\top \tOit (\tOi\tOit)\inv \tOi \ones 
          } \\
          &=
        d
          \tOt\tO + d^2
          \frac{
              \tOt\tO \tOi \ones \ones^\top \tOit \tOt\tO
          }{
            d - \ones^\top \tOit \tOt\tO \tOi \ones 
          } \\
          &=
        d \left(
          \tOt\tO +
          \frac{
              \tOt \ones \ones^\top \tO
          }{
            d - \|\tOi \tO \ones\|_2^2 
          } \right) \\
          &=
        d \left(
          \tOt\tO +
          \frac{
              \tOt \ones \ones^\top \tO
          }{
            d - \|\ones_U\|_2^2 
          } \right).
\end{align*}

Now, we are ready to study the relative efficiency. Let $\bbk = k-1$,
\begin{align*}
  e^\mom &= \frac{1}{\bbk} \Tr(\Sigmaml \Sigmamomi) \\
    &= \frac{1}{\bbk}
    \Tr \Bigg( \frac{1}{d} \left(
      \tOi\tOit - \frac{\tOi \ones \ones^\top \tOit}
        {1+ \|\ones_U\|_2^2} 
        \right) \\
    &\quad \hphantom{\bbk \Tr}
        d
          \left(
          \tOt\tO +
          \frac{
              \tOt \ones \ones^\top \tO
          }{
            d - \|\ones_U\|_2^2 
          }
          \right)
          \Bigg) \\
    &= 
    \frac{1}{\bbk} \Tr(I) + \frac{1}{\bbk} \Tr\left(
          \frac{
              \tOi\tOit \tOt \ones \ones^\top \tO
          }{
            d - \|\ones_U\|_2^2 
          } \right) \\
    &\quad
    - \frac{1}{\bbk} \Tr\left(
      \frac{\tOi \ones \ones^\top \tOit \tOt\tO}{1+ \|\ones_U\|_2^2} 
      \right) \\
    &\quad
    - \frac{1}{\bbk} \Tr\left(
        \frac{\tOi \ones \ones^\top \tOit \tOt \ones \ones^\top \tO
}{(d - \|\ones_U\|_2^2 )(1+ \|\ones_U\|_2^2)} 
          \right) \\
%%%%
    &= 1 + \frac{1}{\bbk}
          \frac{
              \|\tOit \tOt \ones\|_2^2
          }{
            d - \|\ones_U\|_2^2 
          } 
        - \frac{1}{\bbk}
        \frac{\|\tO \tOi \ones\|_2^2}{1+ \|\ones_U\|_2^2} \\
    &\quad
    - \frac{1}{\bbk}
    \frac{\|\tOit \tOt \ones\|_2^2  \|\tO \tOi \ones\|_2^2}
        {(d - \|\ones_U\|_2^2 ) (1+ \|\ones_U\|_2^2)} \\
&= 1
    + \frac{\|\ones_U\|^2}{d - \|\ones_U\|^2}
    - \frac{\|\ones_U\|^2}{1 + \|\ones_U\|^2}\\
&\quad
    - \frac{\|\ones_U\|^4}{(1 + \|\ones_U\|^2)(d - \|\ones_U\|^2)}.
\end{align*}

Now, recall that $\tm = \tO \tz + O_{\neg d, k}$, and $\tm = \frac{1}{d} \ones$. Thus,
\begin{align*}
  \frac{1}{d} \ones &= \tO \tz + O_{\neg d, k} \\
  \frac{1}{d} \tO \tOi \ones &= \tO \tOi \tO \tz + \tO \tOi O_{\neg d, k} \\
  \frac{1}{d} \ones_U &= \tO \tz + \tO \tOi O_{\neg d, k} \\
  \frac{1}{d} \ones_U &=  \frac{1}{d} \ones  - O_{\neg d, k} + \tO \tOi O_{\neg d, k} \\
  \ones_U &=  \ones  - d (I - \tO \tOi) O_{\neg d, k}.
\end{align*}

This gives us bounds on the projection of $\ones$,
\begin{align*}
  \|\ones\|  - d\|(I - \tO \tOi) O_{\neg d, k}\| 
    &\le 
      \|\ones_U\| 
      &\le \|\ones\|  + d\|(I - \tO \tOi) O_{\neg d, k}\|.
\end{align*}
Note that $O_{\neg d, k}$ consists of positive entries which sum to less
than 1, and that $(\tO \tOi - I)$ is a projection on to a $d-k$
dimensional subspace. Thus, $\|(\tO \tOi - I) O_{\neg d, k}\|_2 \le k$

% Firstly, if $\ones$ lies in the null space of $\tO$, then the variances
% of the two are equal. 

% Next, consider the trace of the two, which gives
% us a measure of $\|\hat{z}^\mom\|^2 - \|\hat{z}^\ml\|^2$,
% \begin{align*}
%     \Tr(\Sigmamom - \Sigmaml)
%     &= \frac{1}{d} 
%     \Tr(\tOi \ones \ones^\top \tOit) \left(
%     \frac{1}{1+ \| \tO \tOi \ones\|^2} - \frac{1}{d}
%     \right) \\
%     &= \frac{1}{d} \|\tOi \ones\|^2 \left(
%     \frac{1}{1+ \| \tO \tOi \ones\|^2} - \frac{1}{d}
%     \right) \\
%     &= \frac{1}{d} \|\tOi \tO \tOi \ones\|^2 \left(
%     \frac{1}{1+ \| \tO \tOi \ones\|^2} - \frac{1}{d}
%     \right) \\
%     &\ge \frac{\sigma_{k-1}(\tO)}{d} \|\tO \tOi \ones\|^2 \left(
%     \frac{1}{1+ \| \tO \tOi \ones\|^2} - \frac{1}{d}
%     \right).
% \end{align*}
% 
% It remains for us to bound the projection $\|\tO \tOi \ones\|$. Observe,
%  \begin{align*}
%    O^\top \ones 
%      &= \sum_{\bh = \ones}^{\bh \prec k} \sum_{x = \ones}^{d} \Pr(x | \bh) e_\bh  \\
%      &= \ones \\
%    \tOt \ones
%        &= (O_{\neg d, k}^\top - \ones O_{\neg d, k}^\top) \ones  \\
%        &= \sum_{\bh = \ones}^{\bh \prec k} \sum_{x = \ones}^{x \prec d} \Pr(x | \bh) e_\bh - (\sum_{x = \ones}^{x \prec d} \Pr(x | k)) \ones^\top  \\
%        &= \sum_{\bh = \ones}^{\bh \prec k} (1 - \Pr(d | \bh)) e_\bh - (1 - \Pr(d | k)) \ones  \\
%        &= \sum_{\vh = \ones}^{\vh \prec k} (\Pr(d | k) - \Pr(d | \vh))e_\bh \\
%        &= \tO_{d,k} \ones^\top - \tO_{d,\neg k}.
%  \end{align*}
%  Let $c = \|\tO^\top \ones\|_2 \le \sqrt{k}$.
%  Let $\tO = U W V^\top$ be the SVD of $\tO$. We have,
%  \begin{align*}
%    \ones^\top U W V^\top 
%        &= \ones^\top \tO \\
%    \ones^\top U 
%      &= \ones^\top \tO V W\inv \\
%    \|\ones^\top U\|_2 
%        &\le \frac{\|\ones^\top \tO\|_2}{\sigma_{k}(\tO)} \\
%        &\le \frac{c}{\sigma_{k}(\tO)} \\
%    \|\ones^\top U\|_2 
%        &\ge \frac{c}{\sigma_{1}(\tO)}.
%  \end{align*}


% With concrete expressions for $\Sigmamom$ and $\Sigmaml$, we can quantitatively evaluate their asymptotic efficiency,
% \begin{proof}[Proof for Corollary X] %\corollaryref{efficiency}]
%   Let $\bar k = k -1$ and $\bar d = d -1$. 
%   We have that
%   \begin{align*}
%     \Sigmamom 
%       &= \tOi \tD \tOit - \tOi \tD \ones \ones^\top \tD \tOit, \\
%     \Sigmaml &= 
%          \tOi \tD \tOit - \frac{\tOi \tD v v^\top \tD \tOit }{1 + v\top \tD v - \ones\top \tD \ones}.
%   \end{align*}
% 
%   We use the following identity,
%   \begin{align*}
%     \Tr( A_1 A_2\inv ) 
%       &= \Tr( (A - x x^\top) (A - yy^\top)\inv ) \\
%       &= \Tr( (A - x x^\top) (A\inv + \frac{(A\inv y)(A\inv y)^\top }{1 - y^\top A\inv y}) ) \\
%       &= \Tr( I - xx^\top A\inv + \frac{ y (A\inv y)^\top - x x^\top (A\inv y)(A\inv y)^\top}{1 - y^\top A\inv y} ) \\
%       &= k - x^\top A\inv x + \frac{ y^\top A\inv y - (x^\top A\inv y)^2 }{1 - y^\top A\inv y}.
%   \end{align*}
% 
%   Let us now consider their relative efficiencies, 
%   \begin{align*}
%     e^\mom 
%         &\eqdef \frac{1}{\bbk} \Tr(\Sigmaml \Sigmamomi ) \\
%         \Tr( \tOi \tS\inv \tOit \tOt \tilde\Sigma\inv \tO ) \\
%       &\quad - \frac{1}{\bar k} \frac{s \Tr( \tOi \tS\inv J \ones \ones^\top J \tS\inv \tOit \tOt  \tilde\Sigma\inv \tO  )}
%       {1 + s \ones^\top J \tS\inv J \ones} \\
%         &= \frac{1}{\bar k} \Tr( J \tS\inv J \tilde\Sigma\inv ) - \frac{1}{\bar k} \frac{s \ones^\top J \tS\inv J \tilde\Sigma\inv J \tS\inv J \ones }
%       {1 + s \ones^\top J \tS\inv J \ones},
%   \end{align*}
%   where $J \eqdef \tO \tOi = U U^\top$ as before. 
%   
%   Note that $\Tr(J) = \rank(J) = {\bar k}$.
%   Next we use H\"{o}lder's inequality for the trace: $\Tr(D A) \le
%   \|D\|_\infty \Tr(A)$ if $D$ is diagonal. Thus,
%   \begin{align*}
%       \Tr( J \tS\inv J \tilde\Sigma\inv ) 
%         &= \Tr( \tS\inv J \tilde\Sigma\inv J ) \\
%         &\le \|\tS\inv\|_\infty \Tr( J \tilde\Sigma\inv J ) \\
%         &= \|\tS\inv\|_\infty \Tr( \tilde\Sigma\inv J ) \\
%         &\le \|\tS\inv\|_\infty \|\tilde\Sigma\inv\|_\infty \Tr( J ) \\
%         &= {\bar k} \|\tS\inv\|_\infty \|\tilde\Sigma\inv\|_\infty.
%   \end{align*}
% 
%   To bound the next term, we need bound the projection of the ones
%   vector, $\ones$, through $J \eqdef U U^\top$,
%   \begin{align*}
%       \ones^\top J \tS\inv J \tilde\Sigma\inv J \tS\inv J \ones \\
%       &=
%       (\ones^\top U) (U^\top \tS\inv U) (U^\top \tilde\Sigma\inv U) \\
%       &\quad (U^\top \tS\inv U) (U^\top \ones)  \\
%       &\ge 
%       (\tS\inv)^2_{\min}
%       (\tilde\Sigma\inv)_{\min}
%       \|\ones^\top U\|^2_2 \\
%       &\ge 
%       \frac{\|\ones^\top U\|^2_2}{\|\tS\|^2_\infty \|\tilde\Sigma\|_\infty},
%   \end{align*}
%   where we have used the property that each of the bracketed terms is an
%   eigen-decomposition, and $U^\top \ones$, lies in the subspace $U$.
%   We similarly have that
%   \begin{align*}
%       \ones^\top J \tS\inv J \ones \\
%       &=
%       (\ones^\top U) (U^\top \tS\inv U) (U^\top \ones) \\
%       &\le 
%       \|\tS\inv\|_{\infty}
%       \|\ones^\top U\|^2_2.
%   \end{align*}
% 
%   Finally, to bound $\|\ones^\top U\|_2$, we have:
%   \begin{align*}
%     \ones^\top O 
%       &= \sum_{\bh = \ones}^{\bh \prec k} \sum_{x = \ones}^{d} \Pr(x | \bh) e_\bh  \\
%       &= \ones \\
%     \ones^\top \tO 
%         &= \ones^\top (O_{\neg \neg d, k} - O_{\neg d, k}\ones^\top) \\
%         &= \sum_{\bh = \ones}^{\bh \prec k} \sum_{x = \ones}^{x \prec d} \Pr(x | \bh) e_\bh - (\sum_{x = \ones}^{x \prec d} \Pr(x | k)) \ones^\top  \\
%         &= \sum_{\bh = \ones}^{\bh \prec k} (1 - \Pr(d | \bh)) e_\bh - (1 - \Pr(d | k)) \ones^\top  \\
%         &= \sum_{\vh = \ones}^{\vh \prec k} (\Pr(d | k) - \Pr(d | \vh))e_\bh \\
%         &= \tO_{d,k} \ones^\top - \tO_{d,\neg k}^\top.
%   \end{align*}
%   Let $c = \|\tO_{d,k} \ones^\top - \tO_{d,\neg
%   k}^\top\|_2 \le \sqrt{k}$.
%   Let $\tO = U W V^\top$ be the SVD of $\tO$. We have,
%   \begin{align*}
%     \ones^\top U W V^\top 
%         &= \ones^\top \tO \\
%     \ones^\top U 
%       &= \ones^\top \tO V W\inv \\
%     \|\ones^\top U\|_2 
%         &\le \frac{\|\ones^\top \tO\|_2}{\sigma_{k}(\tO)} \\
%         &\le \frac{c}{\sigma_{k}(\tO)} \\
%     \|\ones^\top U\|_2 
%         &\ge \frac{c}{\sigma_{1}(\tO)}.
%   \end{align*}
% 
%   Thus,
%   \begin{align*}
%       \ones^\top J \tS\inv J \tilde\Sigma\inv J \tS\inv J \ones 
%       &\ge \frac
%           {c^2}
%           { \|\tS\|_{\infty} \|\tilde\Sigma\|_{\infty}
%           \sigma_{1}(\tO)} \\
%       \ones^\top J \tS\inv J \ones  
%       &\le \frac
%           {c^2 \|\tS\inv\|_{\infty}}
%           {\sigma_{k}(\tO)}.
%   \end{align*}
% 
%   Finally, we have the result,
%   \begin{align*}
%     e^\mom 
%     &\le \|\tS\inv\|_\infty  \|\tilde\Sigma\inv\|_\infty \\
%     &\quad - 
%         \frac{1}{\bar k} 
%     \frac{
%         s c^2 /(\|\tS\|^2_{\infty} \|\tilde\Sigma\|_{\infty}
%             \sigma_{1}(\tO))
%     }
%     {1 + (s c^2 \|\tS\inv\|_{\infty})/
%           \sigma_{k}(\tO)
%     }.
%   \end{align*}
% 
%   For the uniform distribution $M[x] = \frac{1}{\bar d + 1}$,
%   $\|\tS\|_\infty = \bbd$, $\|\tilde
%   \Sigma\|_\infty = \frac{\bbd}{(\bbd + 1)^2}$:
%   \begin{align*}
%     e^\mom 
%     &\le \frac{1}{\bbd} \frac{(\bbd + 1)^2}{\bbd} 
%     - \frac{1}{\bbk} \frac{
%     \bbd c^2/(\bbd^2 \frac{\bbd}{(\bbd +1)^2} \sigma_{k}(\tO))
%     }{
%     1 + c^2 \bbd \frac{1}{\bbd} / \sigma_{k}(\tO)
%     } \\
%     &= (1 + \frac{1}{\bbd})^2 - \frac{1}{\bbk} (1 + \frac{1}{\bbd})^2 \frac{c^2/\sigma_{1}(\tO)}
%       {1 + c^2 / \sigma_{k}(\tO)} \\
%     &= (1 + \frac{1}{\bbd})^2 (1 - \frac{1}{\bbk} \frac{c^2/\sigma_{1}(\tO)}{1 + c^2 / \sigma_{k}(\tO)}).
%   \end{align*}
% 
%   This shows that the pseudoinverse estimator is strictly less efficient
%   than the composite likelihood estimator for finite $k$ and $d$.
%   Furthermore, for a fixed $\bbk$, the pseudoinverse estimator is most
%   efficient for large $\bbd$, and in general it is more efficient for
%   larger $\bbk$.
% \end{proof}
% 

% \begin{align*}
%   \Tr(\Sigmamom - \Sigmaml) 
%   &= 
%       \Tr\left( 
%       \frac{\tOi \tD v v^\top \tD \tOit }
%         {1 - \ones^\top \tD \ones + v^\top \tD v } 
%       - \tOi \tD \ones \ones^\top \tD \tOit 
%       \right) \\
%   &= 
%       \frac{v^\top \tD \tOit \tOi \tD v}
%         {1 - \ones^\top \tD \ones + v^\top \tD v } 
%       - \ones^\top \tD \tOit  \tOi \tD \ones \\
% \end{align*}



% ARUN: This is wrong because pseudo-inverses don't work that way.
%
% The above expression is the inversion of an invertible matrix plus
% a rank-one component. The Sherman-Morrison formula allows us to write
% the inverse in closed form:
% \begin{align*}
%   (A + \alpha uv^\top)\inv &= A\inv - \frac{A\inv uv^\top A\inv}{\alpha\inv + v^\top A\inv u}.
% \end{align*}
% 
% This gives us,
% \begin{align}
%     \Sigmaml
%     &= (\tOt \tD\inv \tO)\inv
%       - \frac{
%       (\tOt \tD\inv \tO)\inv (\tOt \ones) (\tOt \ones)^\top (\tOt \tD\inv \tO)\inv }
%       {\td + (\tOt \ones)^\top (\tOt \tD\inv \tO)\inv (\tOt \ones)} \nonumber \\
%     &= \tOi \tD \tOit 
%       - \frac{\tOi \tD (\tOit \tOt \ones) (\tOit \tOt \ones)^\top  \tD \tOit }
%       {\td + (\tOit \tOt \ones)^\top \tD (\tOit \tOt \ones)} \nonumber \\
%     &= \tOi \tD \tOit 
%       - \frac{\tOi \tD v v^\top \tD \tOit }
%       {\td + v^\top \tD v} \nonumber \\
%     &= \tOi \tD \tOit 
%       - \frac{\tOi \tD v v^\top \tD \tOit }
%       {1 - \ones^\top \tD \ones + v^\top \tD v }, \label{eqn:asymp-var-ml}
% \end{align}
% where $v \eqdef (\tO \tOi)^\top \ones$, the projection of $\ones$ into the
% column space of $\tO$. 
% 
% If $k = d$, then by assumption $\tO$ is invertible and $v = \ones$. In this case,
% \begin{align*}
%     \Sigmaml
%     &= \tOi \tD \tOit 
%       - \frac{\tOi \tD \ones \ones^\top \tD \tOit }
%       {1 - \ones^\top \tD \ones + \ones^\top \tD \ones } \\
%     &= \tOi \tD \tOit 
%       - \tOi \tD \ones \ones^\top \tD \tOit.
% \end{align*}
% In other words, when $k = d$, the asymptotic variance of the
% pseudoinverse estimator and the composite likelihood estimator are
% equal.




