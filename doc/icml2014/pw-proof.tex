\subsection{\lemmaref{mom-pw-variance}}
\label{app:pw-proof}

In \sectionref{piecewise}, we compare the asymptotic variance
  $\Sigma^\ml_\sC$ of the composite likelihood estimator for a clique
  $\sC$, with that of the pseudo-inverse estimator, $\Sigma^\mom_\sC$. 
In this part, we will derive the asymptotic variances of these two
  estimators and compare their relative efficiencies.

Recall, that in \sectionref{piecewise} we reparameterized the
  constrained estimators $Z_\sC \in \Delta_{k^m-1}$ with $\tZ_\sC \in
  [0,1]^{k^m}$. 
Similarly, we reparameterized  $M_\sV \in \Delta_{d^m - 1}$,
  $\tM_\sV \eqdef (M_\sV)_{\neg \bd} \in [0,1]^{d^m}$ to compute
  the likelihood.
We used the vectorized forms of the marginal distribution $\tM_\sV \in
  \Re^{d^m}$, the parameters $Z_\sC \in \Re^{k^m}$, $\tZ_\sC \in
  \Re^{k^m-1}$ and the matrix form of the conditional moments, $\mOppAll
  \in \Re^{d^m \times k^m}$, to represent the marginal distribution as
\begin{align*}
  \tM_\sV &= \mOppTAll \tZ_\sC + \mOppAll_{\vk, \neg \bd},
\end{align*}
where $\mOppTAll \eqdef (\mOppAll_{\neg \vk, \neg \bd}
- \mOppAll_{\vk, \neg \bd}\ones^\top)$, $\mOppAll_{\neg \vk, \neg \bd} \in \Re^{d^m - 1 \times
k^m - 1}$ matrix containing the first $d^m-1$ rows and first $k^m-1$ columns of $\mOppAll$,
$\mOppAll_{\vk,\neg \bd} \in \Re^{d^m-1}$ is the last column, ignoring the last row.

\begin{proof}[Proof for \lemmaref{mom-pw-variance}]
  We proceed using the delta method. First, let us look at the
  asymptotic variance of the pseudo-inverse estimator. With the
  reparameterization $\tZ_\sC$, we get,
  \begin{align*}
    \tM_\sV &= \mOppTAll \tZ_\sC + \mOppAll_{\vk,\neg \bd} \\
    \tZ_\sC &= \mOppTAlli (\tM_\sV - \mOppAll_{\vk, \neg \bd}).
  \end{align*}
  As this is just a linear transformation on $\tM_\sV$, we get,
  \begin{align*}
      \Sigmamom_{\sC} &= \mOppTAlli \widetilde\Sigma_\sV \mOppTAllit,
  \end{align*}
  where $\widetilde\Sigma_\sV$ is the variance of $\tM_\sV$; $\widetilde\Sigma_\sV = \widetilde{D}_\sV
  (I - \widetilde{D}_\sV)$, where we have used $\widetilde D_\sV \eqdef \diag(\tM_\sV)$ as
  before.

  Now, let us look at the variance of the piecewise estimator.  The
  objective, \equationref{piecewise-obj} can be written in terms of
  $\tZ_\sC$ and $\mOppTAll$ as,
  \begin{align*}
    \sL_\ml &= \frac{1}{|\sD|} \sum_{\vx \in \sD} \ell(\vx) \\
    \ell(\vx) &= \BI[\vx \neq \bd] \log(\tM_\sV[\vx]) + \BI[\vx = \bd] \log(1 - \ones^\top \tM_\sV) ) \\
              &= \BI[\vx \neq \bd] \log(\mOppTAll[\vx] \tZ_\sC + \mOppAll_{\vk, \neg \bd}[\vx]) \\
              &+ \BI[\vx = \bd] \log(1 - \ones^\top (\mOppTAll \tZ_\sC + \mOppAll_{\vk, \neg \bd})).
  \end{align*}

Using the delta-method \cite{vaart98asymptotic} we have that the asymptotic distribution of 
  $\widehat{{\tZ}}_\sC = \argmax_{\tZ_\sC \in [0,1]^{k^m-1}} \E[\ell(\vx)]$ is,
  \begin{align*}
    \sqrt{n}(\widehat{{\tZ}}_{\sC} - {\tZ}_{\sC}) 
      &\convind \sN( 0, \E[\grad^2 \ell^{-1}] \Var[\grad \ell] \E[\grad^2 \ell^{-1}]).
  \end{align*}

Taking the first derivative,
\begin{align}
  \grad_{\tZ_\sC} \ell(\vx)
  &= 
  \frac{\BI[\vx \neq \bd] \mOppTAllt[\vx] - \BI[\vx = \bd] \mOppTAllt \ones}{\mOppAll[\vx] \mH_\sC} 
  \nonumber \\ 
  &= 
  \begin{bmatrix}
    \mOppTAll \\
    -\ones^\top \mOppTAll
  \end{bmatrix}^\top
  D_{\sV}^{-1} e_\vx, \label{eqn:lhood-grad}
\end{align}
where $e_\vx$ is a one-hot vector and $M_\sV$ is the marginal
distribution of the observed data at the true parameters.

Taking the second derivative,
\begin{align}
  \grad^2_{\tilde \mH_\sC} \ell(\vx)
  &= - \frac{\BI[\vx \neq \bd] \mOppTAll[\vx] \mOppTAllt[\vx] + \BI[\vx = \bd] \mOppTAllt[\bd] \ones \ones^\top \mOppTAll[\bd]}{(\mOppAll[\vx] \mH_\sC)^2} \nonumber \\
  &= - 
  \begin{bmatrix}
    \mOppTAll \\
    \ones^\top \mOppTAll
  \end{bmatrix}^\top
    D_{\sV}^{-1} e_\vx e_\vx^\top D_{\sV}^{-1} 
  \begin{bmatrix}
    \mOppTAll \\
    \ones^\top \mOppTAll
  \end{bmatrix}. \label{eqn:lhood-hess}%
\end{align}

From \equationref{lhood-grad} and \equationref{lhood-hess}, we get
  \begin{align*}
    \Var [\grad \ell(\vx)] &= -
        \begin{bmatrix}
          \mOppTAll \\
          \ones^\top \mOppTAll
        \end{bmatrix}^\top
          D_{\sV}^{-1} \diag(\Sigma_\sV) D_{\sV}^{-1} 
        \begin{bmatrix}
          \mOppTAll \\
          \ones^\top \mOppTAll
        \end{bmatrix} \\
        \E[\grad^2 \ell(\vx)] 
        &= \hphantom{-}
        \begin{bmatrix}
          \mOppTAll \\
          \ones^\top \mOppTAll
        \end{bmatrix}^\top
          D_{\sV}^{-1} \diag(\Sigma_\sV) D_{\sV}^{-1} 
        \begin{bmatrix}
          \mOppTAll \\
          \ones^\top \mOppTAll
        \end{bmatrix}.
  \end{align*}
As expected, $\E[\grad^2 \ell(\vx)] = -\Var [\grad \ell(\vx)]$ because
$\widehat{\tZ_\sC}$ is a maximum likelihood estimator. 
Noting that $\diag(\Sigma_\sV) = D_\sV (I - D_\sV)$, the inner
  expression simplifies to $D_\sV^{-1} (I - D_\sV)$. We get,
\begin{align*}
  \Var [\grad \ell(\vx)] &=
      \begin{bmatrix}
        \mOppTAll \\
        \ones^\top \mOppTAll
      \end{bmatrix}^\top
        D_{\sV}^{-1} (I - D_\sV) 
      \begin{bmatrix}
        \mOppTAll \\
        \ones^\top \mOppTAll
      \end{bmatrix} \\
   &=
    \mOppTAllt \tD_{\sV}^{-1} (I - \tD_\sV) \mOppTAll + \mOppTAllt \frac{\ones^\top \tM_\sV}{1 - \ones^\top \tM_\sV} \ones\ones^\top \mOppTAll \\
   &= {\Sigmamom}\inv + \mOppTAllt \frac{\ones^\top \tM_\sV}{1 - \ones^\top \tM_\sV} \ones\ones^\top \mOppTAll.
\end{align*}

Finally, noting that $\diag(\Sigma_\sV) = D_\sV (I - D_\sV)$, we get,
\begin{align*}
    \Sigmaml_{\sC} 
      &= \E[\grad^2 \ell(\vx)]^{-1} \Var [\grad \ell(\vx)] \E[\grad^2 \ell(\vx)]^{-1} \\
      &= \Var[\grad \ell(\vx)]^{-1} \\
      &= 
        \mOppTAlli (\tD_{\sV}^{-1} (I - \tD_\sV) + \frac{\ones^\top \tM_\sV}{1 - \ones^\top \tM_\sV} \ones\ones^\top)\inv \mOppTAllit \\
      &= ({\Sigmamom}\inv + \mOppTAllt \frac{\ones^\top \tM_\sV}{1 - \ones^\top \tM_\sV} \ones\ones^\top \mOppTAll)\inv.
\end{align*}
Note that given our assumptions, $\ones \succ M_\sV \succ \zeros$. 
Thus the asymptotic variance is finite, and our estimator is consistent
  as well. 
Note also, that the expression for $\Sigmaml$ already implies that
  $Z^\ml_\sC$ is a strictly more efficient estimator than $Z^\mom_\sC$.

\end{proof}

