\subsection{\lemmaref{mom-pw-variance}}
\label{app:pw-proof}

In \sectionref{piecewise}, we compare the asymptotic variance
  $\Sigma^\ml_\sC$ of the composite likelihood estimator for a clique
  $\sC$, with that of the pseudo-inverse estimator, $\Sigma^\mom_\sC$. 
In this part, we will derive the asymptotic variances of these two
  estimators and compare their relative efficiencies.

Recall, that in \sectionref{piecewise} we reparameterized the
  constrained estimators $Z_\sC \in \Delta_{k^m-1}$ with $\tZ_\sC \in
  [0,1]^{k^m}$. 
Similarly, we reparameterized $M_\sV \in \Delta_{d^m - 1}$ with
  $\tM_\sV \eqdef (M_\sV)_{\neg \bd} \in [0,1]^{d^m}$ to compute
  the likelihood.
We used the vectorized forms of the marginal distribution $\tM_\sV \in
  \Re^{d^m}$, the parameters $Z_\sC \in \Re^{k^m}$, $\tZ_\sC \in
  \Re^{k^m-1}$ and the matrix form of the conditional moments, $\mOppAll
  \in \Re^{d^m \times k^m}$, to represent the marginal distribution as
\begin{align*}
  \tM_\sV &= \mOppTAll \tZ_\sC + \mOppAll_{\neg \bd, \vk},
\end{align*}
where $\mOppTAll \eqdef (\mOppAll_{\neg \bd, \neg \vk}
- \mOppAll_{\neg \bd, \vk}\ones^\top)$, $\mOppAll_{\neg \neg \bd, \vk} \in \Re^{d^m - 1 \times
k^m - 1}$ matrix containing the first $d^m-1$ rows and first $k^m-1$ columns of $\mOppAll$,
$\mOppAll_{\neg \bd, \vk} \in \Re^{d^m-1}$ is the last column, ignoring the last row.

\begin{proof}[Proof for \lemmaref{mom-pw-variance}]
  We proceed using the delta method. First, let us look at the
  asymptotic variance of the pseudo-inverse estimator. With the
  reparameterization $\tZ_\sC$, we get,
  \begin{align*}
    \tM_\sV &= \mOppTAll \tZ_\sC + \mOppAll_{\neg \bd, \vk} \\
    \tZ_\sC &= \mOppTAlli (\tM_\sV - \mOppAll_{\neg \bd, \vk}).
  \end{align*}
  As this is just a linear transformation on $\tM_\sV$, we get,
  \begin{align*}
      \Sigmamom_{\sC} &= \mOppTAlli \widetilde\Sigma_\sV \mOppTAllit,
  \end{align*}
  where $\widetilde\Sigma_\sV$ is the variance of $\tM_\sV$: $\widetilde\Sigma_\sV = \widetilde{D}_\sV
  (I - \widetilde{D}_\sV)$, where we have used $\widetilde D_\sV \eqdef \diag(\tM_\sV)$ as
  before.

  Now, let us look at the variance of the piecewise estimator.  The
  objective, \equationref{piecewise-obj} can be written in terms of
  $\tZ_\sC$ and $\mOppTAll$ as,
  \begin{align*}
    \sL_\ml &= \frac{1}{|\sD|} \sum_{\vx \in \sD} \ell(\vx) \\
    \ell(\vx) 
              &= \log(M_\sV[\vx]) \\
              &= \BI[\vx \neq \bd] \log(\tM_\sV[\vx]) + \BI[\vx = \bd] \log(1 - \ones^\top \tM_\sV) ) \\
              &= \BI[\vx \neq \bd] \log(\mOppTAll[\vx] \tZ_\sC + \mOppAll_{\neg \bd, \vk}[\vx]) \\
              &+ \BI[\vx = \bd] \log(1 - \ones^\top (\mOppTAll \tZ_\sC + \mOppAll_{\neg \bd, \vk})).
  \end{align*}

Using the delta-method \cite{vaart98asymptotic} we have that the asymptotic distribution of 
  $\widehat{{\tZ}}_\sC = \argmax_{\tZ_\sC \in [0,1]^{k^m-1}} \E[\ell(\vx)]$ is,
  \begin{align*}
    \sqrt{n}(\widehat{{\tZ}}_{\sC} - {\tZ}_{\sC}) 
      &\convind \sN( 0, \E[\grad^2 \ell^{-1}] \Var[\grad \ell] \E[\grad^2 \ell^{-1}]).
  \end{align*}

Taking the first derivative,
\begin{align}
  \grad_{\tZ_\sC} \ell(\vx)
  &= 
  \frac{\BI[\vx \neq \bd] \mOppTAllt[\vx] - \BI[\vx = \bd] \mOppTAllt \ones}{\mOppAll[\vx] \mH_\sC} 
  \nonumber \\ 
  &= 
  \begin{bmatrix}
    \mOppTAll \\
    -\ones^\top \mOppTAll
  \end{bmatrix}^\top
  D_{\sV}^{-1} e_\vx, \label{eqn:lhood-grad}
\end{align}
where $e_\vx$ is a one-hot vector and $M_\sV$ is the marginal
distribution of the observed data at the true parameters.

Taking the second derivative,
\begin{align}
  \grad^2_{\tilde \mH_\sC} \ell(\vx)
  &= -\frac{\BI[\vx \neq \bd] \mOppTAll[\vx] \mOppTAllt[\vx] + \BI[\vx = \bd] \mOppTAllt \ones \ones^\top \mOppTAll}{(\mOppAll[\vx] \mH_\sC)^2} \nonumber \\
  &= - 
  \begin{bmatrix}
    \mOppTAll \\
    \ones^\top \mOppTAll
  \end{bmatrix}^\top
    D_{\sV}^{-1} e_\vx e_\vx^\top D_{\sV}^{-1} 
  \begin{bmatrix}
    \mOppTAll \\
    \ones^\top \mOppTAll
  \end{bmatrix}. \label{eqn:lhood-hess}%
\end{align}

From \equationref{lhood-grad} and \equationref{lhood-hess}, we get
  \begin{align*}
    \Var [\grad \ell(\vx)] &= -
        \begin{bmatrix}
          \mOppTAll \\
          \ones^\top \mOppTAll
        \end{bmatrix}^\top
          D_{\sV}^{-1} \diag(\Sigma_\sV) D_{\sV}^{-1} 
        \begin{bmatrix}
          \mOppTAll \\
          \ones^\top \mOppTAll
        \end{bmatrix} \\
        \E[\grad^2 \ell(\vx)] 
        &= \hphantom{-}
        \begin{bmatrix}
          \mOppTAll \\
          \ones^\top \mOppTAll
        \end{bmatrix}^\top
          D_{\sV}^{-1} \diag(\Sigma_\sV) D_{\sV}^{-1} 
        \begin{bmatrix}
          \mOppTAll \\
          \ones^\top \mOppTAll
        \end{bmatrix}.
  \end{align*}
As expected, $\E[\grad^2 \ell(\vx)] = -\Var [\grad \ell(\vx)]$ because
$\widehat{\tZ_\sC}$ is a maximum likelihood estimator. 
Noting that $\diag(\Sigma_\sV) = D_\sV (I - D_\sV)$, 
\begin{align*}
  \Var [\grad \ell(\vx)] &=
      - \begin{bmatrix}
        \mOppTAll \\
        \ones^\top \mOppTAll
      \end{bmatrix}^\top
        D_{\sV}^{-1} (I - D_\sV) 
      \begin{bmatrix}
        \mOppTAll \\
        \ones^\top \mOppTAll
      \end{bmatrix} \\
%   &=
%    - \mOppTAllt \tD_{\sV}^{-1} (I - \tD_\sV) \mOppTAll + \mOppTAllt \ones \frac{\ones^\top \tM_\sV}{1 - \ones^\top \tM_\sV} \ones^\top \mOppTAll \\
   &=
    - \mOppTAllt \tS_\sV \mOppTAll + s_\sV \mOppTAllt \ones \ones^\top \mOppTAll,
\end{align*}
where $\tS_\sV$ and $s_\sV$ are defined as follows,
\begin{align*}
  \begin{bmatrix}
          \tS_\sV & \zeros \\
          \zeros^\top & s_\sV
  \end{bmatrix}
  &\eqdef D_\sV\inv (I - D_\sV).
\end{align*}

Finally, we can derive the asymptotic variance of $\Sigmaml_\sC$,
\begin{align*}
    \Sigmaml_{\sC} 
      &= \E[\grad^2 \ell(\vx)]^{-1} \Var [\grad \ell(\vx)] \E[\grad^2 \ell(\vx)]^{-1} \\
      &= -\Var[\grad \ell(\vx)]^{-1} \\
      &= (\mOppTAllt \tS_\sV \mOppTAll + s_\sV (\mOppTAllt \ones)(\mOppTAllt \ones)^\top)\inv,
\end{align*}
The above expression is the inversion of an invertible matrix plus a rank-one component. The Sherman-Morrison formula allows us to write the inverse in closed form:
\begin{align*}
  (A + uv^\top)\inv &= A\inv - \frac{A\inv uv^\top A\inv}{1 + v^\top A\inv u}.
\end{align*}

This gives us,
\begin{align}
    \Sigmaml_{\sC} 
    &= \mOppTAlli \tS_\sV\inv \mOppTAllit 
      - \frac{s_\sV \mOppTAlli \tS_\sV\inv \mOppTAllit \mOppTAllt \ones \ones^\top \mOppTAll \mOppTAlli \tS_\sV\inv \mOppTAllit }
      {1 + s_\sV \ones^\top \mOppTAll \mOppTAlli \tS_\sV\inv \mOppTAllit \mOppTAllt \ones} \nonumber \\
    &= \mOppTAlli \tS_\sV\inv \mOppTAllit 
      - \frac{s_\sV \mOppTAlli \tS_\sV\inv J \ones \ones^\top J \tS_\sV\inv \mOppTAllit }
      {1 + s_\sV \ones^\top J \tS_\sV\inv J \ones}, \label{eqn:asymp-var-ml}
\end{align}
where $J \eqdef \mOppTAll \mOppTAlli = U U^\top$, where $U$ is a $d^m
- 1 \times k^m$ matrix with orthonormal columns.

Given our assumptions, $\ones \succ M_\sV \succ \zeros$, and
  $\ones^\top M_\sV = 1$. Consequently, $\tS_\sV$ is invertible and 
Thus the asymptotic variance is finite and our estimator is consistent
  as well. 
\end{proof}

With concrete expressions for $\Sigmamom_\sC$ and $\Sigmaml_\sC$, we can quantitatively evaluate their asymptotic efficiency,
\begin{proof}[Proof for \corollaryref{efficiency}]
  Let $\bar k = k^m -1$ and $\bar d = d^m -1$. Consider their relative
  efficiencies, 
  \begin{align*}
    e^\mom 
        &\eqdef \frac{1}{\bar k} \Tr(\Sigmaml\Sigmamomi ) \\
        &= \frac{1}{\bar k} \Tr( \mOppTAlli \tS_\sV\inv \mOppTAllit \mOppTAllt \tilde\Sigma_\sV\inv \mOppTAll ) \\
      &\quad - \frac{1}{\bar k} \Tr( \frac{s_\sV \mOppTAlli \tS_\sV\inv J \ones \ones^\top J \tS_\sV\inv \mOppTAllit \mOppTAllt  \tilde\Sigma_\sV\inv \mOppTAll }
      {1 + s_\sV \ones^\top J \tS_\sV\inv J \ones} ) \\
        &= \frac{1}{\bar k} \Tr( J \tS_\sV\inv J \tilde\Sigma_\sV\inv ) - \frac{1}{\bar k} \frac{s_\sV \ones^\top J \tS_\sV\inv J \tilde\Sigma_\sV\inv J \tS_\sV\inv J \ones }
      {1 + s_\sV \ones^\top J \tS_\sV\inv J \ones},
  \end{align*}
  where $J \eqdef \mOppTAll \mOppTAlli = U U^\top$ as before. 
  
  Note that $\Tr(J) = \rank(J) = {\bar k}$.
  Next we use H\"{o}lder's inequality for the trace: $\Tr(D A) \le
  \|D\|_\infty \Tr(A)$ if $D$ is diagonal. Thus,
  \begin{align*}
      \Tr( J \tS_\sV\inv J \tilde\Sigma_\sV\inv ) 
        &= \Tr( \tS_\sV\inv J \tilde\Sigma_\sV\inv J ) \\
        &\le \|\tS_\sV\inv\|_\infty \Tr( J \tilde\Sigma_\sV\inv J ) \\
        &\le \|\tS_\sV\inv\|_\infty \Tr( \tilde\Sigma_\sV\inv J ) \\
        &\le \|\tS_\sV\inv\|_\infty \|\tilde\Sigma_\sV\inv\|_\infty \Tr( J ) \\
        &= {\bar k} \|\tS_\sV\inv\|_\infty \|\tilde\Sigma_\sV\inv\|_\infty.
  \end{align*}

  We also have that,
  \begin{align*}
    \ones^\top \mOppAll 
      &= \sum_{\bh = \ones}^{\bh \prec \bk} \sum_{\vx = \ones}^{\bd} \Pr(\vx | \bh) e_\bh  \\
      &= \ones \\
    \ones^\top \mOppTAll 
        &= \ones^\top (\mOppAll_{\neg \neg \bd, \vk} - \mOppAll_{\neg \bd, \vk}\ones^\top) \\
        &= \sum_{\bh = \ones}^{\bh \prec \bk} \sum_{\vx = \ones}^{\vx \prec \bd} \Pr(\vx | \bh) e_\bh - (\sum_{\vx = \ones}^{\vx \prec \bd} \Pr(\vx | \vk)) \ones  \\
        &= \sum_{\bh = \ones}^{\bh \prec \bk} (1 - \Pr(\bd | \bh)) e_\bh - (1 - \Pr(\bd | \bk)) \ones  \\
        &= \mOppTAll_{\bd,\bk} \ones^\top - \mOppTAll_{\bd,\neg \bk}^\top.
  \end{align*}
  This shows that $\ones$ is not in the null space of $\mOppTAll$. 
  Consequently, we have that 
  \begin{align*}
    \underbrace{(\mOppTAll_{\bd,\bk} \ones^\top \ones - \mOppTAll_{\bd,\neg \bk}^\top \ones)}_{C} \ones^\top &\succ \ones^\top \mOppTAll \mOppTAlli &\succ \ones^\top,
  \end{align*}
  for some constant $C$.

  Thus,
  \begin{align*}
      \ones^\top J \tS_\sV\inv J \tilde\Sigma_\sV\inv J \tS_\sV\inv J \ones 
      &\ge C^3 \sum_{\vx=\ones}^{\vx \prec \bd} \frac{M_\sV[\vx]}{(1 - M_\sV[\vx])^3} \\
      \ones^\top J \tS_\sV\inv J \ones 
      &\le \sum_{\vx=\ones}^{\vx \prec \bd} \frac{M_\sV[\vx]}{1 - M_\sV[\vx]}.
  \end{align*}

  Finally, we have the result,
  \begin{align*}
    e^\mom 
    &\le \|\tS_\sV\inv\|_\infty  \|\tilde\Sigma_\sV\inv\|_\infty \\
    &\quad - \frac{1}{\bar k} \frac{1 - M_\sV[\bd]}{M_\sV[\bd]} \frac{C^3 \sum_{\vx=\ones}^{\vx \prec \bd} \frac{M_\sV[\vx]}{(1 - M_\sV[\vx])^3}}{1 + \frac{1 - M_\sV[\bd]}{M_\sV[\bd]} {\bar k} \sum_{\vx=\ones}^{\vx \prec \bd} \frac{M_\sV[\vx]}{1 - M_\sV[\vx]}}.
  \end{align*}

  For the uniform distribution $M_\sV[x] = \frac{1}{\bar d + 1}$,
  $\|\tS_\sV\inv\|_\infty = \frac{1}{\bar d}$, $\|\tilde
  \Sigma_\sV\inv\|_\infty = \frac{(\bar d + 1)^2}{\bar d}$:
  \begin{align*}
    e^\mom 
    &\ge \frac{1}{\bar d + 1} \frac{(\bar d + 1)^2}{\bar d} 
    - \frac{1}{\bar k} \frac{{\bar d} C^3 \frac{(\bar d + 1)^2}{(\bar d)^3} {\bar d}}
    {1 + {\bar d} (\frac{1}{\bar d}) {\bar d}} \\
    &= 1 + \frac{1}{\bar d} - \frac{1}{\bar k} C^3 \frac{(\bar d+1)^2}{(1+ \bar d)(\bar d)} \\
    &= 1 + \frac{1}{\bar d} - C^3 \frac{1+\bar d}{\bar k} \\
    &= (1 + \frac{1}{\bar d}) (1 - \frac{C^3}{\bar d}{\bar k}).
  \end{align*}
   
\end{proof}

