\subsection{\lemmaref{mom-pw-variance}}
\label{app:pw-proof}

In \sectionref{piecewise}, we compared the asymptotic variance
  $\Sigma^\ml_\sC$ of the composite likelihood estimator for a clique
  $\sC$, with that of the pseudoinverse estimator, $\Sigma^\mom_\sC$. 
  Now we will derive these asymptotic variances in detail.
%In this part, we will derive the asymptotic variances of these two
  %estimators and compare their relative efficiencies.

Recall, that in \sectionref{piecewise} we simplified notation by taking
  $m=1$ and flattening the moments $M_\sV$ and hidden marginals $Z_\sC$
  into vectors $\mu \in \Re^d$ and $z \in \Re^k$ and similarly
  flattening the tensor $O$ into a matrix $O \in \Re^{d\times
  k}$.
The hidden marginals $z$ and observed marginals $\mu$ are related via
  $\mu = O z$.

Note that $\mu$ and $z$ are multinomial distributions and are hence
constrained to lie on simplexes. This makes asymptotic analysis tricky
because $\mu, z$ lie on subspaces with Lebesgue measure zero in $\Re^d$
and $\Re^k$ respectively.
To handle this constraint, we re-parameterize the problem in terms of
  $\tm \in \Re^{d-1}$ and $\tz \in \Re^{k-1}$, 
which are related to the original representation as follows:
\begin{align*}
  \mu &= 
    \begin{bmatrix}
      \tm \\
      1 - \ones^\top\tm
    \end{bmatrix} 
  &
  z &= 
    \begin{bmatrix}
      \tz \\
      1 - \ones^\top\tz
    \end{bmatrix}.
\end{align*}

Now, $\tm$ and $\tz$ are related as,
\begin{align*}
  \mu &= O z \\
  \begin{bmatrix}
    \tm \\
    1 - \ones^\top\tm
  \end{bmatrix} 
  &=
    \begin{bmatrix}
      O_{\neg d,\neg k} & O_{\neg d, k} \\ 
      O_{d,\neg k} & O_{d, k} \\ 
    \end{bmatrix}
    \begin{bmatrix}
      \tz \\
      1 - \ones^\top\tz
    \end{bmatrix} \\
  \tm &= O_{\neg d,\neg k} \tz + O_{\neg d, k} - O_{\neg d, k} \ones^\top \tz \\
      &= \underbrace{(O_{\neg d,\neg k} - O_{\neg d, k} \ones^\top )}_{\eqdef \tO} \tz +  O_{\neg d,k}.
\end{align*}

With this reparameterization, we are ready to derive the asymptotic
variances of the two estimators.

\begin{proof}[Proof for \lemmaref{mom-pw-variance}]
  First, let us look at the
  asymptotic variance of the pseudoinverse estimator. With the
  reparameterization $\tz$, we get,
  \begin{align*}
    \tm &= \tO \tz +  O_{\neg d,k} \\
    \tz &= \tOi (\tm - O_{\neg d,k}).
  \end{align*}
  Note that $\mu$ is a multinomial distribution, hence the variance of
    $\hat\tm$ is $(\tD - \tm \tm^\top)$ where $\tD \eqdef \diag(\tm)$.
  Since $\tz$ is just a linear transformation of $\tm$,
  the asymptotic variance of $\hat{z}^\mom = \tOi \hat\tm$ is:
  \begin{align*}
      \Sigmamom &= \tOi \Var(\hat\tm) \tOit \\
      &= \tOi (\tD - \tm \tm^\top) \tOit \\
      &= \tOi \tD \tOit - \tOi \tD \ones \ones^\top \tD \tOit,
  \end{align*}
  

  Now, let us look at the variance of the piecewise estimator.  Using
  the delta-method \cite{vaart98asymptotic} we have that the asymptotic
  variance of 
  %$\widehat{z}^\ml = \argmin_{\tz \in [0,1]^{k-1}} \hat\E[-\ell(x)]$ is,
  % Definitely not optimizing over the whole set
  $\hat{z}^\ml = \argmax_{\tz} \hat\E[\ell(x ; \tz)]$ is,
  \begin{align*}
    \Sigmaml &= \E[\grad^2 \ell(x ; \tz^*)]^{-1} \Var[\grad \ell(x ; \tz^*)] \E[\grad^2 \ell(x ; \tz^*)]^{-1},
  \end{align*}
  where $\ell(x;\tz)$ is the log-likelihood of the observations $x$
  given parameters $\tz$. We can write $\ell(x;\tz)$ in terms of $\tz$
  and $\tO$ as,
  \begin{align*}
    \ell(x;\tz) 
              &= \log( \mu[x] ) \\
              &= \log( \BI[x \neq d] \tm[x] + \BI[x = d] (1 - \ones^\top \tm ) )\\
              &= \log( \BI[x \neq d] (\tO[x] \tz + O_{\neg d, k}[x]) \\
              &\quad + \BI[x = d] (1 - \ones^\top (\tO \tz + O_{\neg d, k})) ).
  \end{align*}

Taking the first derivative,
\begin{align}
  \grad \ell(x; \tz)
  &= 
  \frac{\BI[x \neq d] \tO[x]^\top - \BI[x = d] \tO^\top \ones }{\mu[x]} 
  \nonumber \\ 
  &= 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}^\top
  D^{-1} e_x, \label{eqn:lhood-grad}
\end{align}
where $e_x$ is an indicator vector on $x$ and $D \eqdef \diag(\mu)$.

It is easily verified that the expectation of the first derivative is indeed $\zeros$.
%Note that the expectation of the first derivative is indeed $\zeros$,
\begin{align*}
  \E[\grad \ell(x; \tz)]
  &= 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}^\top
  D^{-1} \E[e_x] \\
  &= 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}^\top
  D^{-1} \mu \\
  &= 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}^\top
  \ones \\
  &= \tOt\ones - \tOt\ones \\
  &= \zeros.
\end{align*}

Taking the second derivative,
\begin{align}
  \grad^2 \ell(x; \tz)
  &= -\frac{\BI[x \neq d] \tO[x]^\top \tO[x] + \BI[x = d] \tOt \ones \ones^\top \tO}{\mu[x]^2} \nonumber \\
  &= - 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}^\top
    D^{-1} e_x e_x^\top D^{-1} 
  \begin{bmatrix}
    \tO \\
    -\ones^\top \tO
  \end{bmatrix}. \label{eqn:lhood-hess}%
\end{align}

From \equationref{lhood-grad} and \equationref{lhood-hess}, we get
  \begin{align*}
    \E[\grad^2 \ell(x; \tz^*)] 
    &= -
        \begin{bmatrix}
          \tO \\
          -\ones^\top \tO
        \end{bmatrix}^\top
          D^{-1} \E[e_x e_x^\top] D^{-1} 
        \begin{bmatrix}
          \tO \\
          -\ones^\top \tO
        \end{bmatrix} \\
    \Var [\grad \ell(x; \tz^*)] 
    &= \hphantom{-}
        \begin{bmatrix}
          \tO \\
          -\ones^\top \tO
        \end{bmatrix}^\top
          D^{-1} \E[e_x e_x^\top] D^{-1} 
        \begin{bmatrix}
          \tO \\
          -\ones^\top \tO
        \end{bmatrix} \\
    &= \hphantom{-}
        \begin{bmatrix}
          \tO \\
          -\ones^\top \tO
        \end{bmatrix}^\top
          D^{-1} D D^{-1} 
        \begin{bmatrix}
          \tO \\
          -\ones^\top \tO
        \end{bmatrix} \\
    &= \hphantom{-}
        \begin{bmatrix}
          \tO \\
          - \ones^\top \tO
        \end{bmatrix}^\top
        \begin{bmatrix}
          \tD\inv & \zeros \\
          \zeros^\top & \td\inv
        \end{bmatrix}
        \begin{bmatrix}
          \tO \\
          - \ones^\top \tO
        \end{bmatrix} \\
        &= \hphantom{-} \tOt \tD\inv \tO + \td\inv \tOt \ones \ones^\top \tO,
  \end{align*}
where $\td = 1 - \ones^\top \tm$.
As expected, $\E[\grad^2 \ell(x)] = -\Var [\grad \ell(x)]$ because
$\hat{z}$ is a maximum likelihood estimator. 

Finally, we can derive the asymptotic variance of $\Sigmaml$,
\begin{align*}
    \Sigmaml &= \E[\grad^2 \ell(x ; \tz^*)]^{-1} \Var[\grad \ell(x ; \tz^*)] \E[\grad^2 \ell(x ; \tz^*)]^{-1} \\
      &= \Var[\grad \ell(x;\tz^*)]^{-1} \\
      &= (\tOt \tD\inv \tO + \td\inv (\tOt \ones)(\tOt \ones)^\top)\inv,
\end{align*}
The above expression is the inversion of an invertible matrix plus
a rank-one component. The Sherman-Morrison formula allows us to write
the inverse in closed form:
\begin{align*}
  (A + \alpha uv^\top)\inv &= A\inv - \frac{A\inv uv^\top A\inv}{\alpha\inv + v^\top A\inv u}.
\end{align*}

This gives us,
\begin{align}
    \Sigmaml
    &= (\tOt \tD\inv \tO)\inv
      - \frac{
      (\tOt \tD\inv \tO)\inv (\tOt \ones) (\tOt \ones)^\top (\tOt \tD\inv \tO)\inv }
      {\td + (\tOt \ones)^\top (\tOt \tD\inv \tO)\inv (\tOt \ones)} \nonumber \\
    &= \tOi \tD \tOit 
      - \frac{\tOi \tD (\tOit \tOt \ones) (\tOit \tOt \ones)^\top  \tD \tOit }
      {\td + (\tOit \tOt \ones)^\top \tD (\tOit \tOt \ones)} \nonumber \\
    &= \tOi \tD \tOit 
      - \frac{\tOi \tD v v^\top \tD \tOit }
      {\td + v^\top \tD v} \nonumber \\
    &= \tOi \tD \tOit 
      - \frac{\tOi \tD v v^\top \tD \tOit }
      {1 - \ones^\top \tD \ones + v^\top \tD v }, \label{eqn:asymp-var-ml}
\end{align}
where $v \eqdef (\tO \tOi)^\top \ones$, the projection of $\ones$ into the
column space of $\tO$. 

If $k = d$, then by assumption $\tO$ is invertible and $v = \ones$. In this case,
\begin{align*}
    \Sigmaml
    &= \tOi \tD \tOit 
      - \frac{\tOi \tD \ones \ones^\top \tD \tOit }
      {1 - \ones^\top \tD \ones + \ones^\top \tD \ones } \\
    &= \tOi \tD \tOit 
      - \tOi \tD \ones \ones^\top \tD \tOit.
\end{align*}
In other words, when $k = d$, the asymptotic variance of the
pseudoinverse estimator and the composite likelihood estimator are
equal.

Given our assumptions, $\ones \succ \mu \succ \zeros$. Consequently,
$\tD$ is invertible and the asymptotic variance is finite and our
estimator is consistent as well. 
\end{proof}

% With concrete expressions for $\Sigmamom$ and $\Sigmaml$, we can quantitatively evaluate their asymptotic efficiency,
% \begin{proof}[Proof for Corollary X] %\corollaryref{efficiency}]
%   Let $\bar k = k -1$ and $\bar d = d -1$. 
%   We have that
%   \begin{align*}
%     \Sigmamom 
%       &= \tOi \tD \tOit - \tOi \tD \ones \ones^\top \tD \tOit, \\
%     \Sigmaml &= 
%          \tOi \tD \tOit - \frac{\tOi \tD v v^\top \tD \tOit }{1 + v\top \tD v - \ones\top \tD \ones}.
%   \end{align*}
% 
%   We use the following identity,
%   \begin{align*}
%     \Tr( A_1 A_2\inv ) 
%       &= \Tr( (A - x x^\top) (A - yy^\top)\inv ) \\
%       &= \Tr( (A - x x^\top) (A\inv + \frac{(A\inv y)(A\inv y)^\top }{1 - y^\top A\inv y}) ) \\
%       &= \Tr( I - xx^\top A\inv + \frac{ y (A\inv y)^\top - x x^\top (A\inv y)(A\inv y)^\top}{1 - y^\top A\inv y} ) \\
%       &= k - x^\top A\inv x + \frac{ y^\top A\inv y - (x^\top A\inv y)^2 }{1 - y^\top A\inv y}.
%   \end{align*}
% 
%   Let us now consider their relative efficiencies, 
%   \begin{align*}
%     e^\mom 
%         &\eqdef \frac{1}{\bbk} \Tr(\Sigmaml \Sigmamomi ) \\
%         \Tr( \tOi \tS\inv \tOit \tOt \tilde\Sigma\inv \tO ) \\
%       &\quad - \frac{1}{\bar k} \frac{s \Tr( \tOi \tS\inv J \ones \ones^\top J \tS\inv \tOit \tOt  \tilde\Sigma\inv \tO  )}
%       {1 + s \ones^\top J \tS\inv J \ones} \\
%         &= \frac{1}{\bar k} \Tr( J \tS\inv J \tilde\Sigma\inv ) - \frac{1}{\bar k} \frac{s \ones^\top J \tS\inv J \tilde\Sigma\inv J \tS\inv J \ones }
%       {1 + s \ones^\top J \tS\inv J \ones},
%   \end{align*}
%   where $J \eqdef \tO \tOi = U U^\top$ as before. 
%   
%   Note that $\Tr(J) = \rank(J) = {\bar k}$.
%   Next we use H\"{o}lder's inequality for the trace: $\Tr(D A) \le
%   \|D\|_\infty \Tr(A)$ if $D$ is diagonal. Thus,
%   \begin{align*}
%       \Tr( J \tS\inv J \tilde\Sigma\inv ) 
%         &= \Tr( \tS\inv J \tilde\Sigma\inv J ) \\
%         &\le \|\tS\inv\|_\infty \Tr( J \tilde\Sigma\inv J ) \\
%         &= \|\tS\inv\|_\infty \Tr( \tilde\Sigma\inv J ) \\
%         &\le \|\tS\inv\|_\infty \|\tilde\Sigma\inv\|_\infty \Tr( J ) \\
%         &= {\bar k} \|\tS\inv\|_\infty \|\tilde\Sigma\inv\|_\infty.
%   \end{align*}
% 
%   To bound the next term, we need bound the projection of the ones
%   vector, $\ones$, through $J \eqdef U U^\top$,
%   \begin{align*}
%       \ones^\top J \tS\inv J \tilde\Sigma\inv J \tS\inv J \ones \\
%       &=
%       (\ones^\top U) (U^\top \tS\inv U) (U^\top \tilde\Sigma\inv U) \\
%       &\quad (U^\top \tS\inv U) (U^\top \ones)  \\
%       &\ge 
%       (\tS\inv)^2_{\min}
%       (\tilde\Sigma\inv)_{\min}
%       \|\ones^\top U\|^2_2 \\
%       &\ge 
%       \frac{\|\ones^\top U\|^2_2}{\|\tS\|^2_\infty \|\tilde\Sigma\|_\infty},
%   \end{align*}
%   where we have used the property that each of the bracketed terms is an
%   eigen-decomposition, and $U^\top \ones$, lies in the subspace $U$.
%   We similarly have that
%   \begin{align*}
%       \ones^\top J \tS\inv J \ones \\
%       &=
%       (\ones^\top U) (U^\top \tS\inv U) (U^\top \ones) \\
%       &\le 
%       \|\tS\inv\|_{\infty}
%       \|\ones^\top U\|^2_2.
%   \end{align*}
% 
%   Finally, to bound $\|\ones^\top U\|_2$, we have:
%   \begin{align*}
%     \ones^\top O 
%       &= \sum_{\bh = \ones}^{\bh \prec k} \sum_{x = \ones}^{d} \Pr(x | \bh) e_\bh  \\
%       &= \ones \\
%     \ones^\top \tO 
%         &= \ones^\top (O_{\neg \neg d, k} - O_{\neg d, k}\ones^\top) \\
%         &= \sum_{\bh = \ones}^{\bh \prec k} \sum_{x = \ones}^{x \prec d} \Pr(x | \bh) e_\bh - (\sum_{x = \ones}^{x \prec d} \Pr(x | k)) \ones^\top  \\
%         &= \sum_{\bh = \ones}^{\bh \prec k} (1 - \Pr(d | \bh)) e_\bh - (1 - \Pr(d | k)) \ones^\top  \\
%         &= \sum_{\vh = \ones}^{\vh \prec k} (\Pr(d | k) - \Pr(d | \vh))e_\bh \\
%         &= \tO_{d,k} \ones^\top - \tO_{d,\neg k}^\top.
%   \end{align*}
%   Let $c = \|\tO_{d,k} \ones^\top - \tO_{d,\neg
%   k}^\top\|_2 \le \sqrt{k}$.
%   Let $\tO = U W V^\top$ be the SVD of $\tO$. We have,
%   \begin{align*}
%     \ones^\top U W V^\top 
%         &= \ones^\top \tO \\
%     \ones^\top U 
%       &= \ones^\top \tO V W\inv \\
%     \|\ones^\top U\|_2 
%         &\le \frac{\|\ones^\top \tO\|_2}{\sigma_{k}(\tO)} \\
%         &\le \frac{c}{\sigma_{k}(\tO)} \\
%     \|\ones^\top U\|_2 
%         &\ge \frac{c}{\sigma_{1}(\tO)}.
%   \end{align*}
% 
%   Thus,
%   \begin{align*}
%       \ones^\top J \tS\inv J \tilde\Sigma\inv J \tS\inv J \ones 
%       &\ge \frac
%           {c^2}
%           { \|\tS\|_{\infty} \|\tilde\Sigma\|_{\infty}
%           \sigma_{1}(\tO)} \\
%       \ones^\top J \tS\inv J \ones  
%       &\le \frac
%           {c^2 \|\tS\inv\|_{\infty}}
%           {\sigma_{k}(\tO)}.
%   \end{align*}
% 
%   Finally, we have the result,
%   \begin{align*}
%     e^\mom 
%     &\le \|\tS\inv\|_\infty  \|\tilde\Sigma\inv\|_\infty \\
%     &\quad - 
%         \frac{1}{\bar k} 
%     \frac{
%         s c^2 /(\|\tS\|^2_{\infty} \|\tilde\Sigma\|_{\infty}
%             \sigma_{1}(\tO))
%     }
%     {1 + (s c^2 \|\tS\inv\|_{\infty})/
%           \sigma_{k}(\tO)
%     }.
%   \end{align*}
% 
%   For the uniform distribution $M[x] = \frac{1}{\bar d + 1}$,
%   $\|\tS\|_\infty = \bbd$, $\|\tilde
%   \Sigma\|_\infty = \frac{\bbd}{(\bbd + 1)^2}$:
%   \begin{align*}
%     e^\mom 
%     &\le \frac{1}{\bbd} \frac{(\bbd + 1)^2}{\bbd} 
%     - \frac{1}{\bbk} \frac{
%     \bbd c^2/(\bbd^2 \frac{\bbd}{(\bbd +1)^2} \sigma_{k}(\tO))
%     }{
%     1 + c^2 \bbd \frac{1}{\bbd} / \sigma_{k}(\tO)
%     } \\
%     &= (1 + \frac{1}{\bbd})^2 - \frac{1}{\bbk} (1 + \frac{1}{\bbd})^2 \frac{c^2/\sigma_{1}(\tO)}
%       {1 + c^2 / \sigma_{k}(\tO)} \\
%     &= (1 + \frac{1}{\bbd})^2 (1 - \frac{1}{\bbk} \frac{c^2/\sigma_{1}(\tO)}{1 + c^2 / \sigma_{k}(\tO)}).
%   \end{align*}
% 
%   This shows that the pseudoinverse estimator is strictly less efficient
%   than the composite likelihood estimator for finite $k$ and $d$.
%   Furthermore, for a fixed $\bbk$, the pseudoinverse estimator is most
%   efficient for large $\bbd$, and in general it is more efficient for
%   larger $\bbk$.
% \end{proof}
% 

% \begin{align*}
%   \Tr(\Sigmamom - \Sigmaml) 
%   &= 
%       \Tr\left( 
%       \frac{\tOi \tD v v^\top \tD \tOit }
%         {1 - \ones^\top \tD \ones + v^\top \tD v } 
%       - \tOi \tD \ones \ones^\top \tD \tOit 
%       \right) \\
%   &= 
%       \frac{v^\top \tD \tOit \tOi \tD v}
%         {1 - \ones^\top \tD \ones + v^\top \tD v } 
%       - \ones^\top \tD \tOit  \tOi \tD \ones \\
% \end{align*}

