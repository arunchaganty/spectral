\subsection{\lemmaref{mom-pw-variance}}
\label{app:pw-proof}

In \sectionref{piecewise}, we compare the asymptotic variance
  $\Sigma^\ml_\sC$ of the composite likelihood estimator for a clique
  $\sC$, with that of the pseudo-inverse estimator, $\Sigma^\mom_\sC$. 
In this part, we will derive the asymptotic variances of these two
  estimators and compare their relative efficiencies.

Recall, that in \sectionref{piecewise} we reparameterized the
  constrained estimators $Z_\sC \in \Delta_{k^m-1}$ with $\tZ_\sC \in
  [0,1]^{k^m}$. 
Similarly, we reparameterized $M_\sV \in \Delta_{d^m - 1}$ with
  $\tM_\sV \in [0,1]^{d^m}$ to compute
  the likelihood.
We used the vectorized forms of the marginal distribution $\tM_\sV \in
  \Re^{d^m - 1}$, the parameters $\tZ_\sC \in \Re^{k^m-1}$ and the
  matrix form of the conditional moments, $\mOppAll \in \Re^{d^m \times
  k^m}$, to represent the marginal distribution as
\begin{align*}
  \tM_\sV &= \mOppTAll \tZ_\sC + \mOppAll_{\neg \bd, \vk},
\end{align*}
where $\mOppTAll \eqdef (\mOppAll_{\neg \bd, \neg \vk}
- \mOppAll_{\neg \bd, \vk}\ones^\top) \in \Re^{d^m - 1 \times
k^m - 1}$ is a matrix containing the first $d^m-1$ rows and first $k^m-1$ columns of $\mOppAll$
and $\mOppAll_{\neg \bd, \vk} \in \Re^{d^m-1}$ is the last column of
$\mOppAll$, ignoring the last row.

\begin{proof}[Proof for \lemmaref{mom-pw-variance}]
  We proceed using the delta method. First, let us look at the
  asymptotic variance of the pseudo-inverse estimator. With the
  reparameterization $\tZ_\sC$, we get,
  \begin{align*}
    \tM_\sV &= \mOppTAll \tZ_\sC + \mOppAll_{\neg \bd, \vk} \\
    \tZ_\sC &= \mOppTAlli (\tM_\sV - \mOppAll_{\neg \bd, \vk}).
  \end{align*}
  As this is just a linear transformation on $\tM_\sV$, 
  the asymptotic variance of $\widehat{\tZ^\mom_\sC}$ is,
  \begin{align*}
      \Sigmamom_{\sC} &= \mOppTAlli \widetilde\Sigma_\sV \mOppTAllit,
  \end{align*}
  where $\widetilde\Sigma_\sV$ is the variance of $\tM_\sV$: $\widetilde\Sigma_\sV = \widetilde{D}_\sV
  (I - \widetilde{D}_\sV)$, where we have used $\widetilde D_\sV \eqdef \diag(\tM_\sV)$ as
  before.

  Now, let us look at the variance of the piecewise estimator.  Using
  the delta-method \cite{vaart98asymptotic} we have that the asymptotic
  variance of 
  $\widehat{\tZ^\ml_\sC} = \argmax_{\tZ_\sC \in [0,1]^{k^m-1}} \E[\ell(\vx)]$ is,
  \begin{align*}
    \Sigmaml_\sC &= \E[\grad^2 \ell^{-1}] \Var[\grad \ell] \E[\grad^2 \ell^{-1}]),
  \end{align*}
  where $\ell(\bx)$ is the likelihood of the data. $\ell(\bx)$ can be
  written in terms of $\tZ_\sC$ and $\mOppTAll$ as,
  \begin{align*}
    \ell(\vx) 
              &= \log(M_\sV[\vx]) \\
              &= \BI[\vx \neq \bd] \log(\tM_\sV[\vx]) + \BI[\vx = \bd] \log(1 - \ones^\top \tM_\sV) ) \\
              &= \BI[\vx \neq \bd] \log(\mOppTAll[\vx] \tZ_\sC + \mOppAll_{\neg \bd, \vk}[\vx]) \\
              &+ \BI[\vx = \bd] \log(1 - \ones^\top (\mOppTAll \tZ_\sC + \mOppAll_{\neg \bd, \vk})).
  \end{align*}

Taking the first derivative,
\begin{align}
  \grad_{\tZ_\sC} \ell(\vx)
  &= 
  \frac{\BI[\vx \neq \bd] \mOppTAll[\vx]^\top - \BI[\vx = \bd] \ones^\top \mOppTAll}{\mOppAll[\vx] \mH_\sC} 
  \nonumber \\ 
  &= 
  \begin{bmatrix}
    \mOppTAll \\
    -\ones^\top \mOppTAll
  \end{bmatrix}^\top
  D_{\sV}^{-1} e_\vx, \label{eqn:lhood-grad}
\end{align}
where $\mOppTAll[\vx] \in \Re^{1\times k^m-1}$, $e_\vx$ is a one-hot
vector and $M_\sV$ is the marginal distribution of the observed data at
the true parameters.

Taking the second derivative,
\begin{align}
  \grad^2_{\tilde \mH_\sC} \ell(\vx)
  &= -\frac{\BI[\vx \neq \bd] \mOppTAll[\vx]^\top \mOppTAll[\vx] + \BI[\vx = \bd] \mOppTAllt \ones \ones^\top \mOppTAll}{(\mOppAll[\vx] \mH_\sC)^2} \nonumber \\
  &= - 
  \begin{bmatrix}
    \mOppTAll \\
    \ones^\top \mOppTAll
  \end{bmatrix}^\top
    D_{\sV}^{-1} e_\vx e_\vx^\top D_{\sV}^{-1} 
  \begin{bmatrix}
    \mOppTAll \\
    \ones^\top \mOppTAll
  \end{bmatrix}. \label{eqn:lhood-hess}%
\end{align}

From \equationref{lhood-grad} and \equationref{lhood-hess}, we get
  \begin{align*}
    \Var [\grad \ell(\vx)] &= -
        \begin{bmatrix}
          \mOppTAll \\
          \ones^\top \mOppTAll
        \end{bmatrix}^\top
          D_{\sV}^{-1} \diag(\Sigma_\sV) D_{\sV}^{-1} 
        \begin{bmatrix}
          \mOppTAll \\
          \ones^\top \mOppTAll
        \end{bmatrix} \\
        \E[\grad^2 \ell(\vx)] 
        &= \hphantom{-}
        \begin{bmatrix}
          \mOppTAll \\
          - \ones^\top \mOppTAll
        \end{bmatrix}^\top
          D_{\sV}^{-1} \diag(\Sigma_\sV) D_{\sV}^{-1} 
        \begin{bmatrix}
          \mOppTAll \\
          - \ones^\top \mOppTAll
        \end{bmatrix} \\
        &= \hphantom{-}
        \begin{bmatrix}
          \mOppTAll \\
          \ones^\top \mOppTAll
        \end{bmatrix}^\top
          D_{\sV}^{-1} \diag(\Sigma_\sV) D_{\sV}^{-1} 
        \begin{bmatrix}
          \mOppTAll \\
          \ones^\top \mOppTAll
        \end{bmatrix}.
  \end{align*}
The last equality holds because $D_{\sV}^{-1} \diag(\Sigma_\sV)
D_{\sV}^{-1}$ is diagonal.
As expected, $\E[\grad^2 \ell(\vx)] = -\Var [\grad \ell(\vx)]$ because
$\widehat{\tZ_\sC}$ is a maximum likelihood estimator. 
Noting that $\diag(\Sigma_\sV) = D_\sV (I - D_\sV)$, 
\begin{align*}
  \Var [\grad \ell(\vx)] &=
      - \begin{bmatrix}
        \mOppTAll \\
        \ones^\top \mOppTAll
      \end{bmatrix}^\top
        D_{\sV}^{-1} (I - D_\sV) 
      \begin{bmatrix}
        \mOppTAll \\
        \ones^\top \mOppTAll
      \end{bmatrix} \\
%   &=
%    - \mOppTAllt \tD_{\sV}^{-1} (I - \tD_\sV) \mOppTAll + \mOppTAllt \ones \frac{\ones^\top \tM_\sV}{1 - \ones^\top \tM_\sV} \ones^\top \mOppTAll \\
   &=
    - \mOppTAllt \tS_\sV \mOppTAll + s_\sV \mOppTAllt \ones \ones^\top \mOppTAll,
\end{align*}
where $\tS_\sV$ and $s_\sV$ are sub-matrices of $\tS_\sV$, 
\begin{align*}
  \begin{bmatrix}
          \tS_\sV & \zeros \\
          \zeros^\top & s_\sV
  \end{bmatrix}
  &\eqdef D_\sV\inv (I - D_\sV).
\end{align*}

Finally, we can derive the asymptotic variance of $\Sigmaml_\sC$,
\begin{align*}
    \Sigmaml_{\sC} 
      &= \E[\grad^2 \ell(\vx)]^{-1} \Var [\grad \ell(\vx)] \E[\grad^2 \ell(\vx)]^{-1} \\
      &= -\Var[\grad \ell(\vx)]^{-1} \\
      &= (\mOppTAllt \tS_\sV \mOppTAll + s_\sV (\mOppTAllt \ones)(\mOppTAllt \ones)^\top)\inv,
\end{align*}
The above expression is the inversion of an invertible matrix plus a rank-one component. The Sherman-Morrison formula allows us to write the inverse in closed form:
\begin{align*}
  (A + uv^\top)\inv &= A\inv - \frac{A\inv uv^\top A\inv}{1 + v^\top A\inv u}.
\end{align*}

This gives us,
\begin{align}
    \Sigmaml_{\sC} 
    &= \mOppTAlli \tS_\sV\inv \mOppTAllit 
      - \frac{s_\sV \mOppTAlli \tS_\sV\inv \mOppTAllit \mOppTAllt \ones \ones^\top \mOppTAll \mOppTAlli \tS_\sV\inv \mOppTAllit }
      {1 + s_\sV \ones^\top \mOppTAll \mOppTAlli \tS_\sV\inv \mOppTAllit \mOppTAllt \ones} \nonumber \\
    &= \mOppTAlli \tS_\sV\inv \mOppTAllit 
      - \frac{s_\sV \mOppTAlli \tS_\sV\inv J \ones \ones^\top J \tS_\sV\inv \mOppTAllit }
      {1 + s_\sV \ones^\top J \tS_\sV\inv J \ones}, \label{eqn:asymp-var-ml}
\end{align}
where $J \eqdef \mOppTAll \mOppTAlli = U U^\top$, where $U$ is a $d^m
- 1 \times k^m$ matrix with orthonormal columns.

Given our assumptions, $\ones \succ M_\sV \succ \zeros$, and
  $\ones^\top M_\sV = 1$. Consequently, $\tS_\sV$ is invertible and 
Thus the asymptotic variance is finite and our estimator is consistent
  as well. 
\end{proof}

With concrete expressions for $\Sigmamom_\sC$ and $\Sigmaml_\sC$, we can quantitatively evaluate their asymptotic efficiency,
\begin{proof}[Proof for \corollaryref{efficiency}]
  Let $\bar k = k^m -1$ and $\bar d = d^m -1$. Consider their relative
  efficiencies, 
  \begin{align*}
    e^\mom 
        &\eqdef \frac{1}{\bar k} \Tr(\Sigmaml_\sC\Sigmamomi_\sC ) \\
        &= \frac{1}{\bar k} \Tr( \mOppTAlli \tS_\sV\inv \mOppTAllit \mOppTAllt \tilde\Sigma_\sV\inv \mOppTAll ) \\
      &\quad - \frac{1}{\bar k} \frac{s_\sV \Tr( \mOppTAlli \tS_\sV\inv J \ones \ones^\top J \tS_\sV\inv \mOppTAllit \mOppTAllt  \tilde\Sigma_\sV\inv \mOppTAll  )}
      {1 + s_\sV \ones^\top J \tS_\sV\inv J \ones} \\
        &= \frac{1}{\bar k} \Tr( J \tS_\sV\inv J \tilde\Sigma_\sV\inv ) - \frac{1}{\bar k} \frac{s_\sV \ones^\top J \tS_\sV\inv J \tilde\Sigma_\sV\inv J \tS_\sV\inv J \ones }
      {1 + s_\sV \ones^\top J \tS_\sV\inv J \ones},
  \end{align*}
  where $J \eqdef \mOppTAll \mOppTAlli = U U^\top$ as before. 
  
  Note that $\Tr(J) = \rank(J) = {\bar k}$.
  Next we use H\"{o}lder's inequality for the trace: $\Tr(D A) \le
  \|D\|_\infty \Tr(A)$ if $D$ is diagonal. Thus,
  \begin{align*}
      \Tr( J \tS_\sV\inv J \tilde\Sigma_\sV\inv ) 
        &= \Tr( \tS_\sV\inv J \tilde\Sigma_\sV\inv J ) \\
        &\le \|\tS_\sV\inv\|_\infty \Tr( J \tilde\Sigma_\sV\inv J ) \\
        &= \|\tS_\sV\inv\|_\infty \Tr( \tilde\Sigma_\sV\inv J ) \\
        &\le \|\tS_\sV\inv\|_\infty \|\tilde\Sigma_\sV\inv\|_\infty \Tr( J ) \\
        &= {\bar k} \|\tS_\sV\inv\|_\infty \|\tilde\Sigma_\sV\inv\|_\infty.
  \end{align*}

  To bound the next term, we need bound the projection of the ones
  vector, $\ones$, through $J \eqdef U U^\top$,
  \begin{align*}
      \ones^\top J \tS_\sV\inv J \tilde\Sigma_\sV\inv J \tS_\sV\inv J \ones \\
      &=
      (\ones^\top U) (U^\top \tS_\sV\inv U) (U^\top \tilde\Sigma_\sV\inv U) \\
      &\quad (U^\top \tS_\sV\inv U) (U^\top \ones)  \\
      &\ge 
      (\tS_\sV\inv)^2_{\min}
      (\tilde\Sigma_\sV\inv)_{\min}
      \|\ones^\top U\|^2_2 \\
      &\ge 
      \frac{\|\ones^\top U\|^2_2}{\|\tS_\sV\|^2_\infty \|\tilde\Sigma_\sV\|_\infty},
  \end{align*}
  where we have used the property that each of the bracketed terms is an
  eigen-decomposition, and $U^\top \ones$, lies in the subspace $U$.
  We similarly have that
  \begin{align*}
      \ones^\top J \tS_\sV\inv J \ones \\
      &=
      (\ones^\top U) (U^\top \tS_\sV\inv U) (U^\top \ones) \\
      &\le 
      \|\tS_\sV\inv\|_{\infty}
      \|\ones^\top U\|^2_2.
  \end{align*}

  Finally, to bound $\|\ones^\top U\|_2$, we have:
  \begin{align*}
    \ones^\top \mOppAll 
      &= \sum_{\bh = \ones}^{\bh \prec \bk} \sum_{\vx = \ones}^{\bd} \Pr(\vx | \bh) e_\bh  \\
      &= \ones \\
    \ones^\top \mOppTAll 
        &= \ones^\top (\mOppAll_{\neg \neg \bd, \vk} - \mOppAll_{\neg \bd, \vk}\ones^\top) \\
        &= \sum_{\bh = \ones}^{\bh \prec \bk} \sum_{\vx = \ones}^{\vx \prec \bd} \Pr(\vx | \bh) e_\bh - (\sum_{\vx = \ones}^{\vx \prec \bd} \Pr(\vx | \vk)) \ones^\top  \\
        &= \sum_{\bh = \ones}^{\bh \prec \bk} (1 - \Pr(\bd | \bh)) e_\bh - (1 - \Pr(\bd | \bk)) \ones^\top  \\
        &= \sum_{\vh = \ones}^{\vh \prec \vk} (\Pr(\bd | \vk) - \Pr(\bd | \vh))e_\bh \\
        &= \mOppTAll_{\bd,\bk} \ones^\top - \mOppTAll_{\bd,\neg \bk}^\top.
  \end{align*}
  Let $c = \|\mOppTAll_{\bd,\bk} \ones^\top - \mOppTAll_{\bd,\neg
  \bk}^\top\|_2 \le \sqrt{k}$.
  Let $\mOppTAll = U W V^\top$ be the SVD of $\mOppTAll$. We have,
  \begin{align*}
    \ones^\top U W V^\top 
        &= \ones^\top \mOppTAll \\
    \ones^\top U 
      &= \ones^\top \mOppTAll V W\inv \\
    \|\ones^\top U\|_2 
        &\le \frac{\|\ones^\top \mOppTAll\|_2}{\sigma_{\min}(\mOppTAll)} \\
        &\le \frac{c}{\sigma_{\min}(\mOppTAll)} \\
    \|\ones^\top U\|_2 
        &\ge \frac{c}{\sigma_{\max}(\mOppTAll)} \\
  \end{align*}

  Thus,
  \begin{align*}
      \ones^\top J \tS_\sV\inv J \tilde\Sigma_\sV\inv J \tS_\sV\inv J \ones 
      &\ge \frac
          {c^2}
          { \|\tS_\sV\|_{\infty} \|\tilde\Sigma_\sV\|_{\infty}
          \sigma_{\max}(\mOppTAll)} \\
      \ones^\top J \tS_\sV\inv J \ones  
      &\le \frac
          {c^2 \|\tS_\sV\inv\|_{\infty}}
          {\sigma_{\min}(\mOppTAll)}.
  \end{align*}

  Finally, we have the result,
  \begin{align*}
    e^\mom 
    &\le \|\tS_\sV\inv\|_\infty  \|\tilde\Sigma_\sV\inv\|_\infty \\
    &\quad - 
        \frac{1}{\bar k} 
    \frac{
        s_\sV c^2 /(\|\tS_\sV\|^2_{\infty} \|\tilde\Sigma_\sV\|_{\infty}
            \sigma_{\max}(\mOppTAll))
    }
    {1 + (s_\sV c^2 \|\tS_\sV\inv\|_{\infty})/
          \sigma_{\min}(\mOppTAll)
    }.
  \end{align*}

  For the uniform distribution $M_\sV[x] = \frac{1}{\bar d + 1}$,
  $\|\tS_\sV\|_\infty = \bbd$, $\|\tilde
  \Sigma_\sV\|_\infty = \frac{\bbd}{(\bbd + 1)^2}$:
  \begin{align*}
    e^\mom 
    &\le \frac{1}{\bbd} \frac{(\bbd + 1)^2}{\bbd} 
    - \frac{1}{\bbk} \frac{
    \bbd c^2/(\bbd^2 \frac{\bbd}{(\bbd +1)^2} \sigma_{\min}(\mOppTAll))
    }{
    1 + c^2 \bbd \frac{1}{\bbd} / \sigma_{\min}(\mOppTAll)
    } \\
    &= (1 + \frac{1}{\bbd})^2 - \frac{1}{\bbk} (1 + \frac{1}{\bbd})^2 \frac{c^2/\sigma_{\max}(\mOppTAll)}
      {1 + c^2 / \sigma_{\min}(\mOppTAll)} \\
    &= (1 + \frac{1}{\bbd})^2 (1 - \frac{1}{\bbk} \frac{c^2/\sigma_{\max}(\mOppTAll)}{1 + c^2 / \sigma_{\min}(\mOppTAll)}).
  \end{align*}

  This shows that the pseudoinverse estimator is strictly less efficient
  than the composite likelihood estimator for finite $k$ and $d$.
  Furthermore, for a fixed $\bbk$, the pseudoinverse estimator is most
  efficient for large $\bbd$, and in general it is more efficient for
  larger $\bbk$.
\end{proof}

