\section{Proofs}
\label{app:proofs}

In the interest of space, we have omitted some proofs from
the main contents of the paper. We present their proofs in detail below.

%\paragraph{Sample complexity for $Z_\sC$}
%
%Consider the estimation of the conditional moments for a bottleneck $h_i$ with views $x_{v_1},
%x_{v_2}, x_{v_3}$. Let the error in estimation of the moments (i.e.
%  $\|M_{v_1 v_2} - \hat M_{v_1 v_2} \|_F$, etc.)
%$\epsilon$. 
%Results from \citet{anandkumar12moments,anandkumar13tensor} show that 
%\begin{align*}
%  \|\mOpphat{v_1}{i} - \mOppit{v_1}{i}\|^2_F 
%    &= O( 
%    \frac{k {\pi\oft{i}}_{\max}^2}
%    {\sigma_k(M_{v_1,v_2})^5 } \epsilon ). 
%\end{align*}
%
%Similarly, we can apply standard perturbation analysis techniques to get
%an error bound on $Z_\sC$;
%\begin{align*}
%  \|Z_\sC - \hat Z_\sC\|_F 
%  &\le \|M_\sV(\mOppit{v_1}{i_1}, \cdots, \mOppit{v_m}{i_m})  - \hat M_\sV(\hat{\mOppit{v_1}{i_1}}, \cdots, \hat{\mOppit{v_m}{i_m}})\|^2_F.
%\end{align*}
%Let $\|\mOppit{v_1}{i_1}\|_F < O$ and $\|\hat{\mOppit{v_1}{i_1}}\|_F < O$; then, we get
%\begin{align*}
%  \|Z_\sC - \hat Z_\sC\|_F 
%  &\le \|M_\sV - \hat M_\sV\|_F O^m + \|\hat M_\sV\|_F O^{m-1} \max\{\|\hat {\mOppit{v_1}{i_1}} - \mOppit{v_1}{i_1}\|_F\}.
%\end{align*}

\subsection{\lemmaref{pw-variance}}
\label{app:pw-variance-proof}

In \sectionref{piecewise}, we compare the asymptotic variance $\Sigma^\ml_\sC$ of the
composite likelihood estimator for a clique $\sC$, $\hat Z^\ml_\sC$, with
that of the pseudolikelihood estimator $\Sigma^\mom_\sC$.

\begin{proof}
  Using the delta-method \cite{vaart98asymptotic}, we have that the
  asymptotic distribution of $Z_\sC$ is,
  \begin{align*}
    \sqrt{n}(\hat Z_{\sC} - Z_{\sC}) &\convind \sN( 0, \grad^2 \sL_\ml^{-1} \Var[\grad \sL_\ml] \grad^2 \sL_\ml^{-1}).
  \end{align*}

Taking the first derivative,
\begin{align}
  \grad_{\mH_\sC} \sL_\ml(\sX_\sV) 
  &= \sum_{x \in \sD} \frac{\mOppAll[\vx]}{\mH_\sC \cdot \mOppAll[\vx]} \nonumber \\ 
  &= \mOppAll[\vx] \diag(\tilde \mO_{\sV})^{-1} \mO_{\sV}, \label{eqn:lhood-grad}
\end{align}
where $\tilde \mO_\sV$ is marginal distribution with parameters $\mH_\sC$, also represented as a vector in $\Re^{d^m}$.

Taking the second derivative.
\begin{align}
  \grad^2_{\mH_\sC} \sL_\ml(\Sx \sV) 
  &= \sum_{x \in \sD} \frac{\mOppAll[\vx] \mOppAllt[\vx]}{(\mH_\sC \cdot \mOppAll[\vx])^2} \nonumber \\
  &= \sum_{x \in \sD}\mOppAll[\vx] \mOppAllt[\vx] \frac{\mO_{\sV}[\vx]}{\tilde \mO_{\sV}^2[\vx]} \nonumber \\
  &= \mOppAll \diag(\mO_{\sV}) \diag(\tilde \mO_{\sV})^{-2} \mOppAllt. \label{eqn:lhood-hess}%
\end{align}

% DONE: don't need this
%It follows that $\grad^2_{\mH_\sC} \sL_\ml(\Sx \sV) \succ 0$ because
%$\tilde \mO_\sV, \tilde \mO_\sV \succ 0$ and $\mOppAll$ is
%full rank and stochastic.

% PL: this should just be a consequence
%Next, we show that it is
%strictly concave, which guarantees that it has a unique maximizer.

  From \equationref{lhood-grad}, we get
  \begin{align*}
    \Var [\grad \sL_\ml(\vec x_\sC)] &= \mOppAll \diag(\tilde M_\sV) \Sigma_\sV \diag(\tilde M_\sV) \mOppAll^T .
  \end{align*}

  Finally, using \equationref{lhood-hess}, we have
  \begin{align*}
    \Sigma_{Z_\sC} 
      &= \grad^2 \sL_\ml(\vec x_\sC)^{-1} \Var [\grad \sL_\ml(\vec x_\sC)] \grad^2 \sL_\ml(\vec x_\sC)^{-1}) \\
      &= \pinvt{\mOppAll} \diag(\tilde M_\sV) \Sigma_\sV \diag(\tilde M_\sV) \pinv{\mOppAll}.
  \end{align*}

  At the true parameters, $\tilde M_\sV = M_\sV$, completing the proof.
%  \todo{argue that asymptotic variance is finite, so the estimator is consistent (this is technically good form,
%but it's fine given space constraints}
\end{proof}

\subsection{Recovering conditional moments}

% Define up front that we will focus on h, x_1, x_2, x_3
In step 1 of \LearnMarginals, we used the bottleneck property of a hidden
  variable $h_i$ to learn conditional moments $\mOpp{v}{i}$ for every
  'view' $x_v \in \sV_{h_i}$ using \TensorFactorize, provided
  \assumptionref{full-rank-plus} holds;
\begin{assumption*}
For every clique $\sC \in \sG$ (including ones involving observed variables),
every $R^{k \times k}$ slice of the marginals $Z_\sC$ has full column rank $k$.
\end{assumption*}

In this section, we will prove the sufficiency of the assumption. First,
  let us define some additional notation.

\paragraph{Notation}

Let $\sC = \{h_1, \cdots, h_m\}$ be a hidden clique in the graph, and
  let $Z_\sC \eqdef \Pr(h_1, \cdots, h_m)$ be the tensor representing the
  joint distribution. 
We can obtain a marginal distribution for a sub-set $B \subset \sC$ by
  marginalizing out the remaining variables. 
Without loss of generality, let $|B| = m'$ and let the first $m'$
  indices of $Z_\sC$ correspond to the $h_i \in B$; we can represent the
  marginalization operation with linear algebra as $Z_B = Z_\sC(
  \underbrace{I, \cdots, I}_{m' \text{times}}, \underbrace{\ones, \cdots, \ones}_{m - m'
  \text{times}} )$. 

Next, for a $m$-th order tensor $Z_\sC$ and $B \subset \sC$ with $m' \eqdef |B'|$, define the {\em
partial diagonal} $\diag_{B}(Z_\sC)$ to be the $m - m' + 1$-th order tensor with
a diagonal along the elements of $B$; $\diag_{B}(Z_\sC)[i, 
i_{m'+1, \ldots, m}] = Z_\sC[\underbrace{i, \ldots, i}_{m' \text{times}}, i_{m'+1}, \ldots, m]$.


\paragraph{Relation to clique marginals}

To use $\TensorFactorize$ on the bottleneck $h_1$ with views $x_1, x_2,
x_3$, we require that $\pi\oft1 \eqdef \Pr(h_1) \succ 0$ and that $\mOpp{i}{1} \eqdef \Pr(x_v | h_1)$ is full
  rank for every $v \in \{1,2,3\}$ (\assumptionref{full-rank}).
Let us explicitly write out $\pi\oft1, \mOpp{i}{1} \forall i \in
  \{1,2,3\}$ in terms of the clique marginals $Z_\sC$.

By the reduction presented in \lemmaref{reduction}, we can assume
  w.l.o.g. that the observed variables always appear as leaf nodes with a single parent). 
Let $h_v$ be the parent of $x_v$. Then, 
\begin{align*}
  \mOpp{v}{1} &\eqdef \Pr( x_v | h_1 )  \\
              &= \sum_{h_v} \Pr( x_v | h_v ) \Pr( h_v | h_1 ) \\
              &= \mOpp{v}{v} \mYpp{v}{1},
\end{align*}
where $\mYpp{i}{j} \eqdef \Pr( h_i | h_j )$. 

Let $\Succ(h)$ be the parents (neighbors) of a hidden variable $h$ in
  a directed (undirected) graphical model. 
Note that $\mYpp{v}{1}$ can be decomposed in terms of it's parents,
\begin{align*}
  \mYpp{v}{1} &\eqdef \Pr( h_v | h_1 )  \\
  &= \sum_{\vec h \in \sH_{\succ{h_v}}} \Pr( \Succ(h_v) | h_1 ) \Pr( h_v | \Succ(h_v) ) \\
              &= \mOpp{v}{v} \mYpp{v}{1},
\end{align*}


Let $\sC_0 = \{h_1, h_2, h_3, \cdots, h_m\}$ be any clique containing
  $h_1$. $\mPi{1}$ can be obtained from $Z_\sC$ by marginalizing the
  remaining $h_j ~ j \neq 1$, i.e. $\mPi{1} = Z_\sC(\cdot, \ones, \cdots,
  \ones)$ 

The form of $\mOpp{v}{1} = \Pr(x_v \given h_1)$ is a bit more
  complicated. 
We describe the procedure to construct $\mOpp{1}{1}$ from the data which applies to   $\mOpp{v}{1}$.


\paragraph{Getting conditional distributions}

For any clique, 


\subsection{\assumptionref{full-rank-plus}}
%
%- Define an inferential path to go through $x_1, x_2, x_3, \cdots, x_n$, with cliques $\sC_1, \cdots, \sC_n$.
%- Define marginal to be $\diag( Z_\sC(\cdot, 1) ) Z_\sC(\cdot, \cdot)$.
%- Then, $\Pr(x_n | x_1) = Z_{\sC_1}(\cdot, \cdot, 1, 1) \cdots Z_{\sC_n}(\cdot, \cdot, 1, 1)$.
%
%- Thus, as long as $Z_{\sC}(\cdot, \cdot, 0} \succ 0$, $\Pr(x_n | x_1)$ has full rank.
%
%- Answer: partial diagonal.
%- Lemma: if $M \succ 0$ and $M_{ij} > 0$ (i.e. $M$ is a probability distribution), then $1^T M, M 1 \succ 0$ as well.
%- Lemma: if $T \succ 0$ and $T_{ijk} > 0$, then $T(1, \cdot, \cdot) \succ 0$ as well, and continue by induction.
%        - is it possible to write out a condition for $\sigma_k$?
%

