\section{Proofs}
\label{app:proofs}

In the interest of space, we have omitted some proofs from
the main contents of the paper. We present their proofs in detail below.

%\paragraph{Sample complexity for $Z_\sC$}
%
%Consider the estimation of the conditional moments for a bottleneck $h_i$ with views $x_{v_1},
%x_{v_2}, x_{v_3}$. Let the error in estimation of the moments (i.e.
%  $\|M_{v_1 v_2} - \hat M_{v_1 v_2} \|_F$, etc.)
%$\epsilon$. 
%Results from \citet{anandkumar12moments,anandkumar13tensor} show that 
%\begin{align*}
%  \|\mOpphat{v_1}{i} - \mOppit{v_1}{i}\|^2_F 
%    &= O( 
%    \frac{k {\pi\oft{i}}_{\max}^2}
%    {\sigma_k(M_{v_1,v_2})^5 } \epsilon ). 
%\end{align*}
%
%Similarly, we can apply standard perturbation analysis techniques to get
%an error bound on $Z_\sC$;
%\begin{align*}
%  \|Z_\sC - \hat Z_\sC\|_F 
%  &\le \|M_\sV(\mOppit{v_1}{i_1}, \cdots, \mOppit{v_m}{i_m})  - \hat M_\sV(\hat{\mOppit{v_1}{i_1}}, \cdots, \hat{\mOppit{v_m}{i_m}})\|^2_F.
%\end{align*}
%Let $\|\mOppit{v_1}{i_1}\|_F < O$ and $\|\hat{\mOppit{v_1}{i_1}}\|_F < O$; then, we get
%\begin{align*}
%  \|Z_\sC - \hat Z_\sC\|_F 
%  &\le \|M_\sV - \hat M_\sV\|_F O^m + \|\hat M_\sV\|_F O^{m-1} \max\{\|\hat {\mOppit{v_1}{i_1}} - \mOppit{v_1}{i_1}\|_F\}.
%\end{align*}

\subsection{\lemmaref{pw-variance}}
\label{app:pw-variance-proof}

In \sectionref{piecewise}, we compare the asymptotic variance $\Sigma^\ml_\sC$ of the
composite likelihood estimator for a clique $\sC$, $\hat Z^\ml_\sC$, with
that of the pseudolikelihood estimator $\Sigma^\mom_\sC$.

\begin{proof}
  Using the delta-method \cite{vaart98asymptotic}, we have that the
  asymptotic distribution of $Z_\sC$ is,
  \begin{align*}
    \sqrt{n}(\hat Z_{\sC} - Z_{\sC}) &\convind \sN( 0, \grad^2 \sL_\ml^{-1} \Var[\grad \sL_\ml] \grad^2 \sL_\ml^{-1}).
  \end{align*}

Taking the first derivative,
\begin{align}
  \grad_{\mH_\sC} \sL_\ml(\sX_\sV) 
  &= \sum_{x \in \sD} \frac{\mOppAll[\vx]}{\mH_\sC \cdot \mOppAll[\vx]} \nonumber \\ 
  &= \mOppAll[\vx] \diag(\tilde \mO_{\sV})^{-1} \mO_{\sV}, \label{eqn:lhood-grad}
\end{align}
where $\tilde \mO_\sV$ is marginal distribution with parameters $\mH_\sC$, also represented as a vector in $\Re^{d^m}$.

Taking the second derivative.
\begin{align}
  \grad^2_{\mH_\sC} \sL_\ml(\Sx \sV) 
  &= \sum_{x \in \sD} \frac{\mOppAll[\vx] \mOppAllt[\vx]}{(\mH_\sC \cdot \mOppAll[\vx])^2} \nonumber \\
  &= \sum_{x \in \sD}\mOppAll[\vx] \mOppAllt[\vx] \frac{\mO_{\sV}[\vx]}{\tilde \mO_{\sV}^2[\vx]} \nonumber \\
  &= \mOppAll \diag(\mO_{\sV}) \diag(\tilde \mO_{\sV})^{-2} \mOppAllt. \label{eqn:lhood-hess}%
\end{align}

% DONE: don't need this
%It follows that $\grad^2_{\mH_\sC} \sL_\ml(\Sx \sV) \succ 0$ because
%$\tilde \mO_\sV, \tilde \mO_\sV \succ 0$ and $\mOppAll$ is
%full rank and stochastic.

% PL: this should just be a consequence
%Next, we show that it is
%strictly concave, which guarantees that it has a unique maximizer.

  From \equationref{lhood-grad}, we get
  \begin{align*}
    \Var [\grad \sL_\ml(\vec x_\sC)] &= \mOppAll \diag(\tilde M_\sV) \Sigma_\sV \diag(\tilde M_\sV) \mOppAll^T .
  \end{align*}

  Finally, using \equationref{lhood-hess}, we have
  \begin{align*}
    \Sigma_{Z_\sC} 
      &= \grad^2 \sL_\ml(\vec x_\sC)^{-1} \Var [\grad \sL_\ml(\vec x_\sC)] \grad^2 \sL_\ml(\vec x_\sC)^{-1}) \\
      &= \pinvt{\mOppAll} \diag(\tilde M_\sV) \Sigma_\sV \diag(\tilde M_\sV) \pinv{\mOppAll}.
  \end{align*}

  At the true parameters, $\tilde M_\sV = M_\sV$, completing the proof.
%  \todo{argue that asymptotic variance is finite, so the estimator is consistent (this is technically good form,
%but it's fine given space constraints}
\end{proof}

\subsection{Recovering conditional moments}

% Define up front that we will focus on h, x_1, x_2, x_3
In step 1 of \LearnMarginals, we use the bottleneck property of a hidden
  variable $h_i$ to learn conditional moments $\mOpp{v}{i}$ for every
  'view' $x_v \in \sV_{h_i}$ using \TensorFactorize. 
We stated that \assumptionref{full-rank-plus} was sufficient to
  guarantee that \assumptionref{full-rank} holds for the $\TensorFactorize$
  algorithm\reword.
In this section, we'll prove its sufficiency.

First, some additional notation.

\paragraph{Notation}

For a $m$-th order tensor $T$ and index set $\sI$, define the {\em
partial diagonal} $\diag_{\sI}(T)$ to be the $m - |\sI| + 1$ tensor with
a diagonal along the index elements $\sI$; $\diag_{\sI}(T)_{\alpha,
i_{n+1, \ldots, m}} = T_{\alpha, \ldots, \alpha, i_{n+1}, \ldots, m}$.

\paragraph{Relation to clique marginals $Z_\sC$}

To use $\TensorFactorize$ on the bottleneck $h_1$ with views $x_1, x_2,
x_3$, we require that $\pi\oft1 \eqdef \Pr(h_1) \succ 0$ and that $\mOpp{i}{1} \eqdef \Pr(x_v | h_1)$ is full
  rank for every $v \in \{1,2,3\}$ (\assumptionref{full-rank}).
Let us explicitly write out $\pi\oft1, \mOpp{i}{1} \forall i \in
  \{1,2,3\}$ in terms of the clique marginals $Z_\sC$.

Let $\sC_0 = \{h_1, h_2, h_3, \cdots, h_m\}$ be any clique containing
  $h_1$. $\mPi{1}$ can be obtained from $Z_\sC$ by marginalizing the
  remaining $h_j ~ j \neq 1$, i.e. $\mPi{1} = Z_\sC(\cdot, \ones, \cdots,
  \ones)$ 

The form of $\mOpp{v}{1} = \Pr(x_v \given h_1)$ is a bit more
  complicated. 
We describe the procedure to construct $\mOpp{1}{1}$ from the data which applies to   $\mOpp{v}{1}$.

By the reduction presented in \lemmaref{reduction}, we can assume
  w.l.o.g. that $x_i$ are a leaf node for all $i$.
Let $\{h_2, h_3, \cdots, h_l\}$ be the set of paths through hidden
  variables from $h_1$ to $x_1$.

\paragraph{Getting conditional distributions}

For any clique, 


\subsection{\assumptionref{full-rank-plus}}
%
%- Define an inferential path to go through $x_1, x_2, x_3, \cdots, x_n$, with cliques $\sC_1, \cdots, \sC_n$.
%- Define marginal to be $\diag( Z_\sC(\cdot, 1) ) Z_\sC(\cdot, \cdot)$.
%- Then, $\Pr(x_n | x_1) = Z_{\sC_1}(\cdot, \cdot, 1, 1) \cdots Z_{\sC_n}(\cdot, \cdot, 1, 1)$.
%
%- Thus, as long as $Z_{\sC}(\cdot, \cdot, 0} \succ 0$, $\Pr(x_n | x_1)$ has full rank.
%
%- Answer: partial diagonal.
%- Lemma: if $M \succ 0$ and $M_{ij} > 0$ (i.e. $M$ is a probability distribution), then $1^T M, M 1 \succ 0$ as well.
%- Lemma: if $T \succ 0$ and $T_{ijk} > 0$, then $T(1, \cdot, \cdot) \succ 0$ as well, and continue by induction.
%        - is it possible to write out a condition for $\sigma_k$?
%

