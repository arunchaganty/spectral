\section{Proofs}
\label{app:proofs}

In the interest of space, we have omitted some proofs from
the main contents of the paper. 
We present their proofs in detail below.

\subsection{\lemmaref{pw-variance}}
\label{app:pw-variance-proof}

In \sectionref{piecewise}, we compare the asymptotic variance
  $\Sigma^\ml_\sC$ of the composite likelihood estimator for a clique
  $\sC$, with that of the pseudo-inverse estimator $\Sigma^\mom_\sC$. 
In this part, we will derive the asymptotic variance of the composite
  likelihood estimator.

\begin{proof}
  Recall that the composite likelihood estimator $\hat Z_\sC
  \in \Delta_{k^m-1}$ is a constrained M-estimator.
  First, let us re-parametrize the problem to make it unconstrained by
  using the first $k^m - 1$ elements of $Z_\sC$.  
  Let $\tilde Z_\sC \in [0,1]^{k^m-1}$ and let 
  \begin{align*}
    (Z_\sC)_{i_1 \ldots i_m} &= \left\{
      \begin{array}{ll}
        1 - \sum_{\vi} (\tilde Z_\sC)_{i_1 \ldots i_m} & i_1 = \cdots = i_m = k \\
        (\tilde Z_\sC)_{i_1 \ldots i_m} & \text{otherwise.}
        \end{array}
        \right.
  \end{align*}
  The objective can then be written as,
  \begin{align*}
    \sL_\ml &= \frac{1}{|\sD|} \sum_{\vx \in \sD} \ell(\vx) \\
    \ell(\vx) &= \log(\tilde Z_\sC \mOppAll_{\neg \vk}[\vx] + (1 - \tilde Z_\sC) \mOppAll_{\vk}[\vx]),
  \end{align*}
  where $\mOppAll_{\neg \vec k}[\vx] \in \Re^{k^m-1}$ is the first $k^m-1$
    entries of $\mOppAll[\vx]$ and
  $\mOppAll_{\vec k}[\vx] = \mOpp{v_1}{i_1}_{x_1k} \cdots
    \mOpp{v_m}{i_m}_{x_mk}$ is a constant. 
Let $\mOppTAll[\vx] = \mOppAll_{\neg \vk}[\vx] - \mOppAll_{\vk}[\vx]$, then,
  \begin{align*}
    \ell(\vx) &= \log(\tilde Z_\sC(\mOppTAll[\vx]) + \mOppAll_{\vk}[\vx]).
  \end{align*}

Using the delta-method \cite{vaart98asymptotic} we have that the asymptotic distribution of 
  $\widehat{{\tilde Z}}_\sC = \argmax_{\tilde Z_\sC \in [0,1]^{k^m-1}} \E[\ell(\vx)]$ is,
  \begin{align*}
    \sqrt{n}(\widehat{\tilde{Z}}_{\sC} - \tilde{Z}_{\sC}) 
      &\convind \sN( 0, \E[\grad^2 \ell^{-1}] \Var[\grad \ell] \E[\grad^2 \ell^{-1}]).
  \end{align*}

Taking the first derivative,
\begin{align}
  \grad_{\tilde Z_\sC} \ell(\vx)
  &= \frac{\mOppTAll[\vx]}{\mH_\sC(\mOppAll[\vx]) } \nonumber \\ 
  &= \mOppTAll \diag(M_{\sV})^{-1} e_\vx, \label{eqn:lhood-grad}
\end{align}
where $e_\vx$ is a one-hot vector and $M_\sV$ is the marginal
distribution of the observed data at the true parameters.

Taking the second derivative.
\begin{align}
  \grad^2_{\tilde \mH_\sC} \ell(\vx)
  &= - \frac{\mOppTAll[\vx] \mOppTAllt[\vx]}{(\mH_\sC(\mOppAll[\vx]))^2} \nonumber \\
  &= - \mOppTAll \diag(M_{\sV})^{-1} e_\vx e_\vx^\top  \diag(M_{\sV})^{-1} \mOppTAllt. \label{eqn:lhood-hess}%
\end{align}

From \equationref{lhood-grad} and \equationref{lhood-hess}, we get
  \begin{align*}
    \Var [\grad \ell(\vx)] &= \mOppTAll \diag(M_\sV)^{-1} \diag(\Sigma_\sV) \diag(M_\sV)^{-1} \mOppTAllt \\
    \E[\grad^2 \ell(\vx)] &= -\mOppTAll \diag(M_\sV)^{-1} \diag(\Sigma_\sV) \diag(M_\sV)^{-1} \mOppTAllt.
  \end{align*}
As expected, $\E[\grad^2 \ell(\vx)] = -\Var [\grad \ell(\vx)]$ because $\widehat{\tilde Z_\sC}$ is a maximum likelihood estimator. 
Finally, noting that $\Sigma_\sV = \diag(M_\sV) - M_\sV M_\sV^\top$ for
  a multinomial distribution, we get,
\begin{align*}
    \Sigma_{\tilde Z_\sC} 
      &= \E[\grad^2 \ell(\vx)]^{-1} \Var [\grad \ell(\vx)] \E[\grad^2 \ell(\vx)]^{-1} \\
      &= \Var[\grad \ell(\vx)]^{-1} \\
      &= \mOppTAllit \diag(\tilde M_\sV) \diag(\Sigma_\sV)^{-1} \diag(\tilde M_\sV) \mOppTAlli \\
      &= \mOppTAllit \diag(M_\sV) (\diag(M_\sV) - \diag(M_\sV)^2)^{-1} \diag(M_\sV) \mOppTAlli \\
      &= \mOppTAllit \diag(M_\sV) (I - \diag(M_\sV))^{-1} \mOppTAlli \\
      &= \mOppTAllit \tilde D_\sV \mOppTAlli,
\end{align*}
where $\tilde D_\sV = \diag( \frac{M_\sV[\vx]}{1 - M_\sV[\vx]} )$.
Note that given our assumptions, $\ones \succ M_\sV \succ \zeros$. Thus
the asymptotic variance is finite, and our estimator is consistent as
well.

% Note that by the definition of $\tilde Z_\sC$, we have the covariance
% between the first $k^m - 1$ elements of $Z_\sC$. 
% We now need to include the covariance with the last element,
%   $Z_\sC[\vk]$. 
% 
% It is easy to verify that for a vector $\alpha \in \Delta_{k-1}$, where
%   $\alpha_k = \sum_{i=1}^{k-1} \alpha_i$, the covariance between
%   $\alpha_i$ for $1 \le i \le k-1$ and $\alpha_k$ and that the variance
%   of $\alpha_k$ is,
% \begin{align*}
%     \Sigma_{\alpha} &= 
%     \begin{bmatrix}
%     \Sigma_{\tilde \alpha} & \Sigma_{\tilde \alpha} \ones \\
%     \ones^\top \Sigma_{\tilde \alpha} & \ones^\top \Sigma_{\tilde \alpha} \ones
%     \end{bmatrix}.
% \end{align*}
%\begin{align*}
%  \Cov[ \alpha_i, \alpha_k ] 
%    &= \E[ \alpha_i \alpha_k ] - \E[ \alpha_i ]\E[ \alpha_k ] 
%    &&= - \sum_{j=1}^{k-1} \Sigma_{ij} \\
%  \Var[ \alpha_k ] 
%    &= \E[ \alpha_k \alpha_k ] - \E[ \alpha_k ]^2 
%    &&= \sum_{i=1}^{k-1} \sum_{j=1}^{k-1} \Sigma_{ij}.
%\end{align*}

%Thus, the asymptotic variance of $Z_\sC$ is,
%\begin{align*}
%    \Sigma_{Z_\sC} &= 
%    \begin{bmatrix}
%    \Sigma_{\tilde Z_\sC} & \Sigma_{\tilde Z_\sC} \ones \\
%    \ones^\top \Sigma_{\tilde Z_\sC} & \ones^\top \Sigma_{\tilde Z_\sC} \ones
%    \end{bmatrix}.
%\end{align*}

%\todo{How do we compare $\Sigma^\mom$ with this? I need to compute the variance of projecting $\mom$}
%However, this matrix singular (the determinant is $0$).

\end{proof}

\subsection{Recovering conditional moments}

% Define up front that we will focus on h, x_1, x_2, x_3
In step 1 of \LearnMarginals, we used the bottleneck property of a hidden
  variable $h_i$ to learn conditional moments $\mOpp{v}{i}$ for every
  view $x_v \in \sV_{h_i}$ using \TensorFactorize. 
In order to do so, we require that \assumptionref{full-rank} holds, i.e.
\begin{assumption*}[1]
  Given a bottleneck $h_1$ with views $x_1, x_2, x_3$, the conditional
  moments $\mOpp{1}{1}, \mOpp{2}{1}, \mOpp{3}{1}$ have full column rank
  $k$, and $\pi\oft 1 \succ 0$.
\end{assumption*}

In this section, we will show that the following condition
  (\assumptionref{full-rank-plus}) on the clique marginals suffices;
\begin{assumption*}[2]
For every clique $\sC \in \sG$ (including ones involving observed variables),
  every $\Re^{k \times k}$ slice of the marginals $Z_\sC$ has full column
  rank $k$ constructed by summing out the remaining indices.
\end{assumption*}

%First, let us define some additional notation.
%
%\paragraph{Notation}
%
%Let $\sC = \{h_1, \cdots, h_m\}$ be a hidden clique in the graph, and
%  let $Z_\sC \eqdef \Pr(h_1, \cdots, h_m)$ be the tensor representing the
%  joint distribution. 
%We can obtain a marginal distribution for a sub-set $B \subset \sC$ by
%  marginalizing out the remaining variables. 
%Without loss of generality, let $|B| = m'$ and let the first $m'$
%  indices of $Z_\sC$ correspond to the $h_i \in B$; we can represent the
%  marginalization operation with linear algebra as $Z_B = Z_\sC(
%  \underbrace{I, \cdots, I}_{m' \text{times}}, \underbrace{\ones, \cdots, \ones}_{m - m'
%  \text{times}} )$. 
%
%Next, for a $m$-th order tensor $Z_\sC$ and $B \subset \sC$ with $m' \eqdef |B'|$, define the {\em
%partial diagonal} $\diag_{B}(Z_\sC)$ to be the $m - m' + 1$-th order tensor with
%a diagonal along the elements of $B$; $\diag_{B}(Z_\sC)[i, 
%i_{m'+1, \ldots, m}] = Z_\sC[\underbrace{i, \ldots, i}_{m' \text{times}}, i_{m'+1}, \ldots, m]$.
%
%
%\paragraph{Relation to clique marginals}

Let us describe $\mOpp{v}{1}$ in terms of the parameters of the model,
  namely the clique marginals $Z_\sC$.
Without loss of generality, let $x_v$ be the observed variable in
  consideration, and let $h_t$ be the unique parent of $x_v$.
Then, 
\begin{align*}
  \mOpp{v}{1} &\eqdef \Pr( x_v \given h_1 )  \\
              &= \sum_{h_t} \Pr( x_v \given h_t ) \Pr( h_t \given h_1 ) \\
              &= \mOpp{v}{t} \mYpp{t}{1},
\end{align*}
where $\mYpp{i}{j} \eqdef \Pr( h_i \given h_j )$. 
More generally, for two sets of hidden variables $C \eqdef \{h_{C_1}
\cdots h_{C_m} \}$ and $C' \eqdef \{h_{C'_1} \cdots h_{C'_m} \}$, 
define $\mYpp{C}{C'} \eqdef \Pr( h_{C_1} \cdots h_{C_m} \given h_{C'_1}
\cdots h_{C'_n} )$. 
With this representation, $\mYpp{C}{C'}$ is a $|C| + |C'|$-th order
tensor.
  
We will now give a recursive construction for $\mYpp{v}{1}$.
Without loss of generality, let $h_1, h_2, \cdots, h_t$ be a topological
  ordering rooted at $h_1$,
and let $\Pa(h)$ be the parents of a hidden variable $h$ in
  this topological ordering.
Then $\mYpp{t}{1}$ can be expressed recursively as,
\begin{align*}
  \mYpp{t}{1} &\eqdef \Pr( h_t \given h_1 )  \\
  &= \sum_{\vh} \Pr( \Pa(h_t) \given h_1 ) \Pr( h_t \given \Pa(h_t) ) \\
  &= \mYpp{ \Pa(h_t) }{1} \times_{\Pa(h_t)} \mYpp{t}{\Pa(h_t)},
\end{align*}
where $A \times_{C} B$ refers to summation along the indices $C$.

We can recursively describe $\mYpp{ \Pa(h_t) }{1}$ in terms of its
  parents until we reach $h_1$.
Let $C$ be an intermediate set of hidden variables. Let $h_c \neq h_1$
  be some hidden variable in $C$. 
Let $C' = C \union \Pa(h_c) \setminus \{h_c\}$ and $\del C = C'
  \setminus (C \union \{h_1\})$.
Then,
\begin{align*}
  \mYpp{C}{1} &\eqdef \Pr( \vh_C \given h_1 )  \\
  &= \sum_{\vh_{\del C}} \Pr( C' \given h_1 ) \Pr( h_c \given \Pa(h_c) ) \\
  &= \mYpp{ C' }{ 1 } \times_{\del C} \mYpp{c}{\Pa(h_c)},
\end{align*}
Finally, for the base case, $\mYpp{1}{1} = \ones$.
This procedure is algorithmically described in \algorithmref{Y}.

\begin{algorithm}
  \caption{$\mYpp{C}{1}$}
  \label{algo:Y}
  \begin{algorithmic}
    \REQUIRE The root $h_1$, a set of hidden variables $C$.
    \ENSURE The hidden moments distribution $\mYpp{C}{1}$.
    \IF{ $C = \{h_1\}$ }
      \STATE $\mYpp{1}{1} = \ones$.
    \ELSE
      \STATE Let $h_c \neq h_1$ be some hidden variable in $C$.
      \STATE Let $C' = C \union \Pa(h_c) \setminus \{h_c\}$.
      \STATE $\mYpp{C}{1} = \mYpp{C'}{1} \times_{\Pa(h_c)} \mYpp{c}{\Pa(h_c)}$.
    \ENDIF
  \end{algorithmic}
\end{algorithm}

This recursive definition along with \lemmaref{tensor-multiplication} allows us to to describe the
singular values of $\mYpp{C}{1}$ in terms of those of $\mYpp{C'}{1}$ and
$\mYpp{c}{\Pa(h_c)}$. 


Without loss of generality, let $\{c\} \union P = \sC$ for
a particular clique $\sC \in \sG$\footnote{If they are actually composed of two
cliques $\sC_1, \sC_2$, we can construct $Z_{\sC_1 \union \sC_2} = Z_{\sC_1} \cdot
Z_{\sC_2}$.}. Then, 
\begin{align*}
\mYpp{c}{\Pa(h_c)} 
  &\eqdef \Pr(h_c \given \Pa(h_c)) \\
  &= \frac{\Pr(h_c, \Pa(h_c))}{\Pr(\Pa(h_c))} \\
  &= Z_\sC \diag{Z_\sC(\ones, \cdot, \ldots, \cdot)}^{-1}.
\end{align*}

\begin{lemma}[Sufficiency of \assumptionref{full-rank-plus}]
  \label{lem:full-rank-suff}
  Given that \assumptionref{full-rank-plus} holds, then for any hidden
  variable $h_1$ and observed variable $x_v$, $\mPi{1} \succ 0$ and
  $\mOpp{v}{i}$ has full column rank.
\end{lemma}
\begin{proof}
  The important statement that needs to be made is how $\sigma_k(Y)$
  relates to its parents.
\end{proof}

\begin{lemma}[Tensor Multiplication and Singular Values]
  \label{lem:tensor-multiplication}
Let $\sP, \sQ, \sR$ and $M$ be defined as above. 
Let $I \subset [m - k + n - k]$ be an arbitrary subset of the index set of $\sR$,
  and let $I_1$, $I_2$ be the respective subsets of $I$ contained in $\sP$ and $\sQ$.
Then,
\begin{align*}
  \sigma_{\max}(\sP\munf{I_1}) \sigma_{\max}(\sP\munf{I_2}) 
&\ge \sigma_{\max}(\sR\munf{I})  \\
&\ge \sigma_{\min}(\sR\munf{I}) 
\sigma_{\min}(\sP\munf{I_1}) \sigma_{\min}(\sP\munf{I_2}).
\end{align*}
\end{lemma}
\begin{proof}
  Without loss of generality, let $M = \{(1,1), \ldots, (\ell,\ell)\}$. 
  Then,
  \begin{align*}
    \sR_{\vi\vj} &\eqdef \sum_{\vk} \sP_{\vk \vi} \sQ_{\vk \vj}.
  \end{align*}

Consider an $(I)$-unfolding of $\sR$,
  \begin{align*}
    \sR\munf{I}_{\vi,\vj} 
       &= \sum_{\vk} \sP_{\vk \vi} \sQ_{\vk \vj} \\
       &= \sum_{\vk} \sP\munf{I_1}_{\vx_1; \vy_1 \vk} \sQ\munf{I_2}_{\vx_2; \vy_2, \vk},
  \end{align*}
  where $\vx_1 \in \Range(I_1)$, $\vy_1 \in \Range([m] \setminus I_1)$, $\vx_2 \in \Range(I_2)$ and $\vy_2  \in \Range([n] \setminus I_2)$.

The key idea is to represent the above as a matrix multiplication.
To do so, we can temporarily ``inflate'' the unfoldings $\sP\munf{I_1}$
  and $\sQ\munf{I_2}$ by taking the outer product of $\sQ\munf{I_2}$ with
  identity matrices. Let us define 
  $\sP' \eqdef \sP \otimes I_{d_{j'_1}} \otimes \cdots \otimes I_{d_{j'_{|\vj'|}}}$ and 
  $\sQ' \eqdef \sQ \otimes I_{d_{i'_1}} \otimes \cdots \otimes I_{d_{i'_{|\vi'|}}}$.

Now, $\sP'$ is a $(m + 2 * |\vj'|)$-th order tensor and similarly $\sQ'$ is a $(n + 2 * |\vi'|)$-th 
  order tensor. We choose new index sets $I'_1$ and $I'_2$ to include these identity indices. 
Then,
  \begin{align*}
    \sR\munf{I}_{\vi,\vj} 
    &= \sum_{\vy_1, \vy_2 \vk}
    {\sP'}\munf{I'_1}_{\vx_1\vy'_2, \vy_1 \vy_2 \vk} {\sQ'}\munf{I'_2}_{\vx_2\vy'_1, \vy_1 \vy_2 \vk} \\
          \sR\munf{I} &= {\sP'}\munf{I'_1} {\sQ'}\munf{I'_2}.
  \end{align*}

  Finally, we can use \equationref{matrix-singular} to bound the singular values of $\sR\munf{I}$:
  \begin{align*}
    \sigma_{\max}(\sR\munf{I}) 
    &\le \sigma_{\max}({\sP'}\munf{I'_1}) \sigma_{\max}({\sQ'}\munf{I'_2}) \\
      &= \sigma_{\max}(\sP\munf{I_1}) \sigma_{\max}(\sQ\munf{I_2}) \\
    \sigma_{\min}(\sR\munf{I})  
    &\ge \sigma_{\min}({\sP'}\munf{I'_1}) \sigma_{\min}({\sQ'}\munf{I'_2}) \\
      &\ge \sigma_{\min}(\sP\munf{I_1}) \sigma_{\min}(\sQ\munf{I_2}).
  \end{align*}

  We used the well-known property that the singular values of the tensor product of two matrices is equal to the product of singular values of the matrices. \lemmaref{tensor-prod} presents a proof of this fact for completeness.
\end{proof}


% \begin{proof}
% Let us explicitly write out $\mPi{1}, \mOpp{1}{1}, \mOpp{2}{1},
% \mOpp{3}{1}$ in terms of the clique marginals \{ $Z_\sC \given \sC \in \sG \}$.
% 
% \begin{figure}[t]
%   \label{fig:assumption-2}
%   \centering
%   \input{figures/assumption.tikz}
%   \caption{$\mOpp{v}{1}$ can be estimated by considering marginal distributions along any path from $h_1$ to $x_v$}
% \end{figure}
% 
% Let $\sC_0 = \{h_1, \cdots, h_m\}$ be any clique containing
%   $h_1$. $\mPi{1}$ can be obtained from $Z_\sC$ by marginalizing the
%   remaining hidden variables in $\sC$, i.e. $\mPi{1} = Z_{\sC_0}(\cdot,
%   \ones, \cdots, \ones)$.
% Given that $\mPi{1}$ is strictly greater diagonal of some slice of $Z_{\sC_0}$,
%   \assumptionref{full-rank-plus} guarantees that $\mPi{1} \succ 0$.
% 
% Expression $\mOpp{v}{1} = \Pr(x_v \given h_1)$ is a bit more
%   complicated. 
% By the reduction presented in \lemmaref{reduction}, we can assume
%   w.l.o.g. that the observed variables only appear as leaf nodes with
%   a single parent. 
% Let $h_v$ be the parent of $x_v$, and let $h_{i_1}, h_{i_2}, \cdots, h_{i_l}$ be any
%   path from $h_1 \eqdef h_{i_1}$ to $h_v \eqdef h_{i_l}$.
% Then,
% \begin{align*}
%   \mOpp{v}{1} &\eqdef \Pr( x_v \given h_1 )  \\
%               &= \sum_{h_v} \Pr( x_v \given h_v ) \Pr( h_v \given h_1 ) \\
%               &= \sum_{h_{i_l}} \Pr( x_v \given h_{i_l} ) \sum_{h_{i_{l-1}}} \Pr( h_{i_{l}} \given h_{i_{l-1}} ) \Pr( h_{i_{l-1}} \given h_1 ) \\
%               &= \sum_{\vh} \Pr( x_v \given h_{i_l} ) \Pr( h_{i_{l}} \given
%               h_{i_{l-1}} ) \cdots \Pr( h_{i_2} \given h_{i_1} ).
% \end{align*}
% 
% Let $C_{i_j}$ be the clique containing $h_{i_j}$ and $h_{i_{j-1}}$. 
% The conditional distribution $\Pr( h_{i_j} \given h_{i_{j-1}} )$ can be
%   got by first marginalizing all other hidden variables in $\sC$ (achieved by
%   multiplying those indices by $\ones$), and then by normalizing by $\Pr(h_{i_{j-1}})$ (achieved by multiplying by the matrix $\inv{\diag(\mPi{i_{j-1}})}$); \todo{Better notation}
% \begin{align*}
%   Y_{j \given j-1} 
%     &\eqdef \Pr( h_{i_j} \given h_{i_{j-1}} ) \\
%     &= Z_{\sC_{i_j}}(\cdot, \cdot, \underbrace{\ones, \cdots, \ones}_{h \neq h_{i_j}, h_{i_{j-1}}})
%         \inv{\diag(\mPi{i_{j-1}})}.
% \end{align*}
% 
% Finally, we have the following expression,
% \begin{align*}
%   \mOpp{v}{1} &= \mOpp{v}{v} Y_{l \given l-1} \cdots Y_{2 \given 1}.
% \end{align*}
% 
% Note that \assumptionref{full-rank-plus} guarantees that $Y_{j \given
%   j -1}$ is full rank, from which it directly follows that $\mOpp{v}{1}$, completing our proof.
% \end{proof}
% 
% \subsection{Sample Complexity}
% 
% In this section, we will explicitly derive the sample complexity of
%   $\LearnMarginals$. The end result is an estimate of the error of
%   each clique marginal $Z_\sC$.
% The analysis proceeds in two stages; (i) bounding the error in
%   estimating in each conditional moment $\mOpp{v}{i}$ and (ii) bounding
%   the error in estimating $Z_\sC$.
% 
% \paragraph{Estimating $\mOpp{v}{i}$}
% 
% Consider the estimation of the conditional moments for a bottleneck
%   $h_i$ with views $x_{v_1}, x_{v_2}, x_{v_3}$. 
% Let the error in estimation of the moments be upper-bounded by
%   $\epsilon$, $\|M_{v_i v_j} - \hat M_{v_i v_j} \|_F < \epsilon$ for $i,
%   j \in \{1,2,3\}$, and 
% the singular values of the moments be lower-bounded by $\alpha$,
%   $\sigma_k(M_{v_i v_j}) > \alpha$ for $i, j \in \{1,2,3\}$.
% Results from \citet{anandkumar12moments,anandkumar13tensor} show that 
% \begin{align*}
%   \|\mOpphat{v_1}{i} - \mOppit{v_1}{i}\|^2_F 
%     &= O( 
%     \frac{k {\mPi{i}}_{\max}^2}
%     {\beta^5 } \epsilon ). 
% \end{align*}
% We will now express this quantity in terms of the parameters of the model.
% 
% Let $\sigma_1(Z_\sC)$ and $\sigma_k(Z_\sC)$ respectively be the largest
%   and smallest singular value of any slice of $Z_\sC$. 
% From the derivation in \lemmaref{full-rank-suff}, we know that $\mPi{i}$
%   is obtained by marginalizing some slice of $Z_{\sC_j}$, which gives
%   the result that $\sigma_1(Z_\sC) \succ \mPi^{i} \succ \sigma_k(Z_\sC)$.
% Furthermore, 
% \begin{align*}
%   \sigma_k(Y_{j \given j-1}) &\ge \frac{\sigma_k(Z_{\sC_j})}{\mPi{i_{j-1}}_{\max}}
%                              \ge \frac{\sigma_k(Z_{\sC_j})}{\sigma_1(Z_{\sC_j})}
%                              = \cnd(Z_{\sC_j})^{-1},
% \end{align*}
% where $\cnd(Z_{\sC_j}) \eqdef
% \frac{\sigma_1(Z_{\sC_j})}{\sigma_k(Z_{\sC_j})}$ is defined analogously
% to the condition number of a matrix. 
% Finally,
% \begin{align*}
%   \sigma_k(\mOpp{v}{1}) 
%     &\ge \sigma_k(\mOpp{v}{v}) \prod_{j=2}^{l} \sigma_k(Y_{j \given j-1}) \\
%     &\ge \sigma_k(\mOpp{v}{v}) \prod_{j=2}^{l} \cnd(\sC_{j}).
% \end{align*}
% 
% Note that $M_{v_a v_b} = \mOpp{v_a}{i} \diag(\mPi{i})
% \mOpp{v_b}{i}^\top$.
% Without loss of generality, let us assume that $M_{v_a v_b}$ is the
%   moment with the smallest singular value.
% This gives us the following bound on $\beta$,
% \begin{align*}
%   \beta &\ge \sigma_k(M_{v_a,v_b}) \\
%         &\ge \sigma_k(\mOpp{v_a}{i}) \mPi{i}_{\min} \sigma_k(\mOpp{v_b}{i}) \\
%         &\ge \sigma_k(\mOpp{v_a}{v_a}) \sigma_k(\mOpp{v_b}{v_b}) \sigma_k(Z_{\sC_i}) \cnd_{\min}^{d_a + d_b},
% \end{align*}
% where $\cnd_{\min}$ is the smallest $\cnd(Z_\sC)$ for any $\sC \in \sG$
% and $d_a, d_b$ are distance from $h_i$ to $x_{v_a}$ and $x_{v_b}$
% respectively.
% 
% Finally, we have the result,
% \begin{align*}
%   \|\mOpphat{v_1}{i} - \mOppit{v_1}{i}\|^2_F 
%     &= O( 
%     \frac{k \sigma_1^2 }
%     {(\sigma_k^3 \cnd_{\min}^{d_a + d_b})^5}
%     \epsilon ). 
% \end{align*}
% 
% \paragraph{Estimating $Z_\sC$}
% 
% Similarly, we can apply standard perturbation analysis techniques to get
% an error bound on $Z_\sC$;
% \begin{align*}
%   \|Z_\sC - \hat Z_\sC\|_F 
%   &\le \|M_\sV(\mOppit{v_1}{i_1}, \cdots, \mOppit{v_m}{i_m}) \\
%       &\quad - \hat M_\sV(\hat{\mOppit{v_1}{i_1}}, \cdots, \hat{\mOppit{v_m}{i_m}})\|^2_F.
% \end{align*}
% Let $\|\mOppit{v_1}{i_1}\|_F < O$ and $\|\hat{\mOppit{v_1}{i_1}}\|_F < O$; then, we get
% \begin{align*}
%   \|Z_\sC - \hat Z_\sC\|_F 
%   &\le \|M_\sV - \hat M_\sV\|_F O^m \\
%   &\quad + \|\hat M_\sV\|_F O^{m-1} \max\{\|\hat {\mOppit{v_1}{i_1}} - \mOppit{v_1}{i_1}\|_F\}.
% \end{align*}

% Let's return to the X = {X_{ijk}}, Y = Y_{lm} example. We defined Z_{jkm} = \sum_i X_{ijk} Y_{im}.
% 
% Let's look at some mode unfolding of Z - we want to show that each of these is full rank (and we care about their singular values).
% 
% First, consider the unfolding along 'm' - I'm going to single out indices using (Z_m)_jk to be a little clearer with text math:
% 
% ([Z_(3)]_m)_jk = (Z_m)_{jk}
%                      = (Y_m)_{i} (X_i)_{jk}
%                      =  Y_(2) * X_(1)'
% 
% In other words, this is just the product of the two mode unfoldings and this is going to have full rank (because each has full rank). This is clearly the easy case.
% 
% Next, consider
% 
% ([Z_(1)]_j)_km = (Z_j)_{km}
%                      = \sum_i (X_{j})_{ik} Y_{im}.
% 
% The trick is to take the outer product of Y and the identity matrix. Then we get a new "Y" that looks like Y_{ikk'm} = Y_{im} \delta_{kk'}. Then,
% 
% ([Z_(1)]_j)_km = (Z_j)_{km}
%                      = \sum_{ik} (X_j)_{ik} * (Y_{ik})_{k'm}
%                      = X_(2) * Y_(12),
% 
% the product of these two unfoldings. It's easy to show that rank Y_(12)  = rank(Y) because of the property that sigma(A \otimes B) = \sigma(A) \sigma(B).
