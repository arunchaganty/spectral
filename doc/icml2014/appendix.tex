\section{Proofs}
\label{app:proofs}

In the interest of space, we have omitted some proofs from
the main contents of the paper. 
We present their proofs in detail below.

\subsection{\lemmaref{pw-variance}}
\label{app:pw-variance-proof}

In \sectionref{piecewise}, we compare the asymptotic variance $\Sigma^\ml_\sC$ of the
composite likelihood estimator for a clique $\sC$, $\hat Z^\ml_\sC$, with
that of the pseudo-inverse estimator $\Sigma^\mom_\sC$. 
In this part, we will derive the asymptotic variance of the composite
  likelihood estimator.

\begin{proof}
  Recall that the composite likelihood estimator $\hat Z_\sC
  = \argmax_{Z_\sC \in \Delta_{k^m-1}} \hat\E[\log Z_\sC(\mOppAll[\vx])]$ is
  a constrained M-estimator.
  First, let us re-parametrize the problem to make it unconstrained by
  using the first $k^m - 1$ elements of $Z_\sC$.  
  Let $\tilde Z_\sC \in [0,1]^{k^m-1}$ and let 
  \begin{align*}
    (Z_\sC)_{i_1i_2i_3} &= \left\{
      \begin{array}{ll}
        1 - \sum_{i_1,i_2,i_3} (\tilde Z_\sC)_{i_1i_2i_3} & i_1 = i_2 = i_3 = k \\
        (\tilde Z_\sC)_{i_1i_2i_3} & \text{otherwise}
        \end{array}
        \right.
        .
  \end{align*}
  Using the delta-method \cite{vaart98asymptotic} we have that the asymptotic distribution of 
  $\widehat{{\tilde Z}}_\sC = \argmax_{\tilde Z_\sC \in [0,1]^{k^m-1}} \hat\E[\log Z_\sC(\mOppAll[\vx])]$ is,
  \begin{align*}
    \sqrt{n}(\widehat{\tilde{Z}}_{\sC} - \tilde{Z}_{\sC}) &\convind \sN( 0, \grad^2 \sL_\ml^{-1} \Var[\grad \sL_\ml] \grad^2 \sL_\ml^{-1}).
  \end{align*}

Taking the first derivative,
\begin{align}
  \grad_{\tilde Z_\sC} \sL_\ml(\sX_\sV) 
  &= \sum_{x \in \sD} \frac{\mOppAll_{\neg \vec k}[\vx] - \mOppAll_{\vec k}[\vx]}{\mH_\sC(\mOppAll[\vx]) } \nonumber \\ 
  &= \mOppTAll \diag(\tilde \mO_{\sV})^{-1} \mO_{\sV}, \label{eqn:lhood-grad}
\end{align}
where $\mOppAll_{\neg \vec k}[\vx] \in \Re^{k^m-1}$ is the first $k^m-1$
entries of $\mOppAll[\vx]$, $\mOppAll_{\vec k}[\vx] = \mOpp{v_1}{i_1}_{x_1k}
\cdots \mOpp{v_m}{i_m}_{x_mk}$ is a constant and $\tilde \mO_\sV$ is
marginal distribution with parameters $\mH_\sC$.

Taking the second derivative.
\begin{align}
  \grad^2_{\tilde \mH_\sC} \sL_\ml(\Sx \sV) 
  &= \sum_{x \in \sD} \frac{\mOppTAll[\vx] \mOppTAllt[\vx]}{(\mH_\sC(\mOppAll[\vx]))^2} \nonumber \\
  &= \sum_{x \in \sD}\mOppTAll[\vx] \mOppTAllt[\vx] \frac{\mO_{\sV}[\vx]}{\tilde \mO_{\sV}^2[\vx]} \nonumber \\
  &= \mOppTAll \diag(\mO_{\sV}) \diag(\tilde \mO_{\sV})^{-2} \mOppTAllt. \label{eqn:lhood-hess}%
\end{align}

  From \equationref{lhood-grad}, we get
  \begin{align*}
    \Var [\grad \sL_\ml(\vec x_\sC)] &= \mOppTAll \diag(\tilde M_\sV) \Sigma_\sV \diag(\tilde M_\sV) \mOppTAll^T .
  \end{align*}

  Finally, using \equationref{lhood-hess}, we have
  \begin{align*}
    \Sigma_{\tilde Z_\sC} 
      &= \grad^2 \sL_\ml(\vec x_\sC)^{-1} \Var [\grad \sL_\ml(\vec x_\sC)] \grad^2 \sL_\ml(\vec x_\sC)^{-1}) \\
      &= \pinvt{\mOppTAll} \diag(\tilde M_\sV) \Sigma_\sV \diag(\tilde M_\sV) \pinv{\mOppTAll}.
  \end{align*}

Note that by the definition of $\tilde Z_\sC$, we have the covariance
between the first $k^m - 1$ elements of $Z_\sC$. 
We now need to include the covariance with the last element,
  $(Z_\sC)_{\vec k}$. 

It is easy to verify that for a vector $\alpha \in \Delta_{k-1}$, where
  $\alpha_k = \sum_{i=1}^{k-1} \alpha_i$, the covariance between
  $\alpha_i$ for $1 \le i \le k-1$ and $\alpha_k$ and that the variance
  of $\alpha_k$ are,
\begin{align*}
  \Cov[ \alpha_i, \alpha_k ] 
    &= \E[ \alpha_i \alpha_k ] - \E[ \alpha_i ]\E[ \alpha_k ] \\ 
    &= - \sum_{j=1}^{k-1} \Sigma_{ij} \\
  \Var[ \alpha_k ] 
    &= \E[ \alpha_k \alpha_k ] - \E[ \alpha_k ]^2 \\ 
    &= \sum_{i=1}^{k-1} \sum_{j=1}^{k-1} \Sigma_{ij}.
\end{align*}

Thus, the asymptotic variance of $Z_\sC$ is the following,
\begin{align*}
    \Sigma_{Z_\sC} &= 
    \begin{bmatrix}
    \Sigma_{\tilde Z_\sC} & \Sigma_{\tilde Z_\sC} \ones \\
    \ones^\top \Sigma_{\tilde Z_\sC} & \ones^\top \Sigma_{\tilde Z_\sC} \ones
    \end{bmatrix}.
\end{align*}



  At the true parameters, $\tilde M_\sV = M_\sV$, completing the proof.
%  \todo{argue that asymptotic variance is finite, so the estimator is consistent (this is technically good form,
%but it's fine given space constraints}
\end{proof}

\subsection{Recovering conditional moments}

% Define up front that we will focus on h, x_1, x_2, x_3
In step 1 of \LearnMarginals, we used the bottleneck property of a hidden
  variable $h_i$ to learn conditional moments $\mOpp{v}{i}$ for every
  view $x_v \in \sV_{h_i}$ using \TensorFactorize. 
In order to do so, we require that \assumptionref{full-rank} holds, i.e.
\begin{assumption*}
  Given a bottleneck $h_1$ with views $x_1, x_2, x_3$, the conditional
  moments $\mOpp{1}{1}, \mOpp{2}{1}, \mOpp{3}{1}$ have full column rank
  $k$, and $\pi\oft 1 \succ 0$.
\end{assumption*}

In this section, we will show that the following condition
  (\assumptionref{full-rank-plus}) on the clique marginals suffices;
\begin{assumption*}
For every clique $\sC \in \sG$ (including ones involving observed variables),
  every $\Re^{k \times k}$ slice of the marginals $Z_\sC$ has full column
  rank $k$ constructed by summing out the remaining indices.
\end{assumption*}

%First, let us define some additional notation.
%
%\paragraph{Notation}
%
%Let $\sC = \{h_1, \cdots, h_m\}$ be a hidden clique in the graph, and
%  let $Z_\sC \eqdef \Pr(h_1, \cdots, h_m)$ be the tensor representing the
%  joint distribution. 
%We can obtain a marginal distribution for a sub-set $B \subset \sC$ by
%  marginalizing out the remaining variables. 
%Without loss of generality, let $|B| = m'$ and let the first $m'$
%  indices of $Z_\sC$ correspond to the $h_i \in B$; we can represent the
%  marginalization operation with linear algebra as $Z_B = Z_\sC(
%  \underbrace{I, \cdots, I}_{m' \text{times}}, \underbrace{\ones, \cdots, \ones}_{m - m'
%  \text{times}} )$. 
%
%Next, for a $m$-th order tensor $Z_\sC$ and $B \subset \sC$ with $m' \eqdef |B'|$, define the {\em
%partial diagonal} $\diag_{B}(Z_\sC)$ to be the $m - m' + 1$-th order tensor with
%a diagonal along the elements of $B$; $\diag_{B}(Z_\sC)[i, 
%i_{m'+1, \ldots, m}] = Z_\sC[\underbrace{i, \ldots, i}_{m' \text{times}}, i_{m'+1}, \ldots, m]$.
%
%
%\paragraph{Relation to clique marginals}

Let us describe $\mOpp{v}{1}$ in terms of the parameters of the model,
  namely the clique marginals $Z_\sC$.
Without loss of generality, let $x_v$ be the observed variable in
  consideration, and let $h_t$ be the unique parent of $x_v$.
Then, 
\begin{align*}
  \mOpp{v}{1} &\eqdef \Pr( x_v \given h_1 )  \\
              &= \sum_{h_t} \Pr( x_v \given h_t ) \Pr( h_t \given h_1 ) \\
              &= \mOpp{v}{t} \mYpp{t}{1},
\end{align*}
where $\mYpp{i}{j} \eqdef \Pr( h_i \given h_j )$. More generally, for two sets of hidden variables $C \eqdef \{h_{C_1} \cdots h_{C_m} \}$ and $C' \eqdef \{h_{C'_1} \cdots h_{C'_m} \}$, define $\mYpp{C}{C'} \eqdef \Pr( h_{C_1} \cdots h_{C_m} \given h_{C'_1} \cdots h_{C'_n} )$. 
  
We will now give a recursive construction for $\mYpp{v}{1}$.
Without loss of generality, let $h_1, h_2, \cdots, h_t$ be a topological
  ordering rooted at $h_1$,
and let $\Pa(h)$ be the parents of a hidden variable $h$ in
  this topological ordering.
Then $\mYpp{t}{1}$ can be expressed recursively as,
\begin{align*}
  \mYpp{t}{1} &\eqdef \Pr( h_t \given h_1 )  \\
  &= \sum_{\vh} \Pr( \Pa(h_t) \given h_1 ) \Pr( h_t \given \Pa(h_t) ) \\
  &= \mYpp{ \Pa(h_t) }{1} \times_{\Pa(h_t)} \mYpp{t}{\Pa(h_t)},
\end{align*}
where $A \times_{C} B$ refers to summation along the indices $C$.

We can recursively describe $\mYpp{ \Pa(h_t) }{1}$ in terms of its
  parents until we reach $h_1$.
Let $C$ be an intermediate set of hidden variables. Let $h_c \neq h_1$
  be some hidden variable in $C$. 
Let $C' = C \union \Pa(h_c) \setminus \{h_c\}$.
Then,
\begin{align*}
  \mYpp{C}{1} &\eqdef \Pr( \vh_C \given h_1 )  \\
  &= \sum_{\vh_{\Pa(h_c)}} \Pr( C' \given h_1 ) \Pr( h_c \given \Pa(h_c) ) \\
  &= \mYpp{ C' }{ 1 } \times_{\Pa(h_c)} \mYpp{c}{\Pa(h_c)},
\end{align*}

Finally, for the base case, $\mYpp{1}{1} = \ones$.
  
\begin{algorithm}
  \caption{$\mYpp{C}{1}$}
  \label{algo:Y}
  \begin{algorithmic}
    \REQUIRE The root $h_1$, a set of hidden variables $C$.
    \ENSURE The hidden moments distribution $\mYpp{C}{1}$.
    \IF{ $C = \{h_1\}$ }
      \STATE $\mYpp{1}{1} = \ones$.
    \ELSE
      \STATE Let $h_c \neq h_1$ be some hidden variable in $C$.
      \STATE Let $C' = C \union \Pa(h_c) \setminus \{h_c\}$.
      \STATE $\mYpp{C}{1} = \mYpp{C'}{1} \times_{\Pa(h_c)} \mYpp{c}{\Pa(h_c)}$.
    \ENDIF
  \end{algorithmic}
\end{algorithm}

Without loss of generality, let $\{c\} \union P \subset \sC$ for
a particular clique $\sC \in \sG$. If they are actually composed of two
cliques $C_1, C_2$, we can construct $Z_{C_1 \union C_2} = Z_{C_1} \dot
Z_{C_2}$. Furthermore, we can marginalize out any variables in $\sC
\setminus \bar{P}$ - they are conditionally independent of $h_t$.

\begin{lemma}[Sufficiency of \assumptionref{full-rank-plus}]
  \label{lem:full-rank-suff}
  Given that \assumptionref{full-rank-plus} holds, then for any hidden
  variable $h_1$ and observed variable $x_v$, $\mPi{1} \succ 0$ and
  $\mOpp{v}{i}$ has full column rank.
\end{lemma}
\begin{proof}
  The important statement that needs to be made is how $\sigma_k(Y)$
  relates to its parents.

\end{proof}

Let us review some important properties of the Tucker decomposition of
  a tensor.
For an $m$-th order tensor $P \in \Re^{k^m}$, let $P = \sG
  \times_{n=1}^N \Un{n}$ be its Tucker decomposition, where $\sG \in
  \Re^{k^m}$ is its {\em core tensor} and $\Un{n} \in \Re^{k\times k}$ is
  an orthogonal matrix \cite{kolda2009tensor}.
For any sub-tensors $\sG_{i_n = p}$, $\sG_{i_n = q}$, $\innerp{\sG_{i_n
  = p}}{\sG_{i_n = q}} = 0$ if $p \neq q$ and $\|\sG_{i_n = p}\|_F
  = \sigma_{n,p}(P)$.

\begin{property}[Tucker Decomposition and Multiplication]
  Let $P$ and $Q$ be an m-th and n-th order tensors with Tucker decompositions,
  $P = M \times_{i=1}^m \Un{i}$ and
  $Q = N \times_{i=1}^n \Vn{i}$.
  Then, for any index set $I$ of size $k$, 
  $R \eqdef O \times_{I} Q$ has a tucker decomposition,
  $R = \sR \times_{i=1}^{m+n-k} \Wn{i}$. 
  
  Without loss of generality, assume that $I = \{1, \cdots, k\}$, then 
  \begin{align*}
    \Wn{i} &= 1 \left\{
    \begin{array}{l l}
      {\Un{i}}^\top \Vn{i} & i \in [1, k] \\
      \Un{i}              & i \in [k+1, m] \\
      \Vn{i - m + k}      & i \in [m+1, m+n+k]
    \end{array}
    \right.
  \end{align*}

  Furthermore,
  \begin{align*}
    \sigma_{i,p}(R) &= \left\{
    \begin{array}{l l}
      \sigma_{i,p}(P) \sigma_{i,k}(Q) & i \in [1, k] \\
      \sigma_{i,p}(P) & i \in [k+1, m] \\
      \sigma_{i - m + k,p}(P) & i \in [m+1, m+n+k]
    \end{array}
    \right.
  \end{align*}
\end{property}
\begin{proof}
  As a warm-up, consider the simple case where $m = 3$ and $n = 2$.
  Here, we have the following decompositions,
  \begin{align*}
    P &= \sum_{i,j,k} M_{ijk} \Un{1}_{i} \otimes \Un{2}_{j} \otimes \Un{3}_{k} \\
    Q &= \sum_{l,m} N_{lm} \Vn{1}_{l} \otimes \Vn{2}_{m}.
  \end{align*}

  Now, we will try to express $R \eqdef P \times_{1} Q$,
  \begin{align*}
    R_{bcd} &= \sum_{a} P_{abc} Q_{ad} \\
    &= \sum_{a} \sum_{i,j,k} \sum_{l,m} M_{ijk} \un{1}_{ia} \un{2}_{jb} \un{3}_{kc} 
                N_{lm} \vn{1}_{la} \vn{2}_{md} \\
                &= \sum_{a} \sum_{i,j,k} \sum_{m} M_{ijk} (\sum_{l} \un{1}_{ia} N_{lm} \vn{1}_{la}) 
                    \un{2}_{jb} \un{3}_{kc} \vn{2}_{md} \\
                    &= \sum_{i,j,k} \sum_{m} M_{ijk} 
                      \sum_{a} (\un{1}_{i} \circ \sum_{l} N_{lm} \vn{1}_{l})_{a}
                    \un{2}_{jb} \un{3}_{kc} \vn{2}_{md} \\
                    &= \sum_{i,j,k} \sum_{m} M_{ijk} 
                    ({\un{1}_{i}}^\top \sum_{l} N_{lm} \vn{1}_{l}) \un{2}_{jb} \un{3}_{kc} \vn{2}_{md} \\
                    &= \sum_{i,j,k} \sum_{m} M_{ijk} n_{im} \un{2}_{jb} \un{3}_{kc} \vn{2}_{md} \\
  \end{align*}
  Let ${\Wn{1}}'$ be composed of columns of ${\wn{1}}'_{ia}$, i.e.
  ${\Wn{1}}' \eqdef \Un{1} \diag N_m \Vn{1}$. 
  In other words, from the HOSVD procedure, we can absorb $\Un{1} \diag N_m$ into the core tensor, and 


\end{proof}



% \begin{proof}
% Let us explicitly write out $\mPi{1}, \mOpp{1}{1}, \mOpp{2}{1},
% \mOpp{3}{1}$ in terms of the clique marginals \{ $Z_\sC \given \sC \in \sG \}$.
% 
% \begin{figure}[t]
%   \label{fig:assumption-2}
%   \centering
%   \input{figures/assumption.tikz}
%   \caption{$\mOpp{v}{1}$ can be estimated by considering marginal distributions along any path from $h_1$ to $x_v$}
% \end{figure}
% 
% Let $\sC_0 = \{h_1, \cdots, h_m\}$ be any clique containing
%   $h_1$. $\mPi{1}$ can be obtained from $Z_\sC$ by marginalizing the
%   remaining hidden variables in $\sC$, i.e. $\mPi{1} = Z_{\sC_0}(\cdot,
%   \ones, \cdots, \ones)$.
% Given that $\mPi{1}$ is strictly greater diagonal of some slice of $Z_{\sC_0}$,
%   \assumptionref{full-rank-plus} guarantees that $\mPi{1} \succ 0$.
% 
% Expression $\mOpp{v}{1} = \Pr(x_v \given h_1)$ is a bit more
%   complicated. 
% By the reduction presented in \lemmaref{reduction}, we can assume
%   w.l.o.g. that the observed variables only appear as leaf nodes with
%   a single parent. 
% Let $h_v$ be the parent of $x_v$, and let $h_{i_1}, h_{i_2}, \cdots, h_{i_l}$ be any
%   path from $h_1 \eqdef h_{i_1}$ to $h_v \eqdef h_{i_l}$.
% Then,
% \begin{align*}
%   \mOpp{v}{1} &\eqdef \Pr( x_v \given h_1 )  \\
%               &= \sum_{h_v} \Pr( x_v \given h_v ) \Pr( h_v \given h_1 ) \\
%               &= \sum_{h_{i_l}} \Pr( x_v \given h_{i_l} ) \sum_{h_{i_{l-1}}} \Pr( h_{i_{l}} \given h_{i_{l-1}} ) \Pr( h_{i_{l-1}} \given h_1 ) \\
%               &= \sum_{\vh} \Pr( x_v \given h_{i_l} ) \Pr( h_{i_{l}} \given
%               h_{i_{l-1}} ) \cdots \Pr( h_{i_2} \given h_{i_1} ).
% \end{align*}
% 
% Let $C_{i_j}$ be the clique containing $h_{i_j}$ and $h_{i_{j-1}}$. 
% The conditional distribution $\Pr( h_{i_j} \given h_{i_{j-1}} )$ can be
%   got by first marginalizing all other hidden variables in $\sC$ (achieved by
%   multiplying those indices by $\ones$), and then by normalizing by $\Pr(h_{i_{j-1}})$ (achieved by multiplying by the matrix $\inv{\diag(\mPi{i_{j-1}})}$); \todo{Better notation}
% \begin{align*}
%   Y_{j \given j-1} 
%     &\eqdef \Pr( h_{i_j} \given h_{i_{j-1}} ) \\
%     &= Z_{\sC_{i_j}}(\cdot, \cdot, \underbrace{\ones, \cdots, \ones}_{h \neq h_{i_j}, h_{i_{j-1}}})
%         \inv{\diag(\mPi{i_{j-1}})}.
% \end{align*}
% 
% Finally, we have the following expression,
% \begin{align*}
%   \mOpp{v}{1} &= \mOpp{v}{v} Y_{l \given l-1} \cdots Y_{2 \given 1}.
% \end{align*}
% 
% Note that \assumptionref{full-rank-plus} guarantees that $Y_{j \given
%   j -1}$ is full rank, from which it directly follows that $\mOpp{v}{1}$, completing our proof.
% \end{proof}
% 
% \subsection{Sample Complexity}
% 
% In this section, we will explicitly derive the sample complexity of
%   $\LearnMarginals$. The end result is an estimate of the error of
%   each clique marginal $Z_\sC$.
% The analysis proceeds in two stages; (i) bounding the error in
%   estimating in each conditional moment $\mOpp{v}{i}$ and (ii) bounding
%   the error in estimating $Z_\sC$.
% 
% \paragraph{Estimating $\mOpp{v}{i}$}
% 
% Consider the estimation of the conditional moments for a bottleneck
%   $h_i$ with views $x_{v_1}, x_{v_2}, x_{v_3}$. 
% Let the error in estimation of the moments be upper-bounded by
%   $\epsilon$, $\|M_{v_i v_j} - \hat M_{v_i v_j} \|_F < \epsilon$ for $i,
%   j \in \{1,2,3\}$, and 
% the singular values of the moments be lower-bounded by $\alpha$,
%   $\sigma_k(M_{v_i v_j}) > \alpha$ for $i, j \in \{1,2,3\}$.
% Results from \citet{anandkumar12moments,anandkumar13tensor} show that 
% \begin{align*}
%   \|\mOpphat{v_1}{i} - \mOppit{v_1}{i}\|^2_F 
%     &= O( 
%     \frac{k {\mPi{i}}_{\max}^2}
%     {\beta^5 } \epsilon ). 
% \end{align*}
% We will now express this quantity in terms of the parameters of the model.
% 
% Let $\sigma_1(Z_\sC)$ and $\sigma_k(Z_\sC)$ respectively be the largest
%   and smallest singular value of any slice of $Z_\sC$. 
% From the derivation in \lemmaref{full-rank-suff}, we know that $\mPi{i}$
%   is obtained by marginalizing some slice of $Z_{\sC_j}$, which gives
%   the result that $\sigma_1(Z_\sC) \succ \mPi^{i} \succ \sigma_k(Z_\sC)$.
% Furthermore, 
% \begin{align*}
%   \sigma_k(Y_{j \given j-1}) &\ge \frac{\sigma_k(Z_{\sC_j})}{\mPi{i_{j-1}}_{\max}}
%                              \ge \frac{\sigma_k(Z_{\sC_j})}{\sigma_1(Z_{\sC_j})}
%                              = \cnd(Z_{\sC_j})^{-1},
% \end{align*}
% where $\cnd(Z_{\sC_j}) \eqdef
% \frac{\sigma_1(Z_{\sC_j})}{\sigma_k(Z_{\sC_j})}$ is defined analogously
% to the condition number of a matrix. 
% Finally,
% \begin{align*}
%   \sigma_k(\mOpp{v}{1}) 
%     &\ge \sigma_k(\mOpp{v}{v}) \prod_{j=2}^{l} \sigma_k(Y_{j \given j-1}) \\
%     &\ge \sigma_k(\mOpp{v}{v}) \prod_{j=2}^{l} \cnd(\sC_{j}).
% \end{align*}
% 
% Note that $M_{v_a v_b} = \mOpp{v_a}{i} \diag(\mPi{i})
% \mOpp{v_b}{i}^\top$.
% Without loss of generality, let us assume that $M_{v_a v_b}$ is the
%   moment with the smallest singular value.
% This gives us the following bound on $\beta$,
% \begin{align*}
%   \beta &\ge \sigma_k(M_{v_a,v_b}) \\
%         &\ge \sigma_k(\mOpp{v_a}{i}) \mPi{i}_{\min} \sigma_k(\mOpp{v_b}{i}) \\
%         &\ge \sigma_k(\mOpp{v_a}{v_a}) \sigma_k(\mOpp{v_b}{v_b}) \sigma_k(Z_{\sC_i}) \cnd_{\min}^{d_a + d_b},
% \end{align*}
% where $\cnd_{\min}$ is the smallest $\cnd(Z_\sC)$ for any $\sC \in \sG$
% and $d_a, d_b$ are distance from $h_i$ to $x_{v_a}$ and $x_{v_b}$
% respectively.
% 
% Finally, we have the result,
% \begin{align*}
%   \|\mOpphat{v_1}{i} - \mOppit{v_1}{i}\|^2_F 
%     &= O( 
%     \frac{k \sigma_1^2 }
%     {(\sigma_k^3 \cnd_{\min}^{d_a + d_b})^5}
%     \epsilon ). 
% \end{align*}
% 
% \paragraph{Estimating $Z_\sC$}
% 
% Similarly, we can apply standard perturbation analysis techniques to get
% an error bound on $Z_\sC$;
% \begin{align*}
%   \|Z_\sC - \hat Z_\sC\|_F 
%   &\le \|M_\sV(\mOppit{v_1}{i_1}, \cdots, \mOppit{v_m}{i_m}) \\
%       &\quad - \hat M_\sV(\hat{\mOppit{v_1}{i_1}}, \cdots, \hat{\mOppit{v_m}{i_m}})\|^2_F.
% \end{align*}
% Let $\|\mOppit{v_1}{i_1}\|_F < O$ and $\|\hat{\mOppit{v_1}{i_1}}\|_F < O$; then, we get
% \begin{align*}
%   \|Z_\sC - \hat Z_\sC\|_F 
%   &\le \|M_\sV - \hat M_\sV\|_F O^m \\
%   &\quad + \|\hat M_\sV\|_F O^{m-1} \max\{\|\hat {\mOppit{v_1}{i_1}} - \mOppit{v_1}{i_1}\|_F\}.
% \end{align*}

% Let's return to the X = {X_{ijk}}, Y = Y_{lm} example. We defined Z_{jkm} = \sum_i X_{ijk} Y_{im}.
% 
% Let's look at some mode unfolding of Z - we want to show that each of these is full rank (and we care about their singular values).
% 
% First, consider the unfolding along 'm' - I'm going to single out indices using (Z_m)_jk to be a little clearer with text math:
% 
% ([Z_(3)]_m)_jk = (Z_m)_{jk}
%                      = (Y_m)_{i} (X_i)_{jk}
%                      =  Y_(2) * X_(1)'
% 
% In other words, this is just the product of the two mode unfoldings and this is going to have full rank (because each has full rank). This is clearly the easy case.
% 
% Next, consider
% 
% ([Z_(1)]_j)_km = (Z_j)_{km}
%                      = \sum_i (X_{j})_{ik} Y_{im}.
% 
% The trick is to take the outer product of Y and the identity matrix. Then we get a new "Y" that looks like Y_{ikk'm} = Y_{im} \delta_{kk'}. Then,
% 
% ([Z_(1)]_j)_km = (Z_j)_{km}
%                      = \sum_{ik} (X_j)_{ik} * (Y_{ik})_{k'm}
%                      = X_(2) * Y_(12),
% 
% the product of these two unfoldings. It's easy to show that rank Y_(12)  = rank(Y) because of the property that sigma(A \otimes B) = \sigma(A) \sigma(B).
