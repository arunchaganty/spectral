\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
\usepackage{subfigure} 
\usepackage{amsmath,amssymb,amsthm} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Submission and Formatting Instructions for ICML 2014}

\input{macros}

\begin{document} 

\twocolumn[
\icmltitle{Moment Constraints Make Learning Latent-Variable Models Easier}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Arun Tejasvi Chaganty}{chaganty@cs.stanford.edu}
\icmlauthor{Percy Liang}{pliang@cs.stanford.edu}
\icmladdress{Stanford University,
Stanford, CA, USA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{machine learning}

\vskip 0.3in
]

\begin{abstract} 
\end{abstract} 

\section{Introduction}
\label{sec:introduction}

% 1. Latent variable models are good.
Latent variable models offer a succinct representation of a rich model
family. 
% 2. Learning them is hard.
Despite their success across many fields
\cite{quattoni04crf,haghighi06prototype,liang06discrimative,kirkpatrick10painless},
learning these models remains a difficult problem due to the
non-convexity of the likelihood. Local optimization (e.g.
expectation-maximization) is the standard approach, but is susceptible
to local optima.

% 3. People have approached unsupervised learning with the MoM magic sauce, but the sauce is limited.
Recently, unsupervised learning techniques based on the method of moments and
spectral decomposition have offered a refreshing and promising perspective on
this learning problem \citep{hsu09spectral,anandkumar11tree,anandkumar12moments,anandkumar12lda,hsu12identifiability,balle11transducer,balle12automata}.
These methods exploit the linear algebraic properties of the model and
factorize the moments into parameters, providing strong theoretical guarantees.
However, these methods are not as universally applicable as EM.

% 4. State what we do: exploit moment constraints to make the problem easier.
In this work, we exploit the spectral method to learn moment constraints
on the observed variables and show how these constraints can greatly
simplify the learning problem.
% 5. We get moments from third-order tensors from bottlenecks and factorize them into marginals.
The key idea in
\citet{anandkumar12moments,anandkumar13tensor} is to study the conditional
independence factorization structure implied by the model in the form of 
three-view bottlenecks. These bottlenecks which can be factorized into
moments involving the latent variables\vague.

% 6. Provide some intuitive examples - Can I haz HMMs?
Consider a hidden Markov model, with initial probabilities $\pi$,
  transitions $T$ and emissions $O$. 
While \citet{hsu09spectral,anandkumar12moments} provide a consistent estimator
  for the parameters of this model that inverts $O$ to recover $T$ and
  $\pi$. We would like to avoid this potentially sensitive procedure via optimization.
The log-likelihood of this model, 
  \begin{align}
    \sL(\pi, T, O) &= \sum_{x\in\sD} \log( \sum_{h} \pi(h_1) O(x_1 | h_1) T(h_2 | h_1) O(x_2 | h_2) \cdots ),
  \end{align}
  is not convex, even if we knew any one of the parameters.
However, intuitively, if we knew $O$ and found that each symbol was
  produced by exactly one state, learning $T$ and $\pi$ could be trivial
  accomplished via counting. 
In general, $O$ still informs us about the states in the observation
  sequence. If we considered instead the piecewise likelihood, 
  \begin{align}
    \sL_{\textrm{piecewise}}(\pi, T, O) &= \sum_{x\in\sD} \log( \sum_{h_1, h_2} \underbrace{\pi(h_1) T(h_2 | h_1)}_{P(h_1,h_2)} O(x_1 | h_1) O(x_2 | h_2) ),
  \end{align}
  then we observe that the likelihood is convex in the marginal
  distribution $P(h_1,h_2)$. This allows us to recover $P$, from which $T$
  can be easily reconstructed.

% 7. Now outline the rest of our results.
In \sectionref{piecewise} we generalize this observation to a directed grid
  model and a directed tree model and show that the {\em piecewise
  likelihood} is strictly convex, providing consistent parameter
  estimators for these models (which is new)\reword.
The approach can be extended to log-linear
  models (\sectionref{log-linear}), where the problem remains non-convex,
  however, we expect a reduction in the number of local optima. 
In this regime, we show empirically that constraining the moments of the
  model via posterior-regularization allows us to learn better models.

\todo{Related work on spectral methods and measurements.}

\section{Parameter Estimation with Convex Piecewise Likelihoods}
\label{sec:piecewise}

\section{Parameter Estimation for Log-linear Latent-Variable Models}
\label{sec:log-linear}

\paragraph{Setup}
Let $z$ be a collection of variables indexed by $V$ (that is, $z = \{z_j\}_{j \in V}$).
Suppose that some of the variables are observed ($x = \{ z_j \}_{j \in O}$, where $O \subset V$) 
and the rest are latent ($h = \{ z_j \}_{j \in H}$, where $H = V \backslash O$).
Let $C \subset 2^V$ be a collection of subsets of variables,
and let $\phi_c(z_c) \in \Re^d$ be the feature map for each $c \in C$.
In this paper, we will consider both directed and undirected models over $z$.
In the undirected case, we define the standard globally-normalized exponential family model:
\begin{align}
  \label{eqn:undirectedSetup}
  p_\theta(z) = \exp\{ \phi(z)^\top\theta - A(\theta) \},
\end{align}
where $\phi(z) \eqdef \sum_{c \in C} \phi_c(z_c) \in \Re^d$ is the global feature vector, $\theta \in \Re^d$ is the parameter vector,
and $A(\theta) = \log \int \exp\{\phi(z)^\top\theta\} dz$ is the global log-partition function.
To some extent, our methods only depend on the conditional independence
structure of the models.

\paragraph{Problem statement}

% Statement
This paper focuses on the problem of parameter estimation:
We are given $n$ i.i.d.~examples of the observed variables $D = (x^{(1)}, \dots, x^{(n)})$
where each $x^{(i)} \sim p_{\theta^*}$ for some true parameters $\theta^*$.
Our goal is to produce a parameter estimate $\hat\theta$ that approximates $\theta^*$.

% Maximum likelihood
The standard estimation procedure is maximum (marginal) likelihood,
$\sL \eqdef \max_{\theta \in \Re^d} \sum_{x \in D} \log p_\theta(x)$,
which is statistically efficient but computationally intractable.
In practice, one uses gradient-based optimization procedures (e.g., EM or L-BFGS)
on the marginal likelihood, which can get stuck in local optima.

\paragraph{Primer: mixture of Bernoulli}

\section{Experiments}
\label{sec:experiments}

\section{Discussion}
\label{sec:discussion}

\bibliography{pliang}
\bibliographystyle{icml2014}

\end{document} 

