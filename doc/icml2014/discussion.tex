\section{Discussion}
\label{sec:discussion}

Parameter estimation for latent variable models using the method of
  moments has received much attention in last few years.
Our work uses the \TensorFactorize algorithm presented by
  \citet{anandkumar13tensor}, though \citet{anandkumar12moments} first
  studied the consistent parameter estimation algorithm for the
  three-view mixture model.
%\todo{What about Mossel and Roch?}
Several authors have extended this work to settings that do not meet the
  criterion \propertyref{bottleneck}, but rely on knowing more about
  the underlying correlation structure of the problem.
\citet{anandkumar12lda} are able to recover parameters for correlated
  discrete mixtures, e.g. latent Dirichlet allocation; their approach
  follows that of the three-view mixture model, with a pre-processing step
  that subtracts the correlation between the views.
\citet{anandkumar2013linear} study a special family of Bayesian networks
  in which the observed variables are linearly related to a sparse set of
  hidden variables. Once again, the key step is to identify the
  correlations between observed variables and exclude them from the views.
\citet{halpern13noisyor} propose an algorithm to recover factors for
  a more densely connected bipartite noisy-or network; they also use
  bottlenecks to learn parameters, but exploit properties of the model
  to negate correlations from learned edges, allowing for more
  bottlenecks to be discovered. 
In comparison, our work applies to a general family of models, including
  latent tree models, directed and undirected grids, with no assumptions
  on the correlation structure aside from independences encoded in the
  graph. 

Another prominent line of work in the method of moments community has
  focussed on recovering parameters for a transformation that admits an {\em observable
  representation}, i.e. one whose marginals can be directly computed from the data.
\citet{song2011spectral} propose an algorithm to learn an observable
  operator representations for latent tree graphical models, like the
  one in \figureref{examples-tree}, assuming \propertyref{bottleneck}. 
Their approach is similar to ours, but restricted to trees with observed
  variables at the leaves.
\citet{parikh12spectral} extend this approach to general graphical
  models in which \propertyref{bottleneck} holds, but consider the latent
  junction tree representation. 
Consequently, the observable representations have size atleast that of
  a node in the junction tree, which will be of the order of the treewidth
  of the graph. 
Our algorithm only constructs moments of the order of the cliques in the
  graph, which can be exponentially smaller. 
For example, an $n\times n$ grid model has a treewidth of $n$, but each
  clique is of size at most $2$ in the undirected case and $3$ in the
  directed case.
In some sense, an observable operator re-formulation of our algorithm
  subsumes this work.

In this paper, we've described an algorithm that can consistently
  estimate parameters for the class of directed and undirected graphical
  models for which each hidden variable has at least three conditionally
  independent observed variables (\propertyref{bottleneck}).
This condition is sufficiently general to capture a variety of popular
  models like latent trees and grids (like a pairwise latent Markov
  random field).
The undirected model setting applies to log-linear models, allowing
  for discriminative featurization; furthermore, in practice, the
  bottleneck condition can be enforced on a model if the observed
  features can be split into two uncorrelated sets.

The key property we exploited was the presence of exclusive views
  (\propertyref{exclusive-views}) with learnable parameters.
In some sense, the presence of these views helps tie down the hidden
  variables to the data. 
While other authors, e.g \citet{anandkumar12lda, anandkumar2013linear,
  halpern13noisyor} are able to learn parameters for models outside this
  family, they do so with some knowledge of the correlation structure
  between variables. 
In a pre-processing step, they ``subtract'' out the correlations and
  recover a graph that satisfies the bottleneck property
  \propertyref{bottleneck}.
Thus, while \propertyref{bottleneck} is much stronger than
  \propertyref{exclusive-views}, we expect that without any knowledge of
  the correlation between variables, \propertyref{bottleneck} is
  necessary for identifiability.


