\section{Setup}
\label{sec:setup}

Let $\sG$ be a discrete graphical model with observed variables $X$ and
  hidden variables $H$.
Let $\Pa(a) \subset X \union H$ be the parents of a variable $a$ in a directed
  model, and $\sN(a) \subset X \union H$ be the neighbors of $a$ in an
  undirected model.
We will use $x_1, x_2, \cdots, x_L \in X$ to denote observed variables and
  $h_1, h_2, \cdots, h_M \in H$ to denote hidden variables.
For an integer $n$, let $[n] = \{ 1, \dots, n \}$.
For ease of exposition, we assume that $x_v \in [d]$ for
all $v$, and that $h_i \in [k]$ for all $i$.
Let $\sX$ be the joint domain of the observed variables, i.e. $\sX \eqdef [d]^L$ and
let $\sH$ be the joint domain of the hidden variables, i.e. $\sH \eqdef [k]^M$.

% PL: don't know what this means
%We assume that the model $\sG$ is parameterized at every clique $\sC$.
If $\sG$ is directed, the model parameters correspond to
the conditional probability tables for each clique $(\Pa(a), a) \in \sG$.
If $\sG$ is undirected, we assume a log-linear parametrization of each clique $\sC \in \sG$.

\paragraph{Problem statement}

% Statement
This paper focuses on the problem of parameter estimation:
We are given $n$ i.i.d.~examples of the observed variables $\sD
  = (\vx^{(1)}, \dots, \vx^{(n)})$ where each $\vx^{(i)} \sim p_{\theta^*}$ for
  some true parameters $\theta^*$.
Our goal is to produce a parameter estimate $\hat\theta$ that
  approximates $\theta^*$.

% Maximum likelihood
The standard estimation procedure is maximum (marginal) likelihood,
  \begin{align*}
    L_\text{unsup}(\theta) &\eqdef \max_{\theta} \sum_{\vx \in \sD} \log p_\theta(\vx) \\
        &=      \max_{\theta} \sum_{\vx \in \sD} \log \sum_{\vh \in \sH} p_\theta(\vx,\vh).
  \end{align*}
  Maximum likelihood is statistically efficient,
  but in general computationally intractable
  because marginalizing over hidden variables $\vh$ yields a non-convex objective.
In practice, one uses local optimization procedures (e.g., EM
  or L-BFGS) on the marginal likelihood, which can get stuck in local
  optima.
We will later return to likelihoods, but let us first
describe a method of moments approach for parameter estimation.
The starting point is moments of the marginal distribution $\BP(\vx)$,
and a bit of notation is necessary.

\paragraph{Notation}

We use the notation $[\cdot]$ to indicate indexing; for example $M[i]$
  is the $i$-th row of a matrix $M$ and $M[i,j]$ is the $i,j$-th element
  of $M$.
For a tensor $T \in \Re^{d \times \ldots \times d}$ and a vector $\vec
  i = (i_1, \cdots, i_\ell)$, define the projection $T[\vec i] = T_{i_1 \ldots i_\ell}$.
Similarly, for a set $\sC$ of size $k$ and vector $\vec x \in \Re^d$, we
  will use the notation $\vec x_\sC$ to get a vector $\Re^k$ which selects
  the appropriate elements of $\vec x$.

We use $\otimes$ to denote the tensor product: if $u \in \Re^d,
  v \in \Re^k$, then $u \otimes v \in \Re^{d \times k}$.
For an $\ell$-th order tensor $T \in \Re^{d \times \ldots \times
  d}$ and vectors $x_1, \cdots, x_\ell \in \Re^{d}$, define 
  the partial application:
\begin{align*}
  T(x_1, \cdots, x_\ell) 
      %&= \sum_{i_1 = 1}^{d} \cdots \sum_{i_\ell = 1}^{d} 
  &= \sum_{\bi}
            T[i_1, i_2, \ldots, i_\ell] x_1[i_1] \cdots x_\ell[i_\ell].
\end{align*}
Analogously, for a matrices $M_1 \in \Re^{d \times k}, \cdots,
  M_\ell \in \Re^{d \times k}$, we define
\begin{align*}
  T(M_1, \cdots, M_\ell)[\vec j]
      %&= \sum_{i_1 = 1}^{d} \cdots \sum_{i_\ell = 1}^{d} 
  &= \sum_{\bi}
            T[\vec i] {M_1}[i_1,j_1] \cdots M_\ell[i_\ell, j_\ell].
\end{align*}
%where $\vec i = (i_1, \cdots, i_\ell)$ and $\vec j = (i_1, \cdots, j_\ell)$

We consider up to third-order moments:
\begin{align*}
  \mO_i &\eqdef \Pr(x_i), &
  \mO_{ij} &\eqdef \Pr(x_i, x_j), &
  \mO_{ijk} &\eqdef \Pr(x_i, x_j, x_k).
\end{align*}
We will view $\mO_i, \mO_{ij}, \mO_{ijk}$ as tensors of
  orders $1, 2, 3$ in $\Re^d, \Re^{d\times d}, \Re^{d \times d \times d}$.
Next, define the \emph{hidden marginals}:
\begin{align*}
  \mH_i &\eqdef \Pr(h_i), &
  \mH_{ij} &\eqdef \Pr(h_i, h_j), &
  \mH_{ijk} &\eqdef \Pr(h_i, h_j, h_k).
\end{align*}
These are tensors of
  orders $1, 2, 3$ in $\Re^k, \Re^{k\times k}, \Re^{k \times k \times k}$.

\paragraph{Estimating three-view mixture models}

\begin{figure}[t]
  \label{fig:three-view}
  \centering
  \input{figures/three-view.tikz}
  \caption{The canonical three-view mixture model can be estimated consistently using tensor factorization \citep{anandkumar13tensor}.}
\end{figure}

\citet{anandkumar12moments} presented a consistent estimator
  using the method of moments to learn the parameters of a three-view
  mixture model (\figureref{three-view}), an important case of directed
  graphical models. %, that sidesteps the local optima problem altogether. 
In that model, we have three observed variables $x_1, x_2, x_3$
  that are independent conditioned on a hidden variable $h_1$.  
The parameters of this model are the mixture probabilities $\pi\oft 1 \eqdef
  \Pr(h_1) \in \Delta_{k-1}$ and the conditional moments $\mOpp{i}{1} \eqdef
  \Pr(x_i \given h_1) \in \Re^{d \times k}$ for $i \in \{1,2,3\}$.

  The key idea is that the observed moments $\mO_{12}, \mO_{23}, \mO_{13}$ and
  $\mO_{123}$ admit a factorization in terms of parameters:
\begin{align*}
  \mO_{ij} &= \sum_h {\pi \oft 1}[h] {\mOpp{i}{1}}[\cdot,h] \otimes {\mOpp{j}{1}}[\cdot,h] \quad \forall i,j \in \{1,2,3\} \\
  \mO_{123} &= \sum_h {\pi \oft 1}[h] {\mOpp{1}{1}}[\cdot,h] \otimes {\mOpp{2}{1}}[\cdot,h] \otimes  {\mOpp{3}{1}}[\cdot,h].
\end{align*}

\citet{anandkumar13tensor} presented a robust tensor power method, which
  we call $\TensorFactorize$, which converts estimates of the observed moments 
  into estimates of the parameters, $\pi\oft 1, \mOpp{1}{1},
  \mOpp{2}{1},\mOpp{3}{1}$ (up to permutation of the columns), provided
  the following assumption holds:

\begin{assumption}
\label{asm:full-rank}
Assume the conditional moments $\mOpp{1}{1}, \mOpp{2}{1}, \mOpp{3}{1}$ have full column rank
  $k$, and that $\pi\oft 1 \succ 0$.
\end{assumption}

In brief, the algorithm first computes a whitening matrix $W \in
  \Re^{d \times k}$ such that $W^T \mO_{12} W = I_{k \times k}$,
  and uses it to transform $\mO_{123}$ into a symmetric orthogonal tensor.  
Then a robust tensor power method is applied to extract the eigenvectors
of the whitened $\mO_{123}$; unwhitening yields the columns of $\mOpp{3}{1}$.
  $\mOpp{1}{1}$ and $\mOpp{2}{1}$ can then be recovered with a few matrix operations.

The tensor factorization method attacks the heart of the non-convexity
  in latent-variable models.  In the next two sections,
  we will see how to build on top of this result to estimate directed (Section~\ref{sec:directed})
  and undirected (Section~\ref{sec:undirected}) models.
