\section{Setup}
\label{sec:setup}

Let $\sG$ be a discrete graphical model with
observed variables $\bx = (x_1, \dots, x_L)$ and
hidden variables $\bh = (h_{1}, \dots, h_M)$.
We assume that the domains of the variables
are $x_v \in [d]$ for all $v \in [L]$
and $h_i \in [k]$ for all $i \in [M]$,
where $[n] = \{ 1, \dots, n \}$.
Let $\sX \eqdef [d]^L$ and $\sH \eqdef [k]^{M}$ be the joint domains of $\bx$ and $\bh$.

For undirected models $\sG$,
let $\sG$ denote a set of cliques, where each clique $\sC \subseteq \bx \cup \bh$ is a subset of nodes.
The joint distribution is given by an exponential family:
  $p_\theta(\bx,\bh) \propto \prod_{\sC \in \sG} \exp(\theta^\top\phi_\sC(\bx_\sC, \bh_\sC))$,
  where $\theta$ is the parameter vector,
  and $\phi_\sC(\bx_\sC, \bh_\sC)$ is the local feature vector
  which only depends on the observed ($\bx_\sC$) and hidden ($\bh_\sC$) variables in that clique $\sC$.
  Also define $\sN(a) = \{ b \neq a : \exists \sC \supseteq \{ a, b \}$ be the neighbors of variable $a$.

For directed models $\sG$,
  $p_\theta(\bx,\bh) = \prod_{a \in \bx \cup \bh} p_\theta(a \mid \Pa(a))$,
  where $\Pa(a) \subseteq \bx \union \bh$ are the parents of a variable $a$.
  The parameters $\theta$ are the conditional probability tables of all variables,
  and the cliques are $\sG = \{ \{ a \} \union \Pa(a) : a \in \bx \union \bh \}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Problem statement}

% Statement
This paper focuses on the problem of parameter estimation:
We are given $n$ i.i.d.~examples of the observed variables $\sD
  = (\vx^{(1)}, \dots, \vx^{(n)})$ where each $\vx^{(i)} \sim p_{\theta^*}$ for
  some true parameters $\theta^*$.
Our goal is to produce a parameter estimate $\hat\theta$ that
  approximates $\theta^*$.

% Maximum likelihood
The standard estimation procedure is maximum (marginal) likelihood,
  \begin{align*}
    L_\text{unsup}(\theta) &\eqdef \sum_{\vx \in \sD} \log p_\theta(\vx)
        =  \sum_{\vx \in \sD} \log \sum_{\vh \in \sH} p_\theta(\vx,\vh).
  \end{align*}
  Maximum likelihood is statistically efficient,
  but in general computationally intractable
  because marginalizing over hidden variables $\vh$ yields a non-convex objective.
In practice, one uses local optimization procedures (e.g., EM
  or L-BFGS) on the marginal likelihood, but these can get stuck in local
  optima.
We will later return to likelihoods, but let us first
describe a method of moments approach for parameter estimation.
First, a bit of notation is necessary.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Notation}

We use the notation $[\cdot]$ to indicate indexing; for example $M[i]$
  is the $i$-th row of a matrix $M$ and $M[i,j]$ is the $i,j$-th element
  of $M$.
For a tensor $T \in \Re^{d \times \cdots \times d}$ and a vector $\bi
  = (i_1, \ldots, i_\ell)$, define the projection $T[\bi] = T[i_1, \ldots, i_\ell]$.
Similarly, for a set $\sC$ of size $m$ and vector $v \in \Re^d$, we
  will use the notation $v_\sC$ to get a vector in $\Re^m$ which selects
  the appropriate elements of $v$.

We use $\otimes$ to denote the tensor product: if $u \in \Re^d,
  v \in \Re^k$, then $u \otimes v \in \Re^{d \times k}$.
For an $\ell$-th order tensor $T \in \Re^{d \times \ldots \times
  d}$ and vectors $v_1, \cdots, v_\ell \in \Re^{d}$, define 
  the application:
\begin{align*}
  T(v_1, \cdots, v_\ell) 
  &= \sum_{\bi}
            T[\bi] v_1[i_1] \cdots v_\ell[i_\ell].
\end{align*}
Analogously, for a matrices $M_1 \in \Re^{d \times k}, \cdots,
  M_\ell \in \Re^{d \times k}$, we define
\begin{align*}
  T(M_1, \cdots, M_\ell)[\vec j]
  &= \sum_{\bi} T[\vec i] {M_1}[i_1,j_1] \cdots M_\ell[i_\ell, j_\ell].
\end{align*}
%a tensor in $\Re^{k \times \ldots \times k}$. % PL: not true, since [j] is applied
%where $\vec i = (i_1, \cdots, i_\ell)$ and $\vec j = (i_1, \cdots, j_\ell)$

%with respect to the true data distribution $p_{\theta^*}(\bx,\bh)$;
We will use $\Pr(\cdot)$ to denote various moment tensors
constructed from the true data distribution $p_{\theta^*}(\bx,\bh)$:
%We consider up to third-order moments:
\begin{align*}
  \mO_i &\eqdef \Pr(x_i), &
  \mO_{ij} &\eqdef \Pr(x_i, x_j), &
  \mO_{ijk} &\eqdef \Pr(x_i, x_j, x_k).
\end{align*}
Here, $\mO_i, \mO_{ij}, \mO_{ijk}$ are tensors of
  orders $1, 2, 3$ in $\Re^d, \Re^{d\times d}, \Re^{d \times d \times d}$.
Next, we define the \emph{hidden marginals}:
\begin{align*}
  \mH_i &\eqdef \Pr(h_i), &
  \mH_{ij} &\eqdef \Pr(h_i, h_j), &
  \mH_{ijk} &\eqdef \Pr(h_i, h_j, h_k).
\end{align*}
These are tensors of
  orders $1, 2, 3$ in $\Re^k, \Re^{k\times k}, \Re^{k \times k \times k}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Assumptions}

In this section, we state technical assumptions that hold for the rest of the
paper, but that we feel are not central to our main ideas.
The first one ensures that we will encounter all realizations of a hidden variable are possible:

\begin{assumption}[Non-degeneracy]
  \label{asm:non-degeneracy}
  The marginal distribution of each hidden variable $h_i$ has full support: $\Pr(h_i) \succ 0$.
\end{assumption}

Next, we assume the graphical model only has conditional independences given by
the graph:
\begin{assumption}[Faithful]
  \label{prop:ci}  
  For any hidden variables $a,b,c \in \bh$
  such that there is an active trail\footnote{See
  \citet{koller2009probabilistic} for a definition.} between $a$ and $b$ conditioned on $c$,
  we have that $a$ and $b$ are dependent given $c$ (i.e., $a\not\perp b \given c$).
\end{assumption}

Finally, we describe a reduction of any graphical model to
  a canonical form to make our arguments simpler.
\begin{lemma}[Canonical form]
  \label{lem:reduction}
Every directed (undirected) graphical model can be transformed into one in which
  the observed variables are leaves with exactly one parent (neighbor). 
There is a one-to-one correspondence between the parameters of the
  transformed and original models.
\end{lemma}
\begin{proof}
  \begin{figure}
    \centering
    \subimport{figures/}{reduction.tikz}
    \caption{Reduction to canonical form.}
    \label{fig:reduction}
  \end{figure}

  \providecommand{\hp}{\ensuremath{h_\text{\rm new}}}
  Let $x_v$ be an observed variable with parents $\Pa(x_v)$ and children $\text{Ch}(x_v)$.
  Consider the following transformation.
  Replace $x_v$ with a new hidden variable \hp\ with the same
  parents $\Pa(x_v)$ and children $\text{Ch}(x_v) \union \{x_{v_1}, x_{v_2}, x_{v_3}\}$,
  where $x_{v_1},x_{v_2},x_{v_3}$ are three copies of $x_v$
  (\figureref{reduction}).
  Define $\Pr(\hp \mid \Pa(x_v)) = \Pr(x_v \mid \Pa(x_v))$ and
  $\Pr(x_{v_j} \mid \hp) = I$.
  Then, there is a one-to-one correspondence between every value of
  $\hp$ and $x_v$. Consequently, for any clique $\sC \contains \hp$, the
  parameters in the original graphical model can be obtained by
  substituting $\hp$ with $x_v$.

  We apply this procedure for all non-leaf observed variables.
  This procedure also applies straightforwardly for undirected graphical
  models, considering the neighbors $\sN(x_v)$ instead of its parents
  and children.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bottlenecks}

In this section, we describe how tensor factorization
can be used to reveal information about the hidden
variables \citep{anandkumar13tensor}.  The key notion is that of a bottleneck:

\begin{definition}[Bottleneck]
  A hidden variable $h_i$ is said to be a \emph{bottleneck} if there
  exists three observed variables (views), $x_{v_1}, x_{v_2}, x_{v_3}$
  that are conditionally independent given $h_i$,
  and each $\mOpp{v}{i} \eqdef \Pr(x_v \mid h_i) \in \Re^{d \times k}$ has full column rank $k$
  for each $v \in \{v_1, v_2, v_3\}$.
  %Let $\sV_{h_i}$ denote any set of views for $h_i$.
  We say that a subset of hidden variables $S \subseteq \bh$ is a bottleneck
  if every $h \in S$ is a bottleneck.
  We say that a graphical model $\sG$ is bottlenecked if all hidden variables
  are bottlenecks.
\end{definition}
For example, in \figureref{approach}, $x_1^a,x_1^b,x_2^a$ are views of the bottleneck $h_1$;
$x_2^a,x_2^b,x_1^b$ are view of the bottleneck $h_2$.  Therefore, the clique
$\{h_1,h_2\}$ is a bottleneck.  Note that views are allowed to overlap.

%Now define the following property:
%\begin{property}[Uniformly bottlenecked]
%  \label{prop:bottleneck}
%  every hidden variable in $S$ is a bottleneck. 
%  If every hidden clique $\sC \in \sG$ is uniformly bottlenecked, then we say
%  that the graph $\sG$ is uniformly bottlenecked as well.
%\end{property}
%If every hidden variable is a bottleneck, then we can estimate $\mOpp{v}{i}$

%Intuitively, we need to ensure that every marginal distribution
  %$\Pr(x_v)$ can be separated into $k$ different conditional distributions
  %$\Pr(x_v | h_i)$. 
The full rank assumption on the conditional moments $\mOpp{v}{i} = \Pr(x_v \mid h_i)$
ensures that all states of $h_i$ ``behave differently''.
In particular, the conditional distribution of one state cannot be
a mixture of that of other states.
%PL: leave vague
%asks that the conditional distributions are
%  linearly independent; if this were not the case, the conditional
%  distribution for a particular latent state could be imitated by the
%  combination of several other latent states.

%\begin{assumption}
%\label{asm:full-rank}
%The conditional moments $\mOpp{1}{1}$, $\mOpp{2}{1}$, $\mOpp{3}{1}$ have full column rank
%  $k$, and $\pi\oft 1 \succ 0$.
%\end{assumption}

%\begin{figure}[t]
%  \label{fig:three-view}
%  \centering
%  \input{figures/three-view.tikz}
%  \caption{The canonical three-view mixture model can be estimated consistently
%  using tensor factorization \citep{anandkumar13tensor}.}
%\end{figure}

\citet{anandkumar12moments} provide an efficient tensor factorization algorithm
for estimating $\Pr(x_v \mid h_i)$:
  %using the method of moments to learn the parameters of a three-view
\begin{theorem}[Tensor factorization]
Let $h_i \in \bh$ be a hidden variable
with views $x_{v_1},x_{v_2},x_{v_3}$.
Then there exists an algorithm $\TensorFactorize$ that
returns consistent estimates $\mOpphat{v}{i}$ for each $v \in \{v_1,v_2,v_3\}$.
\end{theorem}

%\citet{anandkumar12moments} presented a consistent estimator
  %using the method of moments to learn the parameters of a three-view
  %mixture model (\figureref{three-view}), an important case of directed
  %graphical models. %, that sidesteps the local optima problem altogether. 
%In that model, we have three observed variables $x_1, x_2, x_3$
  %that are independent conditioned on a hidden variable $h_1$.  
%The parameters of this model are the prior probabilities $\pi\oft 1 \eqdef
  %\Pr(h_1) \in \Re^k$ and the conditional moments $\mOpp{i}{1} \eqdef
  %\Pr(x_i \given h_1) \in \Re^{d \times k}$ for $i \in \{1,2,3\}$.

To simplify notation, consider the example in \figureref{three-view} where $h_1=1,v_1=1,v_2=2,v_3=3$.
The key idea is that the observed moments $\mO_{12}, \mO_{23}, \mO_{13}$ and
  $\mO_{123}$ admit a factorization in terms of parameters:
\begin{align*}
  \mO_{ij} &= \sum_h {\pi \oft 1}[h] {\mOppt{i}{1}}[h] \otimes {\mOppt{j}{1}}[h] \\ %\quad \forall i,j \in \{1,2,3\} \\
  \mO_{123} &= \sum_h {\pi \oft 1}[h] {\mOppt{1}{1}}[h] \otimes {\mOppt{2}{1}}[h] \otimes  {\mOppt{3}{1}}[h].
\end{align*}
The algorithm first computes a whitening matrix $W \in
  \Re^{d \times k}$ such that $W^\top \mO_{12} W = I_{k \times k}$,
  and uses $W$ to transform $\mO_{123}$ into a symmetric orthogonal tensor.  
Then a robust tensor power method is applied to extract the eigenvectors
of the whitened $\mO_{123}$; unwhitening yields the columns of $\mOpp{3}{1}$.
The other model parameters can be recovered with a few additional matrix operations.
%  we call $\TensorFactorize$, which transforms estimates of the observed moments 
%  into estimates of the parameters, $\pi\oft 1, \mOpp{1}{1},
%  \mOpp{2}{1},\mOpp{3}{1}$ (up to permutation of the columns), provided
%  the following assumption holds:

%It is easy to see that $\LearnMarginals$ has a polynomial sample complexity
%because it composes two parts that individually have polynomial sample
%complexity.
%From \citet{anandkumar13tensor} given the non-degeneracy assumptions
  %\assumptionref{non-degeneracy},
  We have that $\mOpphat{v}{i}$
  converges to $\mOpp{v}{i}$ at a rate of $n^{-\frac12}$ with a constant
  that depends polynomially on the $k$-th singular value of
  $\mOpp{v}{i}$.
% PL: we don't have a crisp theorem (because there's multiple paths), so leave it vague.
%Recovering the marginals $Z_\sC$ from the conditional moments via $\LearnClique$ is a linear operation and thus also has polynomial sample complexity. 
However, $\sigma_{k}(\mOpp{v}{i})$ can become extremely
small if $h_i$ and $x_v$ are connected via many intermediate hidden
variables.\footnote{To see this, consider a chain-structured graphical model:
$h_1 \to h_2 \cdots \to h_t \to x_v$. In this example, if
$\sigma_k(\Pr(h_{i+1} \given h_{i})) \ge \sigma_k$ for each $i = 1,
\cdots, t-1$, then $\sigma_k(\mOpp{v}{1})$ can be as bad as
$\sigma_k^t \sigma_k(\mOpp{v}{t})$.}.

% No parameters yet
The tensor factorization method attacks the heart of the non-convexity
  in latent-variable models, providing some information about the hidden variables
  in the form of the conditional moments $\mOpp{v}{i} = \Pr(x_v \mid h_i)$.
  Note that $\TensorFactorize$ only looks at the conditional independence structure
  of the graphical model, not its parametrization.
  If $v$ is the single parent of $i$ (e.g., $\Pr(x_1^a \mid h_1)$ in \figureref{approach}),
  then this conditional moment is a parameter of the model.
  If not (e.g., $\Pr(x_2^a \mid h_1)$), then this is not a parameter.
  Furthermore, there are some parameters (e.g., $\Pr(h_4 \mid h_2, h_3)$) which are
  are still not recovered.
  In general, there is a gap between the conditional moments
  and the model parameters,
  something we will address in the next section.

  %In the next two sections,
  %we will see how to use this result to estimate directed (Section~\ref{sec:directed})
  %and undirected (Section~\ref{sec:undirected}) models.
