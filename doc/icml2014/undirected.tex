\section{Consistent Parameter Estimation for Undirected Latent Variable Log-linear Models}
\label{sec:undirected}

In this section, we will show that we can extend the results to
  undirected graphical models parameterized as log-linear latent variable
  models.
We exploit a connection between the moments of the clique potentials and
  the log-partition function to show that learning parameters is convex
  given the clique marginals $Z_\sC$. 
The end result is a consistent estimator for log-linear
  latent-variable models which satisfy \propertyref{exclusive-views}.
Finally, we generalize the result to classes when only a subset of the
  clique marginals can be learned.

As before, let $\sG$ be a graphical model with observed variables $X$ and hidden variables $H$.
We assume that the undirected graphical model is a log-linear latent
  variable model with parameters $\theta$,
\begin{align*}
  p_\theta(\vx, \vh) &= \exp( \sum_\sC \theta_\sC^T \phi_\sC(\vx,\vh) - A(\theta) ),
\end{align*}
where $\phi_\sC(\vx,\vh)$ is a feature vector for the clique $\sC$ and
  $A(\theta) = \sum_{\vx, \vh}  \exp( \sum_\sC \theta_\sC^T \phi_\sC(\vx,\vh) )$ is the
  log-partition function.
While $\phi_\sC(x,h)$ can be an arbitrary featurization, we assume
  without loss of generality that $\phi_\sC(x,h)$ are the indicator
  features $\BI[x,h]$\footnote{Note that $\BI[x,h]$ are sufficient
  statistics for the model, and hence we do not lose any statistical
  efficiency in using them.}.

As before, our objective is to find a set of parameters $\hat \theta$
  that maximize the likelihood of data $\sD$.
\begin{align}
  \hat \theta 
      &= \arg\max_{\theta} \sum_{\vx \in \sD} \log p_\theta(\vx) \nonumber \\
      &= \arg\max_{\theta} \sum_{\vx \in \sD} \log \sum_{\vh \in \sH} p_\theta(\vx,\vh) \nonumber \\
      &= \arg\max_{\theta} \sum_{\vx \in \sD} \log \sum_{\vh \in \sH} \exp( \theta^T\phi(\vx,\vh) - A(\theta) ). \label{eqn:obj-ml}
\end{align}
Note that this problem is not concave in $\theta$. 
However, given the moments $\E[ \phi_\sC(\vx,\vh) ] = Z_\sC$, the
  problem simplifies to,
\begin{align}
  \hat \theta 
  &= \arg\max_{\theta} \sum_{\sC \in \sG} \theta_\sC^T Z_\sC - A(\theta), \label{eqn:obj}
\end{align}
which is concave in $\theta$.
In \sectionref{directed}, we showed that \LearnMarginals is a consistent
  estimator for $Z_\sC$ for any model satisfying \propertyref{exclusive-views}. 
The result is that the output of \algorithmref{undirected}, $\hat\theta$, is a consistent estimator for $\theta^*$
  when the model $p_\theta(\theta)$ is identifiable.

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}
  \caption{\LearnLogLinear}
  \label{algo:undirected}
  \begin{algorithmic}
    \REQUIRE A graphical model $\sG$ satisfying \propertyref{bottleneck}, data $\sD$
    \ENSURE A consistent estimate of parameters $\hat \theta$
      \STATE Apply \LearnMarginals to $\sG$ to learn clique marginals $Z_\sC$.
      \STATE Optimize \equationref{obj} with respect to $\theta$.
  \end{algorithmic}
\end{algorithm}

\paragraph{Learning with partial moment constraints}

In more general models, \propertyref{exclusive-views} may only hold for
  some subset of the cliques $\sP$ in the graph.
To utilize these partial moment constraints, we propose using posterior
  regularization \citep{graca08em} to constrain expectation-maximization.
While the optimization problem \eqref{eqn:obj-ml} is non-convex, we
  expect that the measurements aid recover of parameters, and indeed see
  this is true for experiments on simulated data.
\todo{Include figure}

Posterior regularization optimizes the following objective,
\begin{align*}
  \hat \theta &= \arg\min_{\theta} \arg\min_{q \in \sQ} \KL( q(\vx,\vh) \| p_{\theta}(\vx, \vh) ),
\end{align*}
where $q$ is a marginal distribution that belongs in constrained set
$\sQ$. In our case, $\sQ$ is constrained to agree with the partial
moments we recovered using \LearnMarginals, i.e. 
\begin{align*}
  \sQ &= \{ q(\vx,\vh) \given \forall \sC \in \sP ~ \E_{q(\vx,\vh)}[\phi_\sC(\vx, \vh)] = Z_\sC \}.
\end{align*}

In practice, we would like to regularize for error in our
  estimates of $Z_\sC$; to do so, we use the measurements model of
  \citet{liang09measurements} which defines a Bayesian model for the noise
  in the marginals $Z_\sC$. The resultant optimization problem is,
\begin{align*}
  \hat\theta 
  &= \arg\min_\theta - \sum_{x \in \sD} \log p_\theta(x) + \half \eta_\theta \| \theta \|^2 \\
  &\quad + \half \eta_\beta \| Z_\sC - \sum_{\vx \in \sD} \E_{\theta}[\phi_\sC(\vx,\vh)] \|^2,
\end{align*}
where $\eta_\theta$ and $\eta_\beta$ are regularization parameters. 

\subsection{Examples}

\paragraph{Latent MRFs}

\begin{figure}
  \label{fig:examples-mrf}
  \centering
  \subimport{figures/}{mrf.tikz}
  \caption{Markov Random Field with 2 views}
\end{figure}

Consider the example in \figureref{examples-mrf}, which is a pairwise Markov
  random field with two observed variables for every hidden variable. 
In practice, if two features are known to be independent, then they can
  be separated into independent observed variables.
Let the parameters of the model be $O \eqdef \Phi(x^a_{i,j}, h_{i,j})
  = \Phi(x^b_{i,j}, h_{i,j})$ and $T \eqdef \Phi(h_{i,j}, h_{i \pm 1,j \pm 1})$. 
Note that as this is an undirected model, the clique marginals do not
  correspond to the parameters of the model.

To recover the parameters of this model, note that every hidden
  variable, $h_{ij} \in \sH$ is a bottleneck, with views
  $x^{a}_{ij},x^{b}_{ij}$ and any other observed variable in the graph. 
Thus, \LearnMarginals will be able to learn the marginals of each clique
  and \LearnLogLinear will give us a consistent estimate of the
  parameters. 

However, the objective \equationref{obj} requires us to compute the
  partition function, $A(\theta)$ and its gradient. 
This scales with the treewidth of the model, which is the number
  of rows/columns of the grid in this example, and hence can be
  intractable.
A more computationally efficient, but still consistent, alternative is
to optimize the pseudo-likelihood \todo{how do I write this out}.


