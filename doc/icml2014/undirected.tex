\section{Consistent Parameter Estimation for Undirected Latent Variable Log-linear Models}
\label{sec:undirected}

In this section, we will show that we can extend the results to undirected graphical models.

Let $z$ be a collection of variables indexed by $V$ (that is, $z = \{z_j\}_{j \in V}$).
Suppose that some of the variables are observed ($x = \{ z_j \}_{j \in O}$, where $O \subset V$) 
and the rest are latent ($h = \{ z_j \}_{j \in H}$, where $H = V \backslash O$).
Let $C \subset 2^V$ be a collection of subsets of variables,
and let $\phi_c(z_c) \in \Re^d$ be the feature map for each $c \in C$.
In this paper, we will consider both directed and undirected models over $z$.
In the undirected case, we define the standard globally-normalized exponential family model:
\begin{align}
  \label{eqn:undirectedSetup}
  p_\theta(z) = \exp\{ \phi(z)^\top\theta - A(\theta) \},
\end{align}
where $\phi(z) \eqdef \sum_{c \in C} \phi_c(z_c) \in \Re^d$ is the global feature vector, $\theta \in \Re^d$ is the parameter vector,
and $A(\theta) = \log \int \exp\{\phi(z)^\top\theta\} dz$ is the global log-partition function.
To some extent, our methods only depend on the conditional independence
structure of the models.

\paragraph{Moment Constraints}

In the previous section, we saw that we could learn moments $P(x1,x2)$ given conditions. This makes the problem convex, boom.

\paragraph{Examples}

\paragraph{General latent-variable models with bottlenecks}

Our learning algorithm for three-view mixture models relies on the fact that
the sufficient statistics of the exponential family
can be expressed as a function of the latent moments.
Of course, these conditions do not hold for general log-linear models.
Rather than falling back on plain local optimization of the likelihood in these
cases, we would still like
to somehow use the \emph{partial information} offered by the latent moments.

Partial information is available when the following holds:
%\begin{property}
%  There exists some latent variable $h_{j_0} \in [\nh]$ (the bottleneck)
%  and three conditionally independent observed variables $x_{j_1},x_{j_2},x_{j_3}$ (views) such that
%  each conditional mean matrix $\E[\phix(x_{j}) \mid h_{j_0}] \in \R^{\nphix \times \nh}$ ($j \in \{ j_1, j_2, j_3 \}$) has full column rank $\nh$
%  for some transformation $\phix$.
%\end{property}

Optimization.




