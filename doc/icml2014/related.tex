\section{Related work}
\label{sec:related}

Parameter estimation for latent variable models using the method of
  moments has received much attention in last few years.
Our work uses the \TensorFactorize algorithm presented by
  \citet{anandkumar13tensor}, though \citet{anandkumar12moments} first
  studied the consistent parameter estimation algorithm for the
  three-view mixture model.
%\todo{What about Mossel and Roch?}
Several authors have extended this work to settings that do not meet the
  criterion \propertyref{bottleneck}, but rely on knowing more about
  the underlying correlation structure of the problem.
\citet{anandkumar12lda} are able to recover parameters for correlated
  discrete mixtures, e.g. latent Dirichlet allocation; their approach
  follows that of the three-view mixture model, with a pre-processing step
  that subtracts the correlation between the views.
\citet{anandkumar12linear} study a special family of Bayesian networks
  in which the observed variables are linearly related to a sparse set of
  hidden variables. Once again, the key step is to identify the
  correlations between observed variables and exclude them from the views.
\citet{halpern13noisyor} propose an algorithm to recover factors for
  a more densely connected bipartite noisy-or network; they also use
  bottlenecks to learn parameters, but exploit properties of the model
  to negate correlations from learned edges, allowing for more
  bottlenecks to be discovered. 
In comparison, our work applies to a general family of models, including
  latent tree models, directed and undirected grids, with no assumptions
  on the correlation structure aside from independences encoded in the
  graph. 

Another prominent line of work in the method of moments community has
  focussed on recovering parameters for a transformation that admits an {\em observable
  representation}, i.e. one whose marginals can be directly computed from the data.
\citet{song2011spectral} propose an algorithm to learn an observable
  operator representations for latent tree graphical models, like the
  one in \figureref{examples-tree}, assuming \propertyref{bottleneck}. 
Their approach is similar to ours, but restricted to trees with observed
  variables at the leaves.
\citet{parikh12spectral} extend this approach to general graphical
  models in which \propertyref{bottleneck} holds, but consider the latent
  junction tree representation. 
Consequently, the observable representations have size atleast that of
  a node in the junction tree, which will be of the order of the treewidth
  of the graph. 
Our algorithm only constructs moments of the order of the cliques in the
  graph, which can be exponentially smaller. 
For example, an $n\times n$ grid model has a treewidth of $n$, but each
  clique is of size at most $2$ in the undirected case and $3$ in the
  directed case.
In some sense, an observable operator re-formulation of our algorithm
  subsumes this work.

