\documentclass[xcolor={svgnames}]{beamer}

\setbeameroption{hide notes} 

%\usetheme{NLP}
\usetheme{boxes}
\useoutertheme{infolines}

\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{calc}

\usepackage{soul}

\usepackage{amsmath,amsthm,amssymb}   

\usepackage{listings}
\usepackage[style=authoryear,babel=hyphen]{biblatex}
\addbibresource{ref.bib}
\addbibresource{pliang.bib}

%\usepackage{algorithm,algorithmic}

\usepackage{tikz}
%\usepackage[debug,debugmarks]{scabby}
\usepackage{scabby}

\usepackage[customcolors]{hf-tikz}

\usepackage{mathtools}

\input{macros}
%\input{spectral-macros}
\input{diagrams}

% these will be used later in the title page
\title[Moments and Likelihoods]{Estimating Latent Variable Graphical Models with Moments and Likelihoods}
\author[Chaganty, Liang]{%
    Arun Tejasvi Chaganty\\
    Percy Liang
}
\institute{Stanford University}

\begin{document}

% "Beamer, do the following at the start of every section"
\AtBeginSection[] 
{%
\begin{frame}<beamer> 
\frametitle{Outline} % make a frame titled "Outline"
\tableofcontents[currentsection]  % show TOC and highlight current section
\end{frame}
}

\begin{frame}
  \titlepage
\end{frame}

\section{Introduction}

\begin{frame}
  \frametitle{Latent Variable Graphical Models}

  \splitcolumn{%
      \begin{itemize}
        \item Gaussian Mixture Models \tikzmark{gmm}
        \item Latent Dirichlet Allocation
        \item Hidden Markov Models \tikzmark{hmm}
        \item PCFGs
        \item \dots
      \end{itemize}
  }{%
  \begin{canvas}
    \point{mark}{(1cm,0)};
    \point{gmm}{($(mark) + (0,+0cm)$)};
    \point{hmm}{($(mark) + (0,-1cm)$)};

    \node[anchor=south west] (mog) at (gmm) {%
      \includegraphics[width=0.45\textwidth,height=3cm,keepaspectratio]{figures/mog.pdf}
    };

    %\drawgen{($(gmm) + (0,0.0cm)$)};
    \drawhmm{(hmm)};
  \end{canvas}
  }

\end{frame}

\begin{frame}
  \frametitle{Parameter Estimation is Hard}

  \begin{tikzpicture}
    % x, y
    \llhood{0}{0};
    \node<2->[scale=0.3,circle,fill=black] at (mle) {};
    \node<2-> at ($(mle) + (0.6cm,0)$) {$\mathmb{\textrm{MLE}}$};
    \node<3->[scale=0.3,circle,fill=black] at (em1) {};
    \node<3-> at ($(em1) + (0.5cm,0)$) {$\mathmr{\textrm{EM}}$};
    \node<3->[scale=0.3,circle,fill=black] at (em2) {};
    \node<3-> at ($(em2) + (0.5cm,0)$) {$\mathmr{\textrm{EM}}$};

    \node<4->[scale=0.3,circle,fill=black] at (spec) {};
    \node<4-> at ($(spec) + (0.5cm,0.3cm)$) {$\mathmg{\textrm{MoM}}$};
   % \draw<4->[latex-latex,DarkGreen,line width=1pt] ($(mle) + (-0.8cm,0.8cm)$) -- node[above]{$\mathmg{\epsilon}$} ($(mle) + (+0.8cm,0.8cm)$);
  \end{tikzpicture}

  % Simple message: MLE is consistent but intractable, EM is efficient not but consistent. Can we get something in between.

  \begin{itemize}
    \item<1-> Log-likelihood function is non-convex.
    \item<2-> MLE is consistent but intractable.
    \item<3-> Local methods (EM, gradient descent, etc.) are tractable but inconsistent\alt<3-4>{.}{\em~and generalize easily.}
    \item<4-> {\em Method of moments} estimators can be consistent and
      computationally-efficient, but more data. 
         \uncover<6->{\bf Thus far, applicable to a limited set of models.}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Main contributions}

  \splitcolumn{%
    \begin{itemize}
     % \item {\em Before our work}
     % \begin{itemize}
     %   \item Gaussian Mixture Models \tikzmark{gmm}
     %   \item Hidden Markov Models 
     %   \item Latent Dirichlet Allocation
     % \end{itemize}
     \item<2-> An algorithm for a {\bf broader model family} with
       succinct conditions. \tikzmark{grid}
    \item<3-> Efficiently learn models with low-degree but {\bf high-treewidth}.
    \item<3-> Extends to {\bf log-linear models}.
    \item<4-> Combine moment estimators with composite
      likelihood estimators.
    \end{itemize}
  }{%
  \begin{canvas}
    \point{mark}{(4cm,0)};
    %\point{gmm}{({pic cs:gmm} -| mark)};
    %\point{grid}{({pic cs:grid} -| mark)};
    \point{gmm}{($(mark) + (0,3cm)$)};
    \point{grid}{($(mark) + (0,-1cm)$)};

%    \node[anchor=south west] (mog) at (gmm) {%
%      \includegraphics[width=0.45\textwidth,height=3cm,keepaspectratio]{figures/mog.pdf}
%    };

    \drawgen{($(gmm) + (0,0.0cm)$)};
    \drawgrid<2->{(grid)};
  \end{canvas}
  }


%  \cornertext<1->{\cite{AnandkumarGeHsu2012}}
%
%  \begin{canvas}
%    % Tasks.
%
%    \node<1->[anchor=west] (diag) at (-3cm, 1cm) {%
%      \includegraphics[width=0.45\textwidth,height=3cm,keepaspectratio]{figures/mog.pdf}
%    };
%    %\drawgen{(-3cm,1cm)}
%    \node[below=0.6cm of diag.south] {Before};
%
%    % Highlight
%    \draw<2>[scale=0.8,fill=green,opacity=0.4,dashed] (1cm,2.5cm) rectangle (6.5cm,-2.5cm);
%      \drawgrid{(3cm,1cm)}
%      \node[below=0.1cm of h4.south] {After};
%  \end{canvas}

\end{frame}

% \begin{frame}
%   \frametitle{Related Work}
%   \begin{itemize}
%     \item<1-> Method of Moments [Pearson, 1894]
%     \item<2-> Observable operators
%     \begin{itemize}
%       \item Control Theory [Ljung, 1987]
%       \item Observable operator models [Jaeger, 2000; {\small{Littman/Sutton/Singh, 2004}}]
%       \item Hidden Markov models [Hsu/Kakade/Zhang, 2009]
%       \item Low-treewidth graphs [Parikh et al., 2012]
%       \item Weighted finite state automata [Balle \& Mohri, 2012]
%     \end{itemize}
%      \item<3-> Parameter Estimation
%   \begin{itemize}
%     \item Mixture of Gaussians [Kalai/Moitra/Valiant, 2010]
%     \item \alert{Mixture models, HMMs [Anandkumar/Hsu/Kakade, 2012]}
%     \item Latent Dirichlet Allocation [Anandkumar/Hsu/Kakade, 2012]
%     \item Stochastic block models [Anandkumar/Ge/Hsu/Kakade, 2012]
%     \item Linear Bayesian networks [Anandkumar/Hsu/Javanmard/Kakade, 2012]
%   \end{itemize}
%   \end{itemize}
% \end{frame}

\section{Three-view Mixture Models}

\begin{frame}
  \frametitle{Three-view Mixture Models}
  \cornertext<1->{\cite{AnandkumarGeHsu2012}}
  \begin{canvas}
    % The model
    \point{mark}{(1cm,0)};
    \point{start}{(1cm,1cm)}; %{pic cs:gen} -| mark)};
    \drawgen<1->{($(start) + (-4cm,1cm)$)};
    %\node[anchor=west] (diag1) at ($(start)$) {%
    %  \includegraphics[width=0.45\textwidth,height=2cm,keepaspectratio]{figures/gen.png}
    %};
    \node<1->[anchor=west] (diag) at ($(start) + (0cm,0.0cm)$) {%
      \includegraphics[width=0.45\textwidth,height=3cm,keepaspectratio]{figures/mog.pdf}
    };

    \node<6-> (bottleneck) at ($(x2.south) - (0,0.4cm)$) {\textmb{\bf Bottleneck}};

    \uncover<2>{
    \vectorfactorization{(-3cm,-2cm)}
    }
    \uncover<3>{
    \matrixfactorization{(-3cm,-2cm)}
    }


    \node<5-> (tf) at ($(t3.north) + (-1cm,0.5cm)$) {Tensor eigen-decomposition};

    \uncover<4->{
    \tensorfactorization{(-3cm,-2cm)}
    }

  \end{canvas}
\end{frame}

\begin{frame}
  \frametitle{}

  \begin{canvas}
    % Tasks.
    \drawgen{(-3cm,1cm)}
    \node[below=0.1cm of x2.south] {Bottlenecked};

    % Highlight
    \draw<2>[scale=0.8,fill=green,opacity=0.4,dashed] (1cm,3.0cm) rectangle (6.5cm,-3.0cm);
      \drawgrid{(3cm,1cm)}
      \node[below=0.8cm of h4.south] {Uniformly bottlenecked};

  \end{canvas}

\end{frame}

\section{Uniformly Bottlenecked Models}

\begin{frame}
  \frametitle{Example: a grid model.}
  \splitcolumn{%
    \begin{itemize}
      \item<2-> Bottleneck! $\Pr(x_i^a | h_i)$.
      \item<4-> Linear relation! $\Pr(x_1^b) = \Pr(x_1^b | h_1) \mathmg{\Pr(h_1)}$.
      \item<6-> Linear relation! $\Pr(x_1^b, x_3^a) = \Pr(x_1^b | h_1) \Pr(x_3^b | h_3) \Pr(h_1) \mathmg{\Pr(h_3 | h_1)}$.
      \item<8-> Linear relation! $\Pr(x_2^b, x_3^b, x_4^b) = \Pr(x_2^b | h_2) \Pr(x_3^b | h_3) \Pr(x_4^b | h_4)$\\
        $\mathmg{\Pr(h_4 | h_2, h_3)} \Pr(h_2) \Pr(h_3)$.
      \item<10-> {\bf Solving the bottlenecks has made the problem easy!}
    \end{itemize}
  }{%
  \begin{canvas}
    \point{mark}{(3cm,0)};
    \point{start-grid}{(mark)};
   \node[style=node, scale=0.8] (h1) at (start-grid) {$h_1$};
   \node[style=node, scale=0.8, below left= 0.5cm of h1] (h2) {$h_2$};
   \node[style=node, scale=0.8, below right= 0.5cm of h1] (h3) {$h_3$};
   \node[style=node, scale=0.8, below right= 0.5cm of h2] (h4) {$h_4$};

   \point{pi}{($(h1.north) + (0,0.1cm)$)};
   \draw[-latex] (h1) -- node[scale=0.7,above] (T1) {} (h2);
   \draw[-latex] (h1) -- node[scale=0.7,above] (T2) {} (h3);
   \draw[-latex] (h2) -- (h4);
   \draw[-latex] (h3) -- (h4);
   \point{V}{($(h4.north) + (0,0.1cm)$)};

% Observed nodes
   \node[style=obsnode, scale=0.7, above left=0.3cm of h1] (x1a) {$x^a_1$};
   \node[style=obsnode, scale=0.7, above right=0.3cm of h1] (x1b) {$x^b_1$};
   \draw[-latex] (h1) -- (x1a);
   \draw[-latex] (h1) -- (x1b);

   \node[style=obsnode, scale=0.7, above left=0.3cm of h2] (x2a) {$x^a_2$};
   \node[style=obsnode, scale=0.7, below left=0.3cm of h2] (x2b) {$x^b_2$};
   \draw[-latex] (h2) -- node[scale=0.7,above] (O1) {} (x2a);
   \draw[-latex] (h2) -- node[scale=0.7,below] (O2) {} (x2b);

   \node[style=obsnode, scale=0.7, above right=0.3cm of h3] (x3a) {$x^a_3$};
   \node[style=obsnode, scale=0.7, below right=0.3cm of h3] (x3b) {$x^b_3$};
   \draw[-latex] (h3) -- (x3a);
   \draw[-latex] (h3) -- (x3b);
    
   \node[style=obsnode, scale=0.7, below left=0.3cm of  h4] (x4a) {$x^a_4$};
   \node[style=obsnode, scale=0.7, below right=0.3cm of h4] (x4b) {$x^b_4$};

   \draw[-latex] (h4) -- (x4a);
   \draw[-latex] (h4) -- (x4b);

   % Story - 
  \begin{pgfonlayer}{background}
  \draw<2>[draw=black,fill=green!70,rounded corners,line width=1pt, dotted] 
                  ($(x2a.north west) + (135:0.3cm)$) -- 
                  ($(x2b.south west) + (-135:0.3cm)$) -- 
                  ($(x4a.south east) + (-45:0.3cm)$) -- 
                  ($(h2.east) + (0:0.3cm)$) -- 
                  ($(x2a.north east) + (45:0.3cm)$) -- 
                  cycle;
  \draw<4>[draw=black,fill=green!70,rounded corners,line width=1pt, dotted] 
                  ($(x1a.north west) + (135:0.3cm)$) -- 
                  ($(x1a.north east) + (45:0.3cm)$) -- 
                  ($(h1.south east) + (-45:0.3cm)$) -- 
                  ($(h1.south west) + (-135:0.3cm)$) -- 
                  cycle;
  \draw<6>[draw=black,fill=green!70,rounded corners,line width=1pt, dotted] 
                  ($(x1b.north west) + (135:0.3cm)$) -- 
                  ($(x3a.north east) + (45:0.3cm)$) -- 
                  ($(h3.south east) + (-45:0.3cm)$) -- 
                  ($(h1.south west) + (-135:0.3cm)$) -- 
                  cycle;
  \draw<8>[draw=black,fill=green!70,rounded corners,line width=1pt, dotted] 
                  ($(h2.north west) + (135:0.3cm)$) -- 
                  ($(x2b.south west) + (-135:0.3cm)$) -- 
                  ($(x4a.south west) + (-135:0.3cm)$) -- 
                  ($(x4b.south east) + (-45:0.3cm)$) -- 
                  ($(x3b.north east) + (45:0.3cm)$) -- 
                  ($(h3.north east) + (45:0.3cm)$) -- 
                  cycle;
  \end{pgfonlayer}

   \node<1>[scale=1.0] at (pi) {$\mathmg{\pi}$};
   \node<1>[scale=1.0] at (T1) {$\mathmg{T}$};
   \node<1>[scale=1.0] at (T2) {$\mathmg{T}$};
   \node<1>[scale=1.0] at (V) {$\mathmg{V}$};
   \node<1>[scale=1.0] at (O1) {$\mathmg{O}$};
   \node<1>[scale=1.0] at (O2) {$\mathmg{O}$};

   \node<3->[scale=1.0] at (O1) {\textmg{\checkmark}};
   \node<3->[scale=1.0] at (O2) {\textmg{\checkmark}};

   \node<5->[scale=1.0] at (pi) {\textmg{\checkmark}};

   \node<7->[scale=1.0] at (T1) {\textmg{\checkmark}};
   \node<7->[scale=1.0] at (T2) {\textmg{\checkmark}};

   \node<9->[scale=1.0] at (V) {\textmg{\checkmark}};

  \end{canvas}
  }
\end{frame}
\begin{frame}
  \frametitle{Views and Bottlenecks}
  \splitcolumn{%
  \begin{itemize}
    \item<+-> {\bf Key insight:} Easy given views $\Pr(x|h)$. 
    \item<+-> If each hidden variable has a unique view, learning
      cliques is solving a linear equation!
    \item<+-> {\bf Uniformly bottlenecked}: Every hidden variable has
      $\ge 3$ conditionally independent observed variables.
  \end{itemize}
  }{%
  \begin{canvas}
    \point{mark}{(3cm,0)};
    \point{start-grid}{(mark)};
   \node[style=node, scale=0.8] (h1) at (start-grid) {$h_1$};
   \node[style=node, scale=0.8, below left= 0.5cm of h1] (h2) {$h_2$};
   \node[style=node, scale=0.8, below right= 0.5cm of h1] (h3) {$h_3$};
   \node[style=node, scale=0.8, below right= 0.5cm of h2] (h4) {$h_4$};

   \point{pi}{($(h1.north) + (0,0.1cm)$)};
   \draw[-latex] (h1) -- node[scale=0.7,above] (T1) {} (h2);
   \draw[-latex] (h1) -- node[scale=0.7,above] (T2) {} (h3);
   \draw[-latex] (h2) -- (h4);
   \draw[-latex] (h3) -- (h4);
   \point{V}{($(h4.north) + (0,0.1cm)$)};

% Observed nodes
   \node[style=obsnode, scale=0.7, above left=0.3cm of h1] (x1a) {$x^a_1$};
   \node[style=obsnode, scale=0.7, above right=0.3cm of h1] (x1b) {$x^b_1$};
   \draw[-latex] (h1) -- (x1a);
   \draw[-latex] (h1) -- (x1b);

   \node[style=obsnode, scale=0.7, above left=0.3cm of h2] (x2a) {$x^a_2$};
   \node[style=obsnode, scale=0.7, below left=0.3cm of h2] (x2b) {$x^b_2$};
   \draw[-latex] (h2) -- node[scale=0.7,above] (O1) {} (x2a);
   \draw[-latex] (h2) -- node[scale=0.7,below] (O2) {} (x2b);

   \node[style=obsnode, scale=0.7, above right=0.3cm of h3] (x3a) {$x^a_3$};
   \node[style=obsnode, scale=0.7, below right=0.3cm of h3] (x3b) {$x^b_3$};
   \draw[-latex] (h3) -- (x3a);
   \draw[-latex] (h3) -- (x3b);
    
   \node[style=obsnode, scale=0.7, below left=0.3cm of  h4] (x4a) {$x^a_4$};
   \node[style=obsnode, scale=0.7, below right=0.3cm of h4] (x4b) {$x^b_4$};

   \draw[-latex] (h4) -- (x4a);
   \draw[-latex] (h4) -- (x4b);

   % Story - 
  \begin{pgfonlayer}{background}
  \draw[draw=black,fill=green!70,rounded corners,line width=1pt, dotted] 
                  ($(h2.north west) + (135:0.3cm)$) -- 
                  ($(x2b.south west) + (-135:0.3cm)$) -- 
                  ($(x4a.south west) + (-135:0.3cm)$) -- 
                  ($(x4b.south east) + (-45:0.3cm)$) -- 
                  ($(x3b.north east) + (45:0.3cm)$) -- 
                  ($(h3.north east) + (45:0.3cm)$) -- 
                  cycle;
  \end{pgfonlayer}

   %\node<1>[scale=1.0] at (pi) {$\mathmg{\pi}$};
   %\node<1>[scale=1.0] at (T1) {$\mathmg{T}$};
   %\node<1>[scale=1.0] at (T2) {$\mathmg{T}$};
   %\node<1>[scale=1.0] at (V) {$\mathmg{V}$};
   %\node<1>[scale=1.0] at (O1) {$\mathmg{O}$};
   %\node<1>[scale=1.0] at (O2) {$\mathmg{O}$};

   %\node<3->[scale=1.0] at (O1) {\textmg{\checkmark}};
   %\node<3->[scale=1.0] at (O2) {\textmg{\checkmark}};

   %\node<5->[scale=1.0] at (pi) {\textmg{\checkmark}};

   %\node<7->[scale=1.0] at (T1) {\textmg{\checkmark}};
   %\node<7->[scale=1.0] at (T2) {\textmg{\checkmark}};

   %\node<9->[scale=1.0] at (V) {\textmg{\checkmark}};

  \end{canvas}
  }
  
\end{frame}

\begin{frame}
  \frametitle{Views and Bottlenecks}
  \splitcolumn{%
  \begin{itemize}
    \item<+-> {\bf Algorithm}: (a) estimate $\Pr(x|h)$ using bottlenecks, (b)
      solve linear equation between $\Pr(h_1,\cdots,h_m)$ and $P(x_1,
      \cdots, x_m)$.
    \item<+-> {\bf Assumptions}: For every clique $\Pr(h_1, \cdots, h_m)$,
      every conditioning $\Pr(h_1, \cdots, h_k | h_{k+1}, \cdots h_m)$
      is ``full-rank''.
  \end{itemize}
  }{%
  \begin{canvas}
    \point{mark}{(3cm,0)};
    \point{start-grid}{(mark)};
   \node[style=node, scale=0.8] (h1) at (start-grid) {$h_1$};
   \node[style=node, scale=0.8, below left= 0.5cm of h1] (h2) {$h_2$};
   \node[style=node, scale=0.8, below right= 0.5cm of h1] (h3) {$h_3$};
   \node[style=node, scale=0.8, below right= 0.5cm of h2] (h4) {$h_4$};

   \point{pi}{($(h1.north) + (0,0.1cm)$)};
   \draw[-latex] (h1) -- node[scale=0.7,above] (T1) {} (h2);
   \draw[-latex] (h1) -- node[scale=0.7,above] (T2) {} (h3);
   \draw[-latex] (h2) -- (h4);
   \draw[-latex] (h3) -- (h4);
   \point{V}{($(h4.north) + (0,0.1cm)$)};

% Observed nodes
   \node[style=obsnode, scale=0.7, above left=0.3cm of h1] (x1a) {$x^a_1$};
   \node[style=obsnode, scale=0.7, above right=0.3cm of h1] (x1b) {$x^b_1$};
   \draw[-latex] (h1) -- (x1a);
   \draw[-latex] (h1) -- (x1b);

   \node[style=obsnode, scale=0.7, above left=0.3cm of h2] (x2a) {$x^a_2$};
   \node[style=obsnode, scale=0.7, below left=0.3cm of h2] (x2b) {$x^b_2$};
   \draw[-latex] (h2) -- node[scale=0.7,above] (O1) {} (x2a);
   \draw[-latex] (h2) -- node[scale=0.7,below] (O2) {} (x2b);

   \node[style=obsnode, scale=0.7, above right=0.3cm of h3] (x3a) {$x^a_3$};
   \node[style=obsnode, scale=0.7, below right=0.3cm of h3] (x3b) {$x^b_3$};
   \draw[-latex] (h3) -- (x3a);
   \draw[-latex] (h3) -- (x3b);
    
   \node[style=obsnode, scale=0.7, below left=0.3cm of  h4] (x4a) {$x^a_4$};
   \node[style=obsnode, scale=0.7, below right=0.3cm of h4] (x4b) {$x^b_4$};

   \draw[-latex] (h4) -- (x4a);
   \draw[-latex] (h4) -- (x4b);

   % Story - 
  \begin{pgfonlayer}{background}
  \draw[draw=black,fill=green!70,rounded corners,line width=1pt, dotted] 
                  ($(h2.north west) + (135:0.3cm)$) -- 
                  ($(x2b.south west) + (-135:0.3cm)$) -- 
                  ($(x4a.south west) + (-135:0.3cm)$) -- 
                  ($(x4b.south east) + (-45:0.3cm)$) -- 
                  ($(x3b.north east) + (45:0.3cm)$) -- 
                  ($(h3.north east) + (45:0.3cm)$) -- 
                  cycle;
  \end{pgfonlayer}

   %\node<1>[scale=1.0] at (pi) {$\mathmg{\pi}$};
   %\node<1>[scale=1.0] at (T1) {$\mathmg{T}$};
   %\node<1>[scale=1.0] at (T2) {$\mathmg{T}$};
   %\node<1>[scale=1.0] at (V) {$\mathmg{V}$};
   %\node<1>[scale=1.0] at (O1) {$\mathmg{O}$};
   %\node<1>[scale=1.0] at (O2) {$\mathmg{O}$};

   %\node<3->[scale=1.0] at (O1) {\textmg{\checkmark}};
   %\node<3->[scale=1.0] at (O2) {\textmg{\checkmark}};

   %\node<5->[scale=1.0] at (pi) {\textmg{\checkmark}};

   %\node<7->[scale=1.0] at (T1) {\textmg{\checkmark}};
   %\node<7->[scale=1.0] at (T2) {\textmg{\checkmark}};

   %\node<9->[scale=1.0] at (V) {\textmg{\checkmark}};

  \end{canvas}
  }
  
\end{frame}


\begin{frame}
  \frametitle{More Uniformly Bottlenecked Examples}
  \begin{canvas}
    \drawhmm<1->{(-4cm, 2cm)};
    \node at ($(start-hmm) + (0, 0.6cm)$) {Hidden Markov models};
    \drawtree<1->{(2cm, 2cm)};
    \node at ($(start-tree) + (0, 0.6cm)$) {Latent Tree models};
    \drawnoisyor<3->{(0cm, -2cm)};
    \node<3-> at ($(start-nor) + (0, 0.7cm)$) {Noisy Or (non-example)};

% \begin{pgfonlayer}{background}
% \draw<2->[rounded corners,line width=1pt, dotted, black] 
%                 ($(h2.north east) + (45:0.2cm)$) -- 
%                 ($(x3.north east) + (45:0.2cm)$) -- 
%                 ($(x3.south east) + (-45:0.2cm)$) -- 
%                 ($(x1.south west) + (-135:0.2cm)$) -- 
%                 ($(x1.north west) + (135:0.2cm)$) -- 
%                 ($(h2.north west) + (135:0.2cm)$) -- 
%                 cycle;
% \end{pgfonlayer}
% 
% \begin{pgfonlayer}{background}
% \draw<2>[rounded corners,line width=1pt, dotted, black] 
%                 ($(h4.north east) + (45:0.2cm)$) -- 
%                 ($(x4b.north east) + (45:0.2cm)$) -- 
%                 ($(x4b.south east) + (-45:0.2cm)$) -- 
%                 ($(x3b.south west) + (-135:0.2cm)$) -- 
%                 ($(x3b.north west) + (135:0.2cm)$) -- 
%                 ($(h4.north west) + (135:0.2cm)$) -- 
%                 cycle;
% \end{pgfonlayer}


  \end{canvas}

\end{frame}

% Efficiency 1: EM (+diagram).
\section{Combining with likelihood estimators}
\begin{frame}
  \frametitle{Convex composite likelihoods}
  \splitcolumn{%
    \begin{itemize}
      \item<1-> Given the observed marginals, $\Pr(x|h)$, the hidden marginals
        $\Pr(h_1, \cdots, h_m)$ is linearly related.
      \item<2-> The composite log-likelihood over $x_1, \cdots, x_m$ is
        actually convex!
      \item<3-> Running EM will converge and is more statistically efficient.
    \end{itemize}
  }{%
    \begin{canvas}
      \point{stuff}{(3cm,0cm)};
      \node<1>[scale=0.5] at (0.5cm,0) {\llhood{3.5}{0}};
      \node<2->[anchor=center] (obj) at (stuff) {%
      \includegraphics[width=\textwidth,height=6cm,keepaspectratio]{figures/piecewise-objective.pdf}
      };
    \end{canvas}
  }
\end{frame}

\section{Log-linear models}
\begin{frame}
  \frametitle{Log-linear models}
  \begin{itemize}
    \item<1-> $p_\theta(\bx, \bh) = \exp\left( \theta^\top \phi(\bx,\bh) - A(\theta) \right)$.
    \item<2-> The {\em unsupervised} negative log-likelihood is non-convex,
        \begin{align*}
        L_\text{unsup}(\theta) \eqdef \E_{\bx \sim \sD}[\log \sum_{\bh \in \sH} p_\theta(\bx,\bh)].
        \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Estimation is convex with marginals}
  \begin{itemize}
    \item<1-> $p_\theta(\bx, \bh) = \exp\left( \theta^\top \phi(\bx,\bh) - A(\theta) \right)$.
      \item<1-> In the supervised case with $\bh$ though, the objective {\em is convex},
        \begin{align*}
        \label{eqn:logLinearSupervised}
        L_\text{sup}(\theta) &\eqdef \E_{(\bx,\bh) \sim \sD_\text{sup}}[\log p_\theta(\bx,\bh)] \\
                             &= \theta^\top \left(\sum_{\sC \in \sG} \E[\phi(\bx_\sC,\bh_\sC)]\right) - A(\theta).
        \end{align*}
      \item<2-> {\bf Key idea:} We can estimate marginals $\E_{(\bx,\bh) \sim \sD_\text{sup}}[\log p_\theta(\bx,\bh)]$!
  \end{itemize}
\end{frame}

\section{Conclusions}

\begin{frame}
  \frametitle{Conclusions}
  \begin{itemize}
      \item Uniformly bottlenecked models
      \item Scales with the size of each clique, not the tree-width
      \item Solving bottlenecks breaks problem into convex pieces; can be solved more accurately
      \item The marginals make the log-linear recovery problem convex.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}
    Thank you!
\end{frame}

\end{document}

