# June 18, 2014  

Categorize this message as:
Never show this again
Major point: have a general scheme (flowchart) as a outline
  1. p(x) => P(x|h)
  2. p(x), P(x|h) => P(h1,h2) [pseudoinverse, composite likelihood]
  3. Extract parameters [maximum likelihood (counting or
optimization), pseudolikelihood]

Define observed moments, conditional moments, hidden marginals, parameters
  slowly in the beginning

Say discrete variables
state what the parameters are (list them out)
Say focus on directed models

General
  Much better, smoother

Slide 4:
  Latent tree structure => break up

  Make distinction with observable operator clearer

AHK 2012: based on simultaneous diagonalization

Slide 6:
  Each edge has a 'parameter' => 'set of parameters'

Say: 'empirical moments' => 'observed moments'

Slide 10: recall what a bottleneck is

Slide 11: use red instead of blue - 'done' color needs to contrast more

Slide 14: no closed-form likelihood, but convex, so can use EM

Slide 15: 'we show in the paper that...'

Slide 21: 'low-degree' is not necessary
  'extends to ...'

# June 12, 2014  

slide 3:
  'we use a local optimzation' => 'people typically use' to
distinguish it from you
  make this slide go faster
slide 4:
  EM 'solve it' => 'can generate an algorithm'
slide 5:
  point: method of moments used in conjunction with likelihood
slide 8:
  quickly say: what are the parameters of the model?
  discrete variables; under full rank, as number of examples tends to infinity
slide 9:
  Bigger pause before this part, because now it's your contribution -
need more setup
  Make clear that this is a stepping stone
  KEY POINT: we can recover complex distributions, but hard to get the
actual parameters
  10: write down solving the actual linear equation
  Refer to the examples on slides more
  SAY explicitly: we introduce the notions 'exclusive view',
'uniformly bottleneck'
  Write down the general theorem:
  Say that you assume infinite amount of data -> perturbation

slide 15:
  not clear what the log likelihood is

General comments
  More formal (not necessarily more equations): what are parameters?
   what are the contracts (your theorem and the black box)?
  Refer to the examples on the slides more
  Define terminology
  Related work

# May 1st, 2014
Slide 4: say this is a cartoon
Slide 4: spending too much time on the first part of this slide.  The last bullet is the most important point,
put it on its own slide!

Slide 5: don't talk about three-view under "Main contributions"...
Main contributions should not be 4 bullet points
  Broaden: High treewidth and log-linear models
  Techniques: combine likelihood and moments (not just bonus points, needed for log-linear models)

Slide 7: make colors of vectors and Gaussians match
Spending too much time here (great for NLP talk, not for ICML) - say this is review, and treat it as black box.

Slide 8: bottleneck: say why it's a bottleneck
say "directed grid" model

Slide 10: clearer if you build the graphical model gradually as you mention the parts
 (don't need to say: T doesn't have to be the same)
  Can't see the checkmarks, put checkmarks with the actual parameters

SLide 10: talk about linear relation in a two latent-variable model

Slide 11: should be clearer; uniformly bottlenecked property
Skip talk about parameter sharing

*** Need to talk about p(x_j|h_i) are various properties of the model, not the actual parameters.
We can recover some nasty product of the parameters - need to highlight this challenge.

Give more examples of graphs with bottleneck 

Slide 12: could have better use of space (text is too cramped)

Slide 12: assumptions should be new slide;
A bit too loose: say that matrix, solve

Slide 13: don't talk about edge cases / parameter sharing

Slide 15: terminology needs to be made consistent (piecewise => composite, moment matching => pseudoinverse)

Slide 17: didn't talk about loopy graphs

** Meta statement - say at beginning: use Anandkumar as black box, do some graphical models stuff on top

** Need figure that demonstrates the flow of information (observed moments => hidden moments => parameters)

** Ignore edge cases to save time

** More connection between rank/matrices and probabilities/graphical models

Mention sample complexity?
