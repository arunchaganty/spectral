\section{Consistent Parameter Estimation in Directed Models}
\label{sec:directed}

The tensor factorization method attacks the heart of the non-convexity
  in latent variable models.
  Once we recover the conditional moments $\mOpp{v}{j} \eqdef \Pr(x_v
  | h_j)$ for some $v, j$, we present a systematic approach to learn the
  conditional probability tables for every clique for a class of directed
  graphical models.
We begin by illustrating our approach with an example, followed by
  describing the algorithm in full generality.
For ease of exposition, we will describe our algorithm entirely in the
  context of directed graphical models, though it generalizes to factor
  graphs.
We also present the algorithm solely in terms of the method of moments; in
  \sectionref{piecewise}, we show how we can get a statistically more
  efficient estimator using convex optimization.

\subsection{Example: Directed grid model}

\begin{figure}
  \centering
  \input{figures/grid-outline.tikz}
  \caption{A directed grid model.}
  \label{fig:grid}
\end{figure}

Consider the directed grid model shown in \figureref{grid} which
  captures all the different dependency structures possible in
  a Bayesian network.
The model has eight observed variables $x^a_1, x^b_1 \cdots, x^a_4, x^b_4$ and four
  hidden variables $h_1, \cdots, h_4$.
The parameters of this model are the conditional probability tables
  $\pi \eqdef \Pr(h_1), T \eqdef \Pr(h_2 | h_1) = \Pr(h_3 | h_1),
  V \eqdef \Pr(h_4 | h_2, h_3)$ and $O \eqdef \Pr(x^a_i | h_i)
  =  \Pr(x^b_i | h_i)$. 
Finally, we assume $O$ and $T$ have full column rank.

\paragraph{Estimating $O$}
Note that the observed variables $x^a_1, x^b_1, x^a_2$ are
  conditionally independent given $h_1$, allowing us to use
  \TensorFactorize, as reviewed in \sectionref{setup}, to recover
  the $O$.

\paragraph{Estimating $\pi$}
The moments of $x^a_1$,$\mO_1 \eqdef \Pr(x^a_1)$ are directly related to
  $\pi$; $\mO_1 = O \pi$. 
By \assumptionref{full-rank}, $O$ has full column rank and thus can be
  inverted to recover $\pi$; $\pi = \pinv O \mO_1$.

\paragraph{Estimating $T$}
Similarly, we can write down the moments of $x^a_1, x^a_2$, $\mO_{12}
  \eqdef \Pr(x^a_1, x^a_2)$ in terms of the pairwise marginals $\mH_{12}
  \eqdef \Pr(h_1, h_2)$ and solve for $\mH_{12}$;
\begin{align*}
  \mO_{12} &= O \mH_{12} O^T \\
  \mH_{12} &= \pinv O \mO_{12} \pinvt O.
\end{align*}
$T$ can be recovered from the $\mH_{12}$ by suitable normalization.

\paragraph{Estimating $V$}
Finally, we can estimate $V$ from the third-order moments $\mO_{234} \eqdef \Pr(x^a_2, x^a_3, x^a_4)$ as
follows,
\begin{align*}
  \mO_{234} &= \mH_{234}(O, O, O) \\
  \mH_{234} &= \mO_{234}(\pinv O, \pinv O, \pinv O).
\end{align*}

\subsection{General algorithm}

Fundamentally, our algorithm takes advantage of having a view for every
  hidden variable in a clique to help identify its hidden state. We define this condition as follows,
\begin{property}(Exclusive views)
  \label{prop:exclusive-views}
A clique $\sC$ of $\sG$ is said to posses exclusive views if for every hidden variable $h_i \in \sC$,
  there is some observed variable, $x_{v}$, which is conditionally
  independent of the rest of the clique given $h_i$ and whose conditional
  moment $\mOpp{v}{i} \eqdef \Pr(x_{v} | h_i)$ is known. 
We call $x_v$ the exclusive view of $h_i$ in $\sC$.
\end{property}

\paragraph{Learning cliques}

We now show this condition is sufficient to learn the marginal
  distribution of the clique.
Consider any clique $\sC = \{h_{i_1}, \cdots, h_{i_m}\} \in \sG$ for which this property holds. Let
  $x_{v_j}$ be the exclusive view for $h_{i_j}$ in $\sC$ and $\sV
  = \{x_{v_1}, \cdots, x_{v_m}\}$ be the set of views for the clique
  $\sC$.
We can write down the moments $\mO_\sV$,
\begin{align*}
  \mO_\sV 
  &\eqdef \Pr(\Sx{\sV}) \\
  &= \sum_{\vh \in \sH}
      \Pr(\Sh{\sC}) \Pr(\Sx{\sV} \given \Sh{\sC}) \\
      &= \sum_{\vh \in sH} \Pr(\Sh{\sC}) 
          \Pr(x_{i_1} | h_{v_1}) \cdots \Pr(x_{i_m} | h_{v_m}) \\
    &= Z_{\sC}(\mOpp{v_1}{i_1},\cdots,\mOpp{v_m}{i_m}).
\end{align*}

If each $\mOpp{v_j}{i_j}$ has full column rank, then we can recover the
hidden marginals $Z_\sC$ by inverting the conditional moments,
\begin{align*}
  Z_{\sC} &= \mO_\sV(\pinv{\mOpp{v_1}{i_1}},\cdots,\pinv{\mOpp{v_m}{i_m}}).
\end{align*}
Finally, the conditional probability tables for $\sC$ can easily be gotten from
  $Z_\sC$ via conditioning.
\algorithmref{learnclique} summarizes the procedure, \LearnClique.

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}
  \caption{\LearnClique}
  \label{algo:learnclique}
  \begin{algorithmic}
    \REQUIRE A $\sC$ with \propertyref{exclusive-views}.
    \ENSURE The marginal distribution of the clique, $Z_\sC$.
      \STATE Identify unique views $\Sx{\sV} = \{x_{v_1}, \cdots, x_{v_m}\}$.
      \STATE $\mH_\sC \gets \mO_{\Sx{\sV}}( \pinv{\mOpp{v_1}{i_i}}, \cdots, \pinv{\mOpp{v_m}{i_m}} )$.
  \end{algorithmic}
\end{algorithm}

\paragraph{Structural properties}

If we can guarantee that every clique in a graph possesses exclusive
  views, then we can apply \LearnClique iteratively to learn the entire
  model.
In the example, we were able to guarantee \propertyref{exclusive-views}
  by identifying a hidden variable, $h^1$, with three conditionally
  independent observed variables, $x^a_1, x^b_1, x^a_2$, followed by
  using \TensorFactorize to learn the conditional moments.
We define such a hidden variable to be a bottleneck;
\begin{definition}(Bottleneck)
  A hidden variable $h$ is said to be a bottleneck if there exists at
    least three conditionally independent observed variables, or views,
    $x_1, x_2, x_3$. 
  Let $\sV_{h}$ be the set of views for $h$.
\end{definition}
Finally, the claim we make is that we can recover parameters for any
  graphical model where every hidden variable is a bottleneck;
\begin{property}
  \label{prop:bottleneck}
  Every hidden variable is a bottleneck.
\end{property}
We note that \propertyref{bottleneck} can be relaxed if some cliques
  share parameters;
For example, because we can identify the observation potential from just
  $x^a_1, x^b_1$ and $x^a_2$, we do not require that $h_2, h_3$ or $h_4$
  be bottlenecks, and can correspondingly omit the observations $x^b_2,
  x^b_3$ and $x^b_4$.
In this case, we only require that the hidden variables in distinct
  cliques share parameters.

Applying \TensorFactorize to every bottleneck gives us candidate views
  for every hidden variable. 
To proceed, we need to show two things; (i) under what assumptions
  does \assumptionref{full-rank} hold for each bottleneck and (ii)
  that the candidate views actually imply that \propertyref{exclusive-views}
  holds for every clique in the graph.

To address the first, we will need that the following assumption on the
  parameters of the model:
\begin{assumption} 
  \label{asm:full-rank-plus}
  For every clique $\sC = \{h_{i_1}, \cdots, h_{i_m}\} \in \sG$, let
    $Z_\sC$ be the marginal distribution for that clique. 
  Then every mode-$i$ unfolding of $Z_\sC$ has full column rank.\verify
\end{assumption}
Intuitively, the assumption guarantees that the marginal probability of
  any hidden state is non-zero.

Before we establish the second condition, we describe a reduction to
  a canonical form that will allow us to handle the case where an observed
  variable has multiple parents.


\begin{lemma}(Canonical form) 
  \label{lem:reduction}
Every directed graphical model can be transformed into one in which
  the observed variables are leaves with exactly one parent. 
There is a one-to-one correspondence between the parameters of this
  transformed model and the original one.
\end{lemma}
\begin{proof}
  \begin{figure}
    \centering
    \subimport{figures/}{reduction.tikz}
    \caption{Reduction to canonical form.}
    \label{fig:reduction}
  \end{figure}

  %\providecommand{\hp}{\ensuremath{h_{\textrm{phantom}}}}
  \providecommand{\hp}{\ensuremath{h_p}}

  Let $x_v$ be an observed variable with parents $P$ and children $C$.
  Consider the following transformation.
  Replace $x_v$ with a phantom hidden variable \hp with the same 
  domain\footnote{Though we have assumed that all hidden variables share the
      same domain, $[k]$, our work generalizes to hidden domains of
      arbitrary sizes, provided \assumptionref{full-rank-plus} holds.},
  parents $P$ and children $C \union \{x_{v_1}, x_{v_2}, x_{v_3}\}$,
  where $x_{v_1},x_{v_2},x_{v_3}$ are three copies of $x$
  (\figureref{reduction}). Also, set $\mOpp{v}{p} = I$.

  Then, there is a one-to-one correspondence between every value of
  $\hp$ and $x_v$. Consequently, for any clique $\sC \contains \hp$, the
  parameters in the original graphical model can be obtained by
  substituting $\hp$ with $x_v$.
\end{proof}

\begin{lemma}(Bottlenecks guarantee exclusive views)
  Provided \propertyref{bottleneck} holds, every hidden variable
    $h_{i_1}, \cdots, h_{i_m}$ in a clique $\sC$ has a exclusive view.
\end{lemma}
\begin{proof}
  By \lemmaref{reduction}, we can assume w.l.o.g. that every observed
  variable is a leaf in the graph, causing any v-structures in the graph
  to be active. This allows us to reason about independence simply by
  checking whether or not two nodes are connected or not; note that this
  proof applies for undirected graphs as well.

  Let $h$ be a hidden variable in $\sC$, let $\sV_{h}$ be the set of
    views for it and let $\sC^- \eqdef \sC \setminus \{h\}$.
  By definition, the views of $h$ are conditionally independent, viz.
    every connecting path between two views $x_1, x_2 \in \sV_h$ must pass through
    $h$.
  A view $x \in \sV_{h}$ is exclusive for $h$ iff $x \not\in \sV_{h'} ~ \forall h'
  \in \sC^-$

  Suppose $h$ had no unique view, then $\sV_{h} \subseteq \Union_{h' \in \sC^-} \sV_{h'}$. 
  However, this would imply that for every $x_1, x_2 \in \sV_{h}$, there
    exists some $h_1, h_2 \in \sC^-$ (not necessarily distinct) such that
    $x_1$ is connected to $h_1$ and $x_2$ is connected $h_2$.
  By participation in the clique, either $h_1 = h_2$ or $h_1$ is
    connected to $h_2$, implying that $x_1$ and $x_2$ are connected
    via a path that does not pass through $h$, contradicting the
    statement that $x_1, x_2 \in \sV$. 
    
  Thus it must be the case that $\sV_{h} \subsetneq \Union_{h' \neq
    h \in \sC} \sV_{h'}$ and hence there exists a unique view for $h$.

  Algorithmically, finding the unique views can be done by subtracting
    out the remaining set, i.e. $x_{v_h} \in \sV_h \setminus \Union_{h'
    \in \sC} \sV_{h'}$
\end{proof}

With this lemma in place, we present the full algorithm, \LearnMarginals~
in \algorithmref{directed}.

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}
  \caption{\LearnMarginals}
  \label{algo:directed}
  \begin{algorithmic}
    \REQUIRE A graphical model $\sG$ satisfying \propertyref{bottleneck}, data $\sD$
    \ENSURE Marginals $Z_\sC$ for every clique $\sC \in \sG$

      \FOR{$h_i \in H$} 
        \STATE Apply \TensorFactorize to learn conditional moments
        $\mOpp{v}{i}$ for every $\sV_h$.

%    \COMMENT{Recover observation potentials $O$ using bottlenecks}
      \ENDFOR
%      \COMMENT{\textbf{Step 2:} Recover clique potentials from the piecewise likelihood.}
\FOR{every clique $\sC = \{h_{i_1}, \cdots, h_{i_m}\} \in \sG$} 
  \STATE Apply \LearnClique to learn the marginals $\mH_\sC$.
\ENDFOR
  \end{algorithmic}
\end{algorithm}

\paragraph{Consistency and Sample Complexity}

\LearnMarginals~ combines two consistent algorithms, \TensorFactorize and
  \LearnClique, and is thus consistent itself. Using results from
  \citet{anandkumar12moments,anandkumar13tensor} we can show that that
  learning $\mOpp{v_1}{i}$ for the bottleneck $h_i$ with views $x_{v_1},
  x_{v_2}, x_{v_3}$ has the following sample complexity,
\begin{align*}
  \|\mOpphat{v_1}{i} - \mOpp{v_1}{i}\|^2_F &= \frac{1}{\sqrt{n}} O( \frac{k {\pi\oft{i}}_{\max}^2}{\sigma_k(M_{v_1,v_2})^5} ). 
\end{align*}

We next describe the asymptotic variance for \LearnClique when using the
  method of moments.
\begin{lemma}(Asymptotic variance of $Z_\sC$)
  \label{lem:mom-variance}  
  The asymptotic variance of $Z_\sC$, $\Sigma^\mom_\sC$ is,
  \begin{align*}
    \Sigma^{(\mom)} &= \mOppit{\sV}{\sC} \Sigma_\sV \mOppi{\sV}{\sC},
  \end{align*}
  where $\pinv{\mOpp{\sV}{\sC}} \eqdef \pinv{\mOpp{v_1}{i_1}} \otimes
    \cdots \otimes \pinv{\mOpp{v_m}{i_m}}$, a $k^m \times d^m$ matrix, and 
\begin{align*}
  \Sigma_\sV &= \diag  M_\sV  - M_\sV M_\sV^T,
\end{align*}
  is the variance of the observations $M_\sV$.
\end{lemma}
\begin{proof}
We have for each clique,
\begin{align*}
  Z_{\sC} &= \mO_\sV(\mOppi{v_1}{i_1},\cdots,\mOppi{v_m}{i_m}) \\
\end{align*}

Choosing to represent $Z_\sC$ and $M_\sV$ as vectors and
$\mOppi{v_1}{i_1} \otimes \cdots \otimes \mOppi{v_m}{i_m}$ as the
matrix, we can rewrite this representation as,
\begin{align*}
  Z_{\sC} &= \pinv{\mOpp{\sV}{\sC}} \mO_\sV.
\end{align*}

We know the asymptotic distribution of $M_\sV$ goes as,
\begin{align*}
  \sqrt{n}(\hat M_\sV - M_\sV) \convind \sN(0, \Sigma_\sV),
\end{align*}
where $\Sigma_\sV$ is the variance of the observations. 
Finally, we have that $Z_\sC$ goes as
\begin{align*}
  \sqrt{n}(\hat Z_\sC - Z_\sC) \convind \sN(0, \pinvt{\mOpp{\sV}{\sC}} \Sigma_\sV \pinv{\mOpp{\sV}{\sC}}).
\end{align*}
\end{proof}

Note that extending these results to finite sample bounds can be done
  via a straightforward application of perturbation bounds.

