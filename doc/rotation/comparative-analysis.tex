\section{Comparative Analysis of Existing Algorithms}
\label{sec:comparative-analysis}

\subsection{Mixture Models}
\begin{figure}[htbp]
\floatconts
  {fig:ahk2012-algo-a}
  {\caption{Algorithm A from AHK2012}}
  {\input{ahk2012-algo-a}}
\end{figure}

\begin{figure}[htbp]
\floatconts
  {fig:ahk2012-algo-b}
  {\caption{Algorithm B from AHK2012}}
  {\input{ahk2012-algo-b}}
\end{figure}

Let the absolute error of the inputs, $P_{12}$, $P_{13}$, $P_{123}$ be
$\aerr{P_{12}}, \aerr{P_{13}}$ and $\aerr{P_{123}}$ respectively. The
first step of the algorithm ``aligns'' $P_{123}$ with the singular
vectors of $P_{12}$.
\begin{align}
  \comment{\propositionref{prop:rot}} \aerr{\Up_1^T P_{12} \Up_2} &=& \aerr{P_{12}} \\
  \comment{\propositionref{prop:rot}} \aerr{\Up_1^T P_{123} \Up_2} &=& \aerr{P_{123}}.
\end{align}

Subsequently, $B_{123} = (\Up_1^T P_{123} \Up_2) \inv{(\Up_1^T P_{12} \Up_2)}$
is constructed,
\begin{align}
  \comment{\propositionref{prop:inv}} \aerr{\inv{P_{12}}} &=& \|\inv{P_{12}}\| \frac{ \cnd{A} }{\|P_{12}\| - \aerr{P_{12}} \cnd{A}} \aerr{P_{12}} \\
  \comment{\propositionref{prop:prod}} \aerr{B_{123}} &=& \aerr{P_{123}} \|\inv{P_{12}} \| + \|P_{123}\| \aerr{\inv{P_{12}}}.
\end{align}

$B_{123}$ is projected onto $k$ different $\theta_i$, and subsequently
diagonalised to give eigenvalues $\lambda_i$
\begin{align}
  \comment{\propositionref{prop:sim-eigd}} 
  \aerr{\lambda} &=& O(n^3) \cnd{R}^2 \|\inv{R}\| \inv{\gap{\lambda}}  \aerr{B_{123}} \\
  \comment{\propositionref{prop:proj}}
  \gap{\lambda} 
  &\le& \frac{\delta}{\sqrt{e} n^2 (1 + \sqrt{2\log(\frac{m}{\delta})})}
         \gap{\Up_3^T M_3} \\
  &\le& \frac{\delta}{\sqrt{e} n^2 (1 + \sqrt{2\log(\frac{m}{\delta})})}
         \gap{M_3}.
\end{align}

We can use the fact that $B_{123} = (\Up_1^T M_1) \diag( M_3^T \Up_3
\theta ) \inv{(\Up_1^T M_1)}$ to find $R$,
\begin{align}
  R &=& \Up_1^T M_1 \inv{\diag( \|\Up_1^T M_1 \vec{e}_1 \|, \cdots )} \\
  \| \inv{R} \| &\le& \cnd{\Up_1^T \|M_1\|} \\
          &\le& 2 \cnd{M_1} \\
  \cnd{R} &\le& 4 \cnd{M_1}^2.
\end{align}

Finally, in order to recover $M_3$ from $\lambda_i$, we need to invert
our projection, namely, $\mu_i = \Up_3^T \inv{\Theta} \lambda_i$. 
\begin{align}
  \aerr{\mu_i} &\le& \| \mu_i - \mup_i \|_2 \\
     &\le& \| \Up_3^T \inv{\Theta} \lambdap_i - \mu_i \|_2 \\
     &\le& \| \inv{\Theta} \lambdap_i - \Up_3 \mu_i \|_2 + \|\mu_i\|
     \frac{\|\inv{P_{13}}\| \aerr{P_{13}}}{1 - \|\inv{P_{13}}\| \aerr{P_{13}}} \\
     &\le& \| \inv{\Theta} (\lambdap_i - \lambda_i) \|_2 + 2 \|\mu_i\|
            \frac{\|\inv{P_{13}}\| \aerr{P_{13}}}{1 - \|\inv{P_{13}}\| \aerr{P_{13}}} \\
     &\le& \sqrt{k} \aerr{\lambda} + 2 \|\mu_i\|
            \frac{\|\inv{P_{13}}\| \aerr{P_{13}}}{1 - \|\inv{P_{13}}\| \aerr{P_{13}}} \\
\end{align}

\subsection{LDA}

\begin{figure}[htbp]
\floatconts
  {fig:2svd-lda}
  {\caption{Algorithm 5 from Two SVDs}}
  {\input{2svd-lda}}
\end{figure}

Let the absolute errors of the inputs, $\Pairs$ and $\Triples$ be $\aerr{P}$ and
$\aerr{T}$ respectively. The error in the whitening operator $W$ is then,
\begin{align}
  \comment{\propositionref{prop:white}} \aerr{W} &=& \frac{2}{\sigma_k(P)^2} (\beta_P) \\
  \|W\| &=& \frac{1}{\sqrt{\sigma_k(P)}} \\
  \|\Wp\| &\le& \frac{1}{\sqrt{\sigma_k(P) - \aerr{P}}} \\
          &=& \frac{\|W\|}{\sqrt{1 - \alpha_P}}.
\end{align}

The whitened tensor $T_W = \Triples[W,W,W]$ have the error, 
\begin{eqnarray}
  \aerr{T_W} 
    &\le& \|\Wp\| \aerr{T} + \|T\| (\|W\|^2 + \|W\|\|\Wp\| + \|\Wp\|^2 ) \\
    &\le& \frac{\|\inv{P}\|^{\half}}{\sqrt{1 - \alpha_P}} \aerr{T} + \|T\| \|\inv{P}\| (1 + \frac{1}{\sqrt{1 - \alpha_P}} + \frac{1}{1 - \alpha_P}).
\end{eqnarray}

The singular vectors of $T_W(\theta)$ have the error,
\begin{eqnarray}
  \aerr{v_i} 
    &\le& \frac{2 \sqrt{k} \aerr{T_W}}{\gap{T_W} - \aerr{T_W}}.
\end{eqnarray}

To make the calculation tractable, we assume $\aerr{T_W}
< \frac{\gap{T_W}}{2}$. By the random projection result, with probability $1-\delta$,  

\begin{eqnarray}
  \aerr{v_i} 
    &\le& \frac{2 \sqrt{k}}{\gap{T_W} - \aerr{T_W}}  \aerr{T_W}\\
    &\le& \frac{4 \sqrt{k}}{\gap{T_W}}  \aerr{T_W}\\
    &\approx& \frac{4 \sqrt{k} \aerr{T_W}}{\frac{\delta}{\sqrt{e} \binom{n}{2} (1 + \sqrt{2\log(\frac{k}{\delta})}) \gap{T}}} \\
    &=& \frac{4 \sqrt{ke} \binom{n}{2} (1 + \sqrt{2\log(\frac{k}{\delta})})}{\delta} \gap{T} \aerr{T_W}
\end{eqnarray}

\todo{$\gap{T_W}$ is not the same as the $\gap{A}$ use in AHK2012; needs some new notation.}

Finally, we must normalise these singular vectors, which we do by
projecting $v_i$ onto $W$, i.e. $(\pinv{W})^T v_i$ and dividing by $Z_i
= \frac{2}{(\alpha_0 + 2) T_W[v_i, v_i, v_i]}$ Thus, 

\begin{eqnarray}
  \aerr{O_i} 
  &\le& \| O_i - \Pi_W O_i \| + \| \Pi_W O_i - \frac{1}{\Zp_i} (\pinv{W})^T \vp_i \| \\
  &\le& \|\Pi - \Pi_W\| 
  + \frac{\|\pinv{W}\|}{Z_i} \aerr{v_i} 
  + \frac{\|\vp_i\|}{Z_i} \aerr{\pinv{W}} 
  + \|\pinv{\Wp}\| \|\vp_i\| \aerr{\frac{1}{Z_i}}.
\end{eqnarray}

\subsection{Spherical Gaussians}

Let the absolute error of the inputs, $\cM_{2}$ and $\cM_{3}$ be
$\aerr{\cM_{2}}$ and $\aerr{\cM_{3}}$ respectively. 
\begin{align}
  \sigma^2 &=& \sigma_k( \cM_2 - \mu \mu^T ) \\
  \comment{\theoremref{thm:weyl}}
  \aerr{\sigma^2} &\le& \|(\cM_2 - \mu \mu^T) - (\cMp_2 - \mup \mup^T)\| \\
  &\le& \aerr{\cM_2} + 2 \|\mu\|_p \aerr{\mu} + \aerr{\mu}^2
\end{align}

Next, $M_2$ is defined to be the best k-rank approximation of  $(\cM_2 - \sigma^2 I)$, 
\begin{align}
  \aerr{M_2} &=& \| \Mp_2 - (\cMp_2 - \sigmap^2 I) + (\cMp_2 - \sigmap^2 I) - \cM_2 - \sigma^2 I \| \\
  &\le& \| \Mp_2 - (\cMp_2 - \sigmap^2 I) \| + \| \cMp_2 - \cM_2 \| + \| \sigmap^2 I - \sigma^2 I \| \\
  \comment{\theoremref{thm:weyl}}
  &\le& \sigma_{k+1}(\cMp_2 - \sigmap^2 I) + \aerr{\cM_2 - \sigma^2 I} \\
  &\le& 2\aerr{\cM_2 - \sigma^2 I} \\
  &\le& 2\aerr{\cM_2} + 2 \aerr{\sigma^2} \\
  &\le& 4\aerr{\cM_2} + 4 \|\mu\|_p \aerr{\mu} + 2 \aerr{\mu}^2.
\end{align}

$M_3$ is similarly defined to be $\cM_3 - \sigma^2 (\sum_{i=1}^{d}
(\mu_i \otimes e_i \otimes e_i + e_i \otimes \mu_i \otimes e_i +e_i \otimes
e_i \otimes \mu_i)$,
\begin{align}
  \comment{\propositionref{prop:prod}}
  \aerr{M_3} &\le& \aerr{ \cM_3 } + 3 (\| \mu \| + \aerr{\mu}) \aerr{ \sigma } +  3 \sigma^2 \aerr{\mu}.
\end{align}

We need to construct the whitening operator $W$,
\begin{align}
  \comment{\propositionref{prop:white}} 
  \aerr{W} &=& \frac{2}{\sqrt{\sigma_k(M_2)}} \berr{M_2} \\
  \|W\| &=& \frac{1}{\sqrt{\sigma_k(M_2)}} \\
  \|\Wp\| &\le& \frac{1}{\sqrt{\sigma_k(M_2) - \aerr{M_2}}} \\
       &=& \frac{\|W\|}{\sqrt{1 - \alpha_{M_2}}} \\
  \comment{\propositionref{prop:white}} 
  \aerr{\pinv{W}} &=& 2 \sqrt{M_2 + \aerr{M_2}} \berr{M_2} \\
  \|\pinv{W}\| &=& \sqrt{\|M_2\|} \\
  \|\pinv{\Wp}\| &\le& {\sqrt{\sigma_k(M_2) + \aerr{M_2}}}.
\end{align}

And the whitened tensor, $T_W = M3[W,W,W]$ has the error, 
\begin{eqnarray}
  \aerr{T_W} 
    &\le& \|\Wp\| \aerr{T} + \|T\| (\|W\|^2 + \|W\|\|\Wp\| + \|\Wp\|^2 ) \\
    &\le& \frac{\|\inv{M_2}\|^{\half}}{\sqrt{1 - \alpha_{M_2}}} \aerr{T} + \|T\| \|\inv{M_2}\| (1 + \frac{1}{\sqrt{1 - \alpha_{M_2}}} + \frac{1}{1 - \alpha_{M_2}}).
\end{eqnarray}

We can use the fact that this tensor projected onto a random vector will
be positive semi-definite to perform our eigendecomposition analysis.

The difference between two eigenvalues $\lambda$ and two eigenvectors
$v$ is, 
\begin{align}
  \aerr{\lambda_i} &\le& \aerr{T_W} \\
  \aerr{v_i} &\le& 4\sqrt{2} \aerr{T_W} \gap{T_W}.
\end{align}

Finally, when reconstructing $\mu_i$, 
\begin{align}
  \aerr{\mu_i} &=& \| \frac{\lambdap_i}{\theta^T \pinv{\Wp} \vp_i} \pinv{\Wp} \vp - \mu_i \| \\
  &\le& \frac{1}{\sqrt{w_i}} \| \frac{\lambdap_i}{\theta^T \pinv{\Wp} \vp_i} \sqrt{w_i} \pinv{\Wp} \vp - \sqrt{w_i} \mu_i \| \\
  &\le& | \frac{\lambdap_i}{\theta^T \pinv{\Wp} \vp_i} - \frac{1}{\sqrt{w_i}} | \|\pinv{\Wp} \vp_i\| + \| \pinv{\Wp} \vp_i - \sqrt{w_i} \mu_i \| \\
  \| \pinv{\Wp} \vp_i - \sqrt{w_i} \mu_i \| 
  &=&  \| \pinv{\Wp} \vp_i - \pinv{W} v_i \| \\
  &\le&  \| \pinv{\Wp} \| \aerr{v_i} + \aerr{\pinv{W}} \\
  | \frac{\lambdap_i}{\theta^T \pinv{\Wp} \vp_i} - \frac{1}{\sqrt{w_i}} | 
  &=& \frac{1}{\sqrt{w_i}} |\sqrt{w_i} \lambdap_i - \theta^T \pinv{\Wp} \vp_i | \\
  &=& \frac{1}{\sqrt{w_i}} |\sqrt{w_i} (\lambdap_i - \lambda_i) + \theta^T ( \pinv{W}v_i -  \pinv{W}\vp_i) | \\
  &=& \aerr{\lambda_i} + \aerr{\pinv{W}v_i} \\
  &=& \aerr{\lambda_i} + \frac{1}{\sqrt{w_i}} \| \pinv{\Wp} \| \aerr{v_i} + \aerr{\pinv{W}} \\
  \aerr{\mu_i} 
  &\le& \aerr{\lambda_i} + (1+\frac{1}{\sqrt{w_i}}) \| \pinv{\Wp} \| \aerr{v_i} + \aerr{\pinv{W}}.
\end{align}


